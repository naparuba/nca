import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from typing import Tuple, List, Optional, Dict, Any
import os
import argparse
import json
import time
from pathlib import Path

# Import du module de visualisation v9
from visualize_modular_progressive_obstacles_variable_intensity import create_complete_visualization_suite

# =============================================================================
# Configuration et initialisation modulaire
# =============================================================================

class ModularConfig:
    """
    Configuration √©tendue pour l'apprentissage modulaire progressif avec intensit√©s variables (Version 8__).
    H√©rite des param√®tres de base et ajoute la gestion des √©tapes avec intensit√©s variables.
    """
    def __init__(self, seed: int = 123):
        # Param√®tres mat√©riels de base
        self.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        self.SEED = seed

        # Param√®tres de grille
        self.GRID_SIZE = 16
        self.SOURCE_INTENSITY = 1.0

        # Param√®tres d'entra√Ænement modulaire
        self.TOTAL_EPOCHS = 500  # Augment√© pour l'apprentissage modulaire
        self.NCA_STEPS = 20
        self.LEARNING_RATE = 1e-3
        self.BATCH_SIZE = 4

        # Configuration modulaire (NOUVEAU)
        self.MODULAR_TRAINING = True
        self.ENABLE_CURRICULUM = True
        self.ADAPTIVE_THRESHOLDS = True

        # === NOUVEAUX PARAM√àTRES VERSION 8__ ===
        # R√©partition des √©tapes modifi√©e (ratios des √©poques totales)
        self.STAGE_1_RATIO = 0.3  # 30% - Sans obstacles (modifi√© de 0.5)
        self.STAGE_2_RATIO = 0.3  # 30% - Un obstacle (inchang√©)
        self.STAGE_3_RATIO = 0.2  # 20% - Obstacles multiples (inchang√©)
        self.STAGE_4_RATIO = 0.2  # 20% - Intensit√©s variables (NOUVEAU)

        # Calcul automatique des √©poques par √©tape
        self.STAGE_1_EPOCHS = int(self.TOTAL_EPOCHS * self.STAGE_1_RATIO)
        self.STAGE_2_EPOCHS = int(self.TOTAL_EPOCHS * self.STAGE_2_RATIO)
        self.STAGE_3_EPOCHS = int(self.TOTAL_EPOCHS * self.STAGE_3_RATIO)
        self.STAGE_4_EPOCHS = int(self.TOTAL_EPOCHS * self.STAGE_4_RATIO)  # NOUVEAU

        # Param√®tres intensit√©s variables (NOUVEAU)
        self.VARIABLE_INTENSITY_TRAINING = True
        self.MIN_SOURCE_INTENSITY = 0.0    # Intensit√© minimale (√©teint)
        self.MAX_SOURCE_INTENSITY = 1.0    # Intensit√© maximale (standard)
        self.DEFAULT_SOURCE_INTENSITY = 1.0 # Intensit√© de r√©f√©rence (√©tapes 1-3)

        # Configuration simple pour √©tape 4 (NOUVEAU)
        self.STAGE_4_SOURCE_CONFIG = {
            'intensity_distribution': 'uniform',       # Distribution des intensit√©s
            'sample_per_simulation': True,            # Nouvelle intensit√© √† chaque simulation
            'fixed_during_simulation': True,          # Intensit√© fixe pendant la simulation
            'intensity_range_expansion': True,        # √âlargir progressivement la plage
            'initial_range': [0.5, 1.0],            # Plage initiale restreinte
            'final_range': [0.0, 1.0]               # Plage finale compl√®te
        }

        # Seuils de convergence adaptatifs par √©tape (CORRIG√â)
        self.CONVERGENCE_THRESHOLDS = {
            1: 0.0002,  # √âtape 1
            2: 0.0002,  # √âtape 2
            3: 0.0002,  # √âtape 3
            4: 0.0002,   # √âtape 4
        }

        # Param√®tres de visualisation
        self.PREVIS_STEPS = 30
        self.POSTVIS_STEPS = 50
        self.SAVE_ANIMATIONS = True
        self.SAVE_STAGE_CHECKPOINTS = True
        self.OUTPUT_DIR = "nca_outputs_modular_progressive_obstacles_variable_intensity"  # Nom sans pr√©fixe

        # Param√®tres du mod√®le
        self.HIDDEN_SIZE = 128
        self.N_LAYERS = 3

        # Param√®tres d'obstacles par √©tape (√âTENDU)
        self.STAGE_OBSTACLE_CONFIG = {
            1: {'min_obstacles': 0, 'max_obstacles': 0},  # Pas d'obstacles
            2: {'min_obstacles': 1, 'max_obstacles': 1},  # Un seul obstacle
            3: {'min_obstacles': 2, 'max_obstacles': 4},  # 2-4 obstacles
            4: {'min_obstacles': 1, 'max_obstacles': 2}   # 1-2 obstacles (NOUVEAU)
        }

        self.MIN_OBSTACLE_SIZE = 2
        self.MAX_OBSTACLE_SIZE = 4

        # Optimisations (h√©rit√©es de v6)
        self.USE_OPTIMIZATIONS = True
        self.USE_SEQUENCE_CACHE = True
        self.USE_VECTORIZED_PATCHES = True
        self.CACHE_SIZE = 200
        self.USE_MIXED_PRECISION = False

        # Nouveaux param√®tres modulaires
        self.MAX_STAGE_RETRIES = 3  # Tentatives max par √©tape
        self.EARLY_STOPPING_PATIENCE = 20  # Patience pour arr√™t pr√©coce
        self.STAGE_TRANSITION_SMOOTHING = True  # Lissage des transitions

def parse_modular_arguments():
    """
    Parse les arguments √©tendus pour l'apprentissage modulaire.
    """
    parser = argparse.ArgumentParser(
        description='Neural Cellular Automaton - Apprentissage Modulaire Progressif',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Arguments de base
    parser.add_argument('--seed', type=int, default=123,
                       help='Graine al√©atoire pour la reproductibilit√©')
    parser.add_argument('--vis-seed', type=int, default=3333,
                       help='Graine pour les visualisations')
    parser.add_argument('--total-epochs', type=int, default=500,
                       help='Nombre total d\'√©poques d\'entra√Ænement')
    parser.add_argument('--grid-size', type=int, default=16,
                       help='Taille de la grille')
    parser.add_argument('--batch-size', type=int, default=4,
                       help='Taille des batches')

    # Arguments modulaires (NOUVEAUX)
    parser.add_argument('--stage1-ratio', type=float, default=0.3,
                       help='Ratio d\'√©poques pour l\'√©tape 1 (sans obstacles)')
    parser.add_argument('--stage2-ratio', type=float, default=0.3,
                       help='Ratio d\'√©poques pour l\'√©tape 2 (un obstacle)')
    parser.add_argument('--stage3-ratio', type=float, default=0.2,
                       help='Ratio d\'√©poques pour l\'√©tape 3 (obstacles multiples)')
    parser.add_argument('--stage4-ratio', type=float, default=0.2,
                       help='Ratio d\'√©poques pour l\'√©tape 4 (intensit√©s variables)')

    parser.add_argument('--enable-curriculum', action='store_true', default=True,
                       help='Activer l\'apprentissage par curriculum')
    parser.add_argument('--adaptive-thresholds', action='store_true', default=True,
                       help='Utiliser des seuils adaptatifs pour l\'avancement')
    parser.add_argument('--max-obstacles', type=int, default=4,
                       help='Nombre maximum d\'obstacles en √©tape 3')
    parser.add_argument('--save-stage-checkpoints', action='store_true', default=True,
                       help='Sauvegarder les mod√®les √† chaque √©tape')

    return parser.parse_args()

# Parse des arguments et configuration
args = parse_modular_arguments()
cfg = ModularConfig(seed=args.seed)

# Mise √† jour depuis les arguments
cfg.TOTAL_EPOCHS = args.total_epochs
cfg.GRID_SIZE = args.grid_size
cfg.BATCH_SIZE = args.batch_size
cfg.STAGE_1_RATIO = args.stage1_ratio
cfg.STAGE_2_RATIO = args.stage2_ratio
cfg.STAGE_3_RATIO = args.stage3_ratio
cfg.STAGE_4_RATIO = args.stage4_ratio
cfg.ENABLE_CURRICULUM = args.enable_curriculum
cfg.ADAPTIVE_THRESHOLDS = args.adaptive_thresholds
cfg.SAVE_STAGE_CHECKPOINTS = args.save_stage_checkpoints

# Recalcul des √©poques par √©tape
cfg.STAGE_1_EPOCHS = int(cfg.TOTAL_EPOCHS * cfg.STAGE_1_RATIO)
cfg.STAGE_2_EPOCHS = int(cfg.TOTAL_EPOCHS * cfg.STAGE_2_RATIO)
cfg.STAGE_3_EPOCHS = int(cfg.TOTAL_EPOCHS * cfg.STAGE_3_RATIO)
cfg.STAGE_4_EPOCHS = int(cfg.TOTAL_EPOCHS * cfg.STAGE_4_RATIO)  # NOUVEAU

# Configuration max obstacles √©tape 3
cfg.STAGE_OBSTACLE_CONFIG[3]['max_obstacles'] = args.max_obstacles

# Gestion matplotlib (h√©rit√©e de v6)
def setup_matplotlib():
    """Configure matplotlib pour l'affichage interactif ou la sauvegarde."""
    try:
        matplotlib.use('Qt5Agg')
        plt.ion()
        fig, ax = plt.subplots()
        plt.close(fig)
        print("‚úÖ Mode interactif activ√©")
        return True
    except:
        try:
            matplotlib.use('TkAgg')
            plt.ion()
            fig, ax = plt.subplots()
            plt.close(fig)
            print("‚úÖ Mode interactif activ√© (TkAgg)")
            return True
        except:
            print("‚ö†Ô∏è  Mode non-interactif d√©tect√© - les animations seront sauvegard√©es")
            matplotlib.use('Agg')
            return False

# Initialisation
interactive_mode = setup_matplotlib()
if os.name == 'nt':
    interactive_mode = False

torch.manual_seed(cfg.SEED)
np.random.seed(cfg.SEED)

# Cr√©ation du r√©pertoire de sortie avec seed
cfg.OUTPUT_DIR = f"nca_outputs_modular_progressive_obstacles_variable_intensity_seed_{cfg.SEED}"
if cfg.SAVE_ANIMATIONS:
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

print(f"üéØ Configuration Modulaire NCA v7__")
print(f"Device: {cfg.DEVICE}")
print(f"Seed: {cfg.SEED}")
print(f"Mode interactif: {interactive_mode}")
print(f"R√©pertoire de sortie: {cfg.OUTPUT_DIR}")
print(f"Apprentissage modulaire: {cfg.MODULAR_TRAINING}")
print(f"Curriculum learning: {cfg.ENABLE_CURRICULUM}")
print(f"√âtapes: {cfg.STAGE_1_EPOCHS} + {cfg.STAGE_2_EPOCHS} + {cfg.STAGE_3_EPOCHS} + {cfg.STAGE_4_EPOCHS} = {cfg.TOTAL_EPOCHS} √©poques")

# =============================================================================
# Gestionnaire d'obstacles progressifs
# =============================================================================

class ProgressiveObstacleManager:
    """
    Gestionnaire intelligent des obstacles selon l'√©tape d'apprentissage.
    G√©n√®re des environnements appropri√©s pour chaque phase du curriculum.
    """

    def __init__(self, device: str = cfg.DEVICE):
        self.device = device
        self.stage_configs = cfg.STAGE_OBSTACLE_CONFIG

    def generate_stage_environment(self, stage: int, size: int, source_pos: Tuple[int, int],
                                 seed: Optional[int] = None) -> torch.Tensor:
        """
        G√©n√®re un environnement d'obstacles adapt√© √† l'√©tape courante.

        Args:
            stage: Num√©ro d'√©tape (1, 2, ou 3)
            size: Taille de la grille
            source_pos: Position de la source (i, j)
            seed: Graine pour la reproductibilit√©

        Returns:
            Masque des obstacles [H, W]
        """
        if stage not in self.stage_configs:
            raise ValueError(f"√âtape {stage} non support√©e. √âtapes valides: {list(self.stage_configs.keys())}")

        config = self.stage_configs[stage]

        if stage == 1:
            return self._generate_stage_1_environment(size)
        elif stage == 2:
            return self._generate_stage_2_environment(size, source_pos, seed)
        elif stage == 3:
            return self._generate_stage_3_environment(size, source_pos, seed)
        elif stage == 4:
            return self._generate_stage_4_environment(size, source_pos, seed)

    def _generate_stage_1_environment(self, size: int) -> torch.Tensor:
        """√âtape 1: Aucun obstacle - grille vide pour apprentissage de base."""
        return torch.zeros((size, size), dtype=torch.bool, device=self.device)

    def _generate_stage_2_environment(self, size: int, source_pos: Tuple[int, int],
                                    seed: Optional[int] = None) -> torch.Tensor:
        """√âtape 2: Un seul obstacle pour apprentissage du contournement."""
        obstacle_mask = torch.zeros((size, size), dtype=torch.bool, device=self.device)

        if seed is not None:
            g = torch.Generator(device=self.device)
            g.manual_seed(seed)
        else:
            g = None

        # Un seul obstacle de taille al√©atoire
        obstacle_size = torch.randint(cfg.MIN_OBSTACLE_SIZE, cfg.MAX_OBSTACLE_SIZE + 1,
                                    (1,), generator=g, device=self.device).item()

        # Placement en √©vitant la source et les bords
        max_pos = size - obstacle_size
        if max_pos <= 1:
            return obstacle_mask  # Grille trop petite

        source_i, source_j = source_pos

        for attempt in range(100):  # Plus de tentatives pour √©tape 2
            i = torch.randint(1, max_pos, (1,), generator=g, device=self.device).item()
            j = torch.randint(1, max_pos, (1,), generator=g, device=self.device).item()

            # V√©rifier non-chevauchement avec source
            if not (i <= source_i < i + obstacle_size and j <= source_j < j + obstacle_size):
                obstacle_mask[i:i+obstacle_size, j:j+obstacle_size] = True
                break

        return obstacle_mask

    def _generate_stage_3_environment(self, size: int, source_pos: Tuple[int, int],
                                    seed: Optional[int] = None) -> torch.Tensor:
        """√âtape 3: Obstacles multiples pour gestion de la complexit√©."""
        obstacle_mask = torch.zeros((size, size), dtype=torch.bool, device=self.device)

        if seed is not None:
            g = torch.Generator(device=self.device)
            g.manual_seed(seed)
        else:
            g = None

        config = self.stage_configs[3]
        n_obstacles = torch.randint(config['min_obstacles'], config['max_obstacles'] + 1,
                                  (1,), generator=g, device=self.device).item()

        source_i, source_j = source_pos
        placed_obstacles = []

        for obstacle_idx in range(n_obstacles):
            obstacle_size = torch.randint(cfg.MIN_OBSTACLE_SIZE, cfg.MAX_OBSTACLE_SIZE + 1,
                                        (1,), generator=g, device=self.device).item()

            max_pos = size - obstacle_size
            if max_pos <= 1:
                continue

            for attempt in range(50):
                i = torch.randint(1, max_pos, (1,), generator=g, device=self.device).item()
                j = torch.randint(1, max_pos, (1,), generator=g, device=self.device).item()

                # V√©rifications multiples pour √©tape 3
                valid_position = True

                # 1. Pas de chevauchement avec source
                if i <= source_i < i + obstacle_size and j <= source_j < j + obstacle_size:
                    valid_position = False

                # 2. Pas de chevauchement avec obstacles existants
                for obs_i, obs_j, obs_size in placed_obstacles:
                    if (i < obs_i + obs_size and i + obstacle_size > obs_i and
                        j < obs_j + obs_size and j + obstacle_size > obs_j):
                        valid_position = False
                        break

                if valid_position:
                    obstacle_mask[i:i+obstacle_size, j:j+obstacle_size] = True
                    placed_obstacles.append((i, j, obstacle_size))
                    break

        # Validation finale de connectivit√©
        if not self._validate_connectivity(obstacle_mask, source_pos):
            print("‚ö†Ô∏è  Connectivit√© non garantie - g√©n√©ration d'un environnement plus simple")
            return self._generate_stage_2_environment(size, source_pos, seed)

        return obstacle_mask

    def _generate_stage_4_environment(self, size: int, source_pos: Tuple[int, int],
                                    seed: Optional[int] = None) -> torch.Tensor:
        """√âtape 4: Intensit√©s variables avec obstacles pour gestion avanc√©e."""
        obstacle_mask = torch.zeros((size, size), dtype=torch.bool, device=self.device)

        if seed is not None:
            g = torch.Generator(device=self.device)
            g.manual_seed(seed)
        else:
            g = None

        # Configuration simple pour √©tape 4
        config = self.stage_configs[4]
        n_obstacles = torch.randint(config['min_obstacles'], config['max_obstacles'] + 1,
                                  (1,), generator=g, device=self.device).item()

        source_i, source_j = source_pos
        placed_obstacles = []

        for obstacle_idx in range(n_obstacles):
            obstacle_size = torch.randint(cfg.MIN_OBSTACLE_SIZE, cfg.MAX_OBSTACLE_SIZE + 1,
                                        (1,), generator=g, device=self.device).item()

            max_pos = size - obstacle_size
            if max_pos <= 1:
                continue

            for attempt in range(50):
                i = torch.randint(1, max_pos, (1,), generator=g, device=self.device).item()
                j = torch.randint(1, max_pos, (1,), generator=g, device=self.device).item()

                # V√©rifications multiples pour √©tape 4
                valid_position = True

                # 1. Pas de chevauchement avec source
                if i <= source_i < i + obstacle_size and j <= source_j < j + obstacle_size:
                    valid_position = False

                # 2. Pas de chevauchement avec obstacles existants
                for obs_i, obs_j, obs_size in placed_obstacles:
                    if (i < obs_i + obs_size and i + obstacle_size > obs_i and
                        j < obs_j + obs_size and j + obstacle_size > obs_j):
                        valid_position = False
                        break

                if valid_position:
                    obstacle_mask[i:i+obstacle_size, j:j+obstacle_size] = True
                    placed_obstacles.append((i, j, obstacle_size))
                    break

        # Validation finale de connectivit√©
        if not self._validate_connectivity(obstacle_mask, source_pos):
            print("‚ö†Ô∏è  Connectivit√© non garantie - g√©n√©ration d'un environnement plus simple")
            return self._generate_stage_3_environment(size, source_pos, seed)

        return obstacle_mask

    def _validate_connectivity(self, obstacle_mask: torch.Tensor, source_pos: Tuple[int, int]) -> bool:
        """
        Valide qu'un chemin de diffusion reste possible avec les obstacles.
        Utilise un algorithme de flood-fill simplifi√©.
        """
        H, W = obstacle_mask.shape
        source_i, source_j = source_pos

        # Matrice de visite
        visited = torch.zeros_like(obstacle_mask, dtype=torch.bool)
        visited[obstacle_mask] = True  # Les obstacles sont "d√©j√† visit√©s"

        # Flood-fill depuis la source
        stack = [(source_i, source_j)]
        visited[source_i, source_j] = True
        accessible_cells = 1

        while stack:
            i, j = stack.pop()

            # Parcours des 4 voisins
            for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                ni, nj = i + di, j + dj

                if (0 <= ni < H and 0 <= nj < W and
                    not visited[ni, nj] and not obstacle_mask[ni, nj]):
                    visited[ni, nj] = True
                    stack.append((ni, nj))
                    accessible_cells += 1

        # Au moins 50% de la grille doit √™tre accessible pour une bonne diffusion
        total_free_cells = (H * W) - obstacle_mask.sum().item()
        connectivity_ratio = accessible_cells / max(total_free_cells, 1)

        return connectivity_ratio >= 0.5

    def get_difficulty_metrics(self, stage: int, obstacle_mask: torch.Tensor) -> Dict[str, float]:
        """
        Calcule des m√©triques de difficult√© pour l'environnement g√©n√©r√©.

        Returns:
            Dictionnaire avec les m√©triques de complexit√©
        """
        H, W = obstacle_mask.shape
        total_cells = H * W
        obstacle_cells = obstacle_mask.sum().item()

        metrics = {
            'stage': stage,
            'obstacle_ratio': obstacle_cells / total_cells,
            'free_cells': total_cells - obstacle_cells,
            'complexity_score': stage * (obstacle_cells / total_cells)
        }

        return metrics

# =============================================================================
# Simulateur de diffusion (h√©rit√© et adapt√© de v6)
# =============================================================================

class DiffusionSimulator:
    """
    Simulateur de diffusion de chaleur adapt√© pour l'apprentissage modulaire.
    Utilise le gestionnaire d'obstacles progressifs et support intensit√©s variables (Version 8__).
    """

    def __init__(self, device: str = cfg.DEVICE):
        self.kernel = torch.ones((1, 1, 3, 3), device=device) / 9.0
        self.device = device
        self.obstacle_manager = ProgressiveObstacleManager(device)

    def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor,
             source_intensity: Optional[float] = None) -> torch.Tensor:
        """Un pas de diffusion de chaleur avec obstacles et support intensit√© variable."""
        x = grid.unsqueeze(0).unsqueeze(0)
        new_grid = F.conv2d(x, self.kernel, padding=1).squeeze(0).squeeze(0)

        # Contraintes
        new_grid[obstacle_mask] = 0.0

        # MODIFICATION VERSION 8__ : Support intensit√© variable
        if source_intensity is not None:
            new_grid[source_mask] = source_intensity  # Intensit√© sp√©cifique
        else:
            new_grid[source_mask] = grid[source_mask]  # Comportement original

        return new_grid

    def generate_stage_sequence(self, stage: int, n_steps: int, size: int,
                              seed: Optional[int] = None, source_intensity: Optional[float] = None) -> Tuple[List[torch.Tensor], torch.Tensor, torch.Tensor, Optional[float]]:
        """
        G√©n√®re une s√©quence adapt√©e √† l'√©tape d'apprentissage courante.

        Args:
            stage: √âtape d'apprentissage (1, 2, 3, ou 4)
            n_steps: Nombre d'√©tapes de simulation
            size: Taille de la grille
            seed: Graine pour la reproductibilit√©
            source_intensity: Intensit√© sp√©cifique pour √©tape 4 (None = intensit√© standard)

        Returns:
            (s√©quence, masque_source, masque_obstacles, intensit√©_utilis√©e)
        """
        # Position al√©atoire de la source
        if seed is not None:
            g = torch.Generator(device=self.device)
            g.manual_seed(seed)
            i0 = torch.randint(2, size-2, (1,), generator=g, device=self.device).item()
            j0 = torch.randint(2, size-2, (1,), generator=g, device=self.device).item()
        else:
            i0 = torch.randint(2, size-2, (1,), device=self.device).item()
            j0 = torch.randint(2, size-2, (1,), device=self.device).item()

        # G√©n√©ration d'obstacles selon l'√©tape
        obstacle_mask = self.obstacle_manager.generate_stage_environment(stage, size, (i0, j0), seed)

        # MODIFICATION VERSION 8__ : Gestion intensit√© variable pour √©tape 4
        if stage == 4 and source_intensity is not None:
            # √âtape 4 : utilise l'intensit√© sp√©cifi√©e
            used_intensity = source_intensity
        else:
            # √âtapes 1-3 : intensit√© standard
            used_intensity = cfg.SOURCE_INTENSITY

        # Initialisation
        grid = torch.zeros((size, size), device=self.device)
        grid[i0, j0] = used_intensity  # Utilise l'intensit√© appropri√©e

        source_mask = torch.zeros_like(grid, dtype=torch.bool)
        source_mask[i0, j0] = True

        # S'assurer que la source n'est pas dans un obstacle
        if obstacle_mask[i0, j0]:
            obstacle_mask[i0, j0] = False

        # Simulation temporelle avec intensit√© appropri√©e
        sequence = [grid.clone()]
        for _ in range(n_steps):
            if stage == 4:
                # √âtape 4 : passe l'intensit√© sp√©cifique
                grid = self.step(grid, source_mask, obstacle_mask, source_intensity)
            else:
                # √âtapes 1-3 : comportement original
                grid = self.step(grid, source_mask, obstacle_mask)
            sequence.append(grid.clone())

        return sequence, source_mask, obstacle_mask, used_intensity

# Instance globale du simulateur modulaire
simulator = DiffusionSimulator()

# =============================================================================
# Mod√®le NCA (h√©rit√© de v6 avec adaptations mineures)
# =============================================================================

class ImprovedNCA(nn.Module):
    """
    Neural Cellular Automaton optimis√© pour l'apprentissage modulaire.
    Architecture identique √† v6 mais avec support √©tendu pour le curriculum.
    """

    def __init__(self, input_size: int = 11, hidden_size: int = cfg.HIDDEN_SIZE,
                 n_layers: int = cfg.N_LAYERS):
        super().__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.n_layers = n_layers

        # Architecture profonde avec normalisation
        layers = []
        current_size = input_size

        for i in range(n_layers):
            layers.append(nn.Linear(current_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
            current_size = hidden_size

        # Couche de sortie stabilis√©e
        layers.append(nn.Linear(hidden_size, 1))
        layers.append(nn.Tanh())

        self.network = nn.Sequential(*layers)
        self.delta_scale = 0.1

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass avec scaling des deltas."""
        delta = self.network(x)
        return delta * self.delta_scale

# =============================================================================
# Updaters NCA (h√©rit√©es de v6)
# =============================================================================

class OptimizedNCAUpdater:
    """
    Updater optimis√© avec extraction vectoris√©e des patches.
    VERSION 8__ : Support des intensit√©s variables pour l'√©tape 4.
    """

    def __init__(self, model: ImprovedNCA, device: str = cfg.DEVICE):
        self.model = model
        self.device = device

    def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor,
             source_intensity: Optional[float] = None) -> torch.Tensor:
        """Application optimis√©e du NCA avec support intensit√© variable."""
        H, W = grid.shape

        # Extraction vectoris√©e des patches 3x3
        grid_padded = F.pad(grid.unsqueeze(0).unsqueeze(0), (1, 1, 1, 1), mode='replicate')
        patches = F.unfold(grid_padded, kernel_size=3, stride=1)
        patches = patches.squeeze(0).transpose(0, 1)  # [H*W, 9]

        # Features additionnelles
        source_flat = source_mask.flatten().float().unsqueeze(1)  # [H*W, 1]
        obstacle_flat = obstacle_mask.flatten().float().unsqueeze(1)  # [H*W, 1]
        full_patches = torch.cat([patches, source_flat, obstacle_flat], dim=1)  # [H*W, 11]

        # Application seulement sur positions valides
        valid_mask = ~obstacle_mask.flatten()

        if valid_mask.any():
            valid_patches = full_patches[valid_mask]
            deltas = self.model(valid_patches)

            new_grid = grid.clone().flatten()
            new_grid[valid_mask] += deltas.squeeze()
            new_grid = torch.clamp(new_grid, 0.0, 1.0).reshape(H, W)
        else:
            new_grid = grid.clone()

        # Contraintes finales (VERSION 8__ : Support intensit√© variable)
        new_grid[obstacle_mask] = 0.0
        if source_intensity is not None:
            # √âtape 4 : applique l'intensit√© sp√©cifique
            new_grid[source_mask] = source_intensity
        else:
            # √âtapes 1-3 : comportement original
            new_grid[source_mask] = grid[source_mask]

        return new_grid

class NCAUpdater:
    """
    Updater standard avec boucles Python.
    VERSION 8__ : Support des intensit√©s variables pour l'√©tape 4.
    """

    def __init__(self, model: ImprovedNCA, device: str = cfg.DEVICE):
        self.model = model
        self.device = device

    def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor,
             source_intensity: Optional[float] = None) -> torch.Tensor:
        """Application standard du NCA avec support intensit√© variable."""
        H, W = grid.shape
        new_grid = grid.clone()

        patches = []
        positions = []

        for i in range(1, H-1):
            for j in range(1, W-1):
                if obstacle_mask[i, j]:
                    continue

                patch = grid[i-1:i+2, j-1:j+2].reshape(-1)  # 9 √©l√©ments
                is_source = source_mask[i, j].float()
                is_obstacle = obstacle_mask[i, j].float()
                full_patch = torch.cat([patch, is_source.unsqueeze(0), is_obstacle.unsqueeze(0)])

                patches.append(full_patch)
                positions.append((i, j))

        if patches:
            patches_tensor = torch.stack(patches)
            deltas = self.model(patches_tensor)

            for idx, (i, j) in enumerate(positions):
                new_value = grid[i, j] + deltas[idx].squeeze()
                new_grid[i, j] = torch.clamp(new_value, 0.0, 1.0)

        # Contraintes (VERSION 8__ : Support intensit√© variable)
        new_grid[obstacle_mask] = 0.0
        if source_intensity is not None:
            # √âtape 4 : applique l'intensit√© sp√©cifique
            new_grid[source_mask] = source_intensity
        else:
            # √âtapes 1-3 : comportement original
            new_grid[source_mask] = grid[source_mask]

        return new_grid

# =============================================================================
# Planificateur de curriculum (NOUVEAU)
# =============================================================================

class CurriculumScheduler:
    """
    Gestionnaire de la progression automatique entre les √©tapes d'apprentissage.
    D√©cide quand passer √† l'√©tape suivante selon des crit√®res adaptatifs.
    """

    def __init__(self, convergence_thresholds: Dict[int, float], patience: int = 10):
        self.thresholds = convergence_thresholds
        self.patience = patience
        self.stage_metrics_history = {stage: [] for stage in [1, 2, 3]}
        self.no_improvement_counts = {stage: 0 for stage in [1, 2, 3]}

    def should_advance_stage(self, current_stage: int, recent_losses: List[float]) -> bool:
        """
        D√©termine s'il faut passer √† l'√©tape suivante avec une logique plus stricte.

        Args:
            current_stage: √âtape courante
            recent_losses: Pertes r√©centes pour √©valuation

        Returns:
            True si on doit avancer √† l'√©tape suivante
        """
        if not recent_losses or current_stage >= 3:
            return False

        # LOGIQUE AM√âLIOR√âE : Plus stricte pour √©viter les fausses convergences

        # 1. Convergence: moyenne des pertes r√©centes (√©tendue √† 10 √©poques)
        if len(recent_losses) < 10:
            return False  # Pas assez d'√©poques pour juger

        avg_recent_loss = np.mean(recent_losses[-10:])  # 10 derni√®res √©poques (√©tait 5)
        threshold = self.thresholds.get(current_stage, 0.05)

        # 2. Crit√®re principal: convergence atteinte ET stable
        converged = avg_recent_loss < threshold

        # 3. Crit√®re de stabilit√©: les 5 derni√®res pertes doivent √™tre proches
        if len(recent_losses) >= 5:
            last_5_losses = recent_losses[-5:]
            stability = np.std(last_5_losses) < 0.001  # Variance faible = stabilit√©
        else:
            stability = False

        # 4. Crit√®re secondaire renforc√©: stagnation prolong√©e
        if len(recent_losses) >= 3:
            improvement = recent_losses[-3] - recent_losses[-1]  # Sur 3 √©poques
            if improvement < 0.0001:  # Am√©lioration quasi-nulle (plus strict)
                self.no_improvement_counts[current_stage] += 1
            else:
                self.no_improvement_counts[current_stage] = 0

        stagnated = self.no_improvement_counts[current_stage] >= self.patience * 2  # Double patience

        # D√âCISION : Convergence ET stabilit√© OU stagnation prolong√©e
        return (converged and stability) or stagnated

    def adjust_learning_rate(self, optimizer, stage: int, epoch_in_stage: int):
        """Ajuste le learning rate selon l'√©tape et la progression."""
        base_lr = cfg.LEARNING_RATE

        # R√©duction progressive par √©tape
        stage_multipliers = {1: 1.0, 2: 0.8, 3: 0.6}
        stage_lr = base_lr * stage_multipliers.get(stage, 0.5)

        # D√©croissance cosine au sein de l'√©tape
        stage_epochs = {1: cfg.STAGE_1_EPOCHS, 2: cfg.STAGE_2_EPOCHS, 3: cfg.STAGE_3_EPOCHS}
        max_epochs = stage_epochs.get(stage, 50)

        cos_factor = 0.5 * (1 + np.cos(np.pi * epoch_in_stage / max_epochs))
        final_lr = stage_lr * (0.1 + 0.9 * cos_factor)  # Ne descend pas sous 10% du LR de base

        for param_group in optimizer.param_groups:
            param_group['lr'] = final_lr

    def get_stage_loss_weights(self, stage: int) -> Dict[str, float]:
        """Retourne les poids de pond√©ration des pertes par √©tape."""
        weights = {
            1: {'mse': 1.0, 'convergence': 2.0, 'stability': 1.0},
            2: {'mse': 1.0, 'convergence': 1.5, 'stability': 1.5, 'adaptation': 1.0},
            3: {'mse': 1.0, 'convergence': 1.0, 'stability': 2.0, 'robustness': 1.5},
            4: {'mse': 1.0, 'convergence': 1.2, 'stability': 2.5, 'robustness': 2.0}  # NOUVEAU
        }
        return weights.get(stage, weights[1])

# =============================================================================
# Cache de s√©quences optimis√© par √©tape (NOUVEAU)
# =============================================================================

class OptimizedSequenceCache:
    """
    Cache sp√©cialis√© par √©tape pour l'entra√Ænement modulaire.
    Maintient des caches s√©par√©s pour chaque √©tape d'apprentissage.
    """

    def __init__(self, simulator: DiffusionSimulator, device: str = cfg.DEVICE):
        self.simulator = simulator
        self.device = device
        self.stage_caches = {}  # Cache par √©tape
        self.cache_sizes = {1: 150, 2: 200, 3: 250}  # Plus de vari√©t√© pour √©tapes complexes
        self.current_indices = {}

    def initialize_stage_cache(self, stage: int):
        """Initialise le cache pour une √©tape sp√©cifique."""
        if stage in self.stage_caches:
            return  # D√©j√† initialis√©

        cache_size = self.cache_sizes.get(stage, 200)
        print(f"üéØ G√©n√©ration de {cache_size} s√©quences pour l'√©tape {stage}...")

        sequences = []
        for i in range(cache_size):
            if i % 50 == 0:
                print(f"   √âtape {stage}: {i}/{cache_size}")

            # CORRECTION : G√©rer le tuple de retour avec 4 √©l√©ments
            target_seq, source_mask, obstacle_mask, _ = self.simulator.generate_stage_sequence(
                stage=stage,
                n_steps=cfg.NCA_STEPS,
                size=cfg.GRID_SIZE
            )

            sequences.append({
                'target_seq': target_seq,
                'source_mask': source_mask,
                'obstacle_mask': obstacle_mask,
                'stage': stage
            })

        self.stage_caches[stage] = sequences
        self.current_indices[stage] = 0
        print(f"‚úÖ Cache √©tape {stage} cr√©√© ({cache_size} s√©quences)")

    def get_stage_batch(self, stage: int, batch_size: int):
        """R√©cup√®re un batch pour l'√©tape sp√©cifi√©e."""
        if stage not in self.stage_caches:
            self.initialize_stage_cache(stage)

        cache = self.stage_caches[stage]
        batch = []

        for _ in range(batch_size):
            batch.append(cache[self.current_indices[stage]])
            self.current_indices[stage] = (self.current_indices[stage] + 1) % len(cache)

        return batch

    def shuffle_stage_cache(self, stage: int):
        """M√©lange le cache d'une √©tape sp√©cifique."""
        if stage in self.stage_caches:
            import random
            random.shuffle(self.stage_caches[stage])

    def clear_stage_cache(self, stage: int):
        """Lib√®re la m√©moire du cache d'une √©tape."""
        if stage in self.stage_caches:
            del self.stage_caches[stage]
            del self.current_indices[stage]
            print(f"üóëÔ∏è  Cache √©tape {stage} lib√©r√©")

# =============================================================================
# Entra√Æneur modulaire principal (NOUVEAU)
# =============================================================================

class ModularTrainer:
    """
    Syst√®me d'entra√Ænement modulaire progressif.
    G√®re l'apprentissage par √©tapes avec transitions automatiques et intensit√©s variables (Version 8__).
    """

    def __init__(self, model: ImprovedNCA, device: str = cfg.DEVICE):
        self.model = model
        self.device = device

        # Choix de l'updater optimis√©
        if cfg.USE_OPTIMIZATIONS and cfg.USE_VECTORIZED_PATCHES:
            print("üöÄ Utilisation de l'updater optimis√© vectoris√©")
            self.updater = OptimizedNCAUpdater(model, device)
        else:
            print("‚ö†Ô∏è  Utilisation de l'updater standard")
            self.updater = NCAUpdater(model, device)

        # Optimiseur et planificateur
        self.optimizer = optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=1e-4)
        self.loss_fn = nn.MSELoss()

        # Curriculum et m√©triques
        if cfg.ENABLE_CURRICULUM:
            self.curriculum = CurriculumScheduler(cfg.CONVERGENCE_THRESHOLDS)
        else:
            self.curriculum = None

        # Cache optimis√© par √©tape
        if cfg.USE_OPTIMIZATIONS and cfg.USE_SEQUENCE_CACHE:
            self.sequence_cache = OptimizedSequenceCache(simulator, device)
            self.use_cache = True
        else:
            self.use_cache = False

        # VERSION 8__ : Gestionnaire d'intensit√©s pour √©tape 4
        self.intensity_manager = SimulationIntensityManager(device)

        # √âtat d'entra√Ænement (MODIFI√â pour inclure √©tape 4)
        self.current_stage = 1
        self.stage_histories = {stage: {'losses': [], 'epochs': [], 'lr': []} for stage in [1, 2, 3, 4]}
        self.global_history = {'losses': [], 'stages': [], 'epochs': []}
        self.stage_start_epochs = {}
        self.total_epochs_trained = 0

    def train_step(self, target_sequence: List[torch.Tensor], source_mask: torch.Tensor,
                   obstacle_mask: torch.Tensor, stage: int, source_intensity: Optional[float] = None) -> float:
        """
        Un pas d'entra√Ænement adapt√© √† l'√©tape courante.

        Args:
            target_sequence: S√©quence cible
            source_mask: Masque des sources
            obstacle_mask: Masque des obstacles
            stage: √âtape courante d'entra√Ænement
            source_intensity: Intensit√© sp√©cifique pour √©tape 4 (VERSION 8__)

        Returns:
            Perte pour ce pas
        """
        self.optimizer.zero_grad()

        # Initialisation avec intensit√© appropri√©e (VERSION 8__)
        grid_pred = torch.zeros_like(target_sequence[0])
        if stage == 4 and source_intensity is not None:
            grid_pred[source_mask] = source_intensity  # Intensit√© variable
        else:
            grid_pred[source_mask] = cfg.SOURCE_INTENSITY  # Intensit√© standard

        total_loss = torch.tensor(0.0, device=self.device)

        # D√©roulement temporel
        for t_step in range(cfg.NCA_STEPS):
            target = target_sequence[t_step + 1]

            # VERSION 8__ : Utilise l'updater avec intensit√© appropri√©e
            if stage == 4 and source_intensity is not None:
                grid_pred = self.updater.step(grid_pred, source_mask, obstacle_mask, source_intensity)
            else:
                grid_pred = self.updater.step(grid_pred, source_mask, obstacle_mask)

            # Perte pond√©r√©e selon l'√©tape
            step_loss = self.loss_fn(grid_pred, target)
            if self.curriculum:
                weights = self.curriculum.get_stage_loss_weights(stage)
                step_loss = step_loss * weights.get('mse', 1.0)

            total_loss = total_loss + step_loss

        avg_loss = total_loss / cfg.NCA_STEPS

        # Backpropagation avec clipping
        avg_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        self.optimizer.step()
        return avg_loss.item()

    def train_stage_4(self, max_epochs: int) -> Dict[str, Any]:
        """
        VERSION 8__ : Entra√Ænement sp√©cialis√© pour l'√©tape 4 avec intensit√©s variables.

        Args:
            max_epochs: Nombre maximum d'√©poques pour cette √©tape

        Returns:
            Dictionnaire avec les m√©triques de l'√©tape
        """
        print(f"\nüéØ === √âTAPE 4 - INTENSIT√âS VARIABLES - D√âBUT ===")
        print(f"üìã Apprentissage avec intensit√©s variables [0.0, 1.0]")
        print(f"‚è±Ô∏è  Maximum {max_epochs} √©poques")

        stage = 4
        self.current_stage = stage
        self.stage_start_epochs[stage] = self.total_epochs_trained

        # M√©triques de l'√©tape
        stage_losses = []
        intensity_stats_history = []
        epoch_in_stage = 0
        early_stop = False

        # Boucle d'entra√Ænement de l'√©tape 4
        for epoch_in_stage in range(max_epochs):
            epoch_losses = []
            epoch_intensities = []
            current_intensity = 0.0  # CORRECTION : Initialisation par d√©faut
            intensity_stats = {'mean': 0.0}  # CORRECTION : Initialisation par d√©faut

            # Ajustement du learning rate si curriculum activ√©
            if self.curriculum:
                # Adaptation pour √©tape 4
                base_lr = cfg.LEARNING_RATE
                stage_lr = base_lr * 0.4  # LR r√©duit pour √©tape 4
                cos_factor = 0.5 * (1 + np.cos(np.pi * epoch_in_stage / max_epochs))
                final_lr = stage_lr * (0.1 + 0.9 * cos_factor)

                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = final_lr

            # Progression dans l'√©tape 4 pour curriculum d'intensit√©
            epoch_progress = epoch_in_stage / max(max_epochs - 1, 1)

            # Entra√Ænement par batch avec intensit√©s variables
            for batch_idx in range(cfg.BATCH_SIZE):
                # VERSION 8__ : √âchantillonne intensit√© pour cette simulation
                current_intensity = self.intensity_manager.sample_simulation_intensity(epoch_progress)
                epoch_intensities.append(current_intensity)

                # G√©n√®re s√©quence avec intensit√© fixe pour cette simulation
                target_seq, source_mask, obstacle_mask, used_intensity = simulator.generate_stage_sequence(
                    stage=4,
                    n_steps=cfg.NCA_STEPS,
                    size=cfg.GRID_SIZE,
                    source_intensity=current_intensity
                )

                # Entra√Æne avec cette intensit√© fixe
                loss = self.train_step(target_seq, source_mask, obstacle_mask, stage, current_intensity)
                epoch_losses.append(loss)



            # Statistiques de l'√©poque
            if epoch_losses:
                avg_epoch_loss = np.mean(epoch_losses)
                stage_losses.append(avg_epoch_loss)
                current_lr = self.optimizer.param_groups[0]['lr']

                # CORRECTION CRITIQUE : Mise √† jour des historiques manquantes pour l'√©tape 4
                self.stage_histories[stage]['losses'].append(avg_epoch_loss)
                self.stage_histories[stage]['epochs'].append(epoch_in_stage)
                self.stage_histories[stage]['lr'].append(current_lr)

                self.global_history['losses'].append(avg_epoch_loss)
                self.global_history['stages'].append(stage)
                self.global_history['epochs'].append(self.total_epochs_trained)

                self.total_epochs_trained += 1  # CRITIQUE : Incr√©mentation manquante

                # Statistiques des intensit√©s de cette √©poque
                if epoch_intensities:
                    intensity_stats = {
                        'mean': np.mean(epoch_intensities),
                        'std': np.std(epoch_intensities),
                        'min': np.min(epoch_intensities),
                        'max': np.max(epoch_intensities),
                        'range': [self.intensity_manager.get_progressive_range(epoch_progress)]
                    }
                    intensity_stats_history.append(intensity_stats)

                # Affichage p√©riodique avec statistiques d'intensit√©
                if epoch_in_stage % 10 == 0 or epoch_in_stage == max_epochs - 1:
                    intensity_range = self.intensity_manager.get_progressive_range(epoch_progress)
                    print(f"  √âpoque {epoch_in_stage:3d}/{max_epochs-1} | "
                          f"Loss: {avg_epoch_loss:.6f} | "
                          f"LR: {current_lr:.2e} | "
                          f"Intensit√©s: [{intensity_range[0]:.2f}, {intensity_range[1]:.2f}] | "
                          f"Moy: {intensity_stats['mean']:.3f}")
                    print(f"    üìä Historique global: {len(self.global_history['losses'])} entr√©es")

                # V√©rification de convergence adapt√©e √† l'√©tape 4
                if (epoch_in_stage >= 15):  # Minimum 15 √©poques pour √©tape 4
                    threshold = cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.025)
                    if avg_epoch_loss < threshold:
                        print(f"üéØ Convergence atteinte √† l'√©poque {epoch_in_stage}")
                        print(f"   Seuil: {threshold:.3f}, Loss: {avg_epoch_loss:.6f}")
                        early_stop = True
                        break
            else:
                print(f"‚ö†Ô∏è √âpoque {epoch_in_stage}: Aucune perte valide calcul√©e")

        # Nettoyage de l'historique des intensit√©s pour √©conomiser la m√©moire
        self.intensity_manager.clear_history()

        # R√©sum√© de l'√©tape avec statistiques d'intensit√©
        final_loss = stage_losses[-1] if stage_losses else float('inf')
        convergence_met = final_loss < cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.025)

        # Statistiques finales des intensit√©s
        global_intensity_stats = self.intensity_manager.get_intensity_statistics()

        stage_metrics = {
            'stage': stage,
            'epochs_trained': epoch_in_stage + 1,
            'final_loss': final_loss,
            'convergence_met': convergence_met,
            'early_stopped': early_stop,
            'loss_history': stage_losses,
            'intensity_stats_history': intensity_stats_history,  # VERSION 8__
            'global_intensity_stats': global_intensity_stats     # VERSION 8__
        }

        print(f"‚úÖ === √âTAPE 4 - TERMIN√âE ===")
        print(f"üìä √âpoques entra√Æn√©es: {epoch_in_stage + 1}/{max_epochs}")
        print(f"üìâ Perte finale: {final_loss:.6f}")
        print(f"üéØ Convergence: {'‚úÖ OUI' if convergence_met else '‚ùå NON'}")
        print(f"‚ö° Arr√™t pr√©coce: {'‚úÖ OUI' if early_stop else '‚ùå NON'}")
        print(f"üî¢ Intensit√©s utilis√©es: {global_intensity_stats['count']} "
              f"(moy: {global_intensity_stats['mean']:.3f}, "
              f"plage: [{global_intensity_stats['min']:.3f}, {global_intensity_stats['max']:.3f}])")

        # Sauvegarde du checkpoint d'√©tape
        if cfg.SAVE_STAGE_CHECKPOINTS:
            self.save_stage_checkpoint(stage, stage_metrics)

        return stage_metrics

    def train_stage(self, stage: int, max_epochs: int) -> Dict[str, Any]:
        """
        Entra√Ænement complet d'une √©tape sp√©cifique (VERSION 8__ : d√©l√®gue √©tape 4).
        """
        if stage == 4:
            # VERSION 8__ : √âtape 4 utilise la m√©thode sp√©cialis√©e
            return self.train_stage_4(max_epochs)

        # √âtapes 1-3 : comportement original
        print(f"\nüéØ === √âTAPE {stage} - D√âBUT ===")
        stage_name = {1: "Sans obstacles", 2: "Un obstacle", 3: "Obstacles multiples"}[stage]
        print(f"üìã {stage_name}")
        print(f"‚è±Ô∏è  Maximum {max_epochs} √©poques")

        self.current_stage = stage
        self.stage_start_epochs[stage] = self.total_epochs_trained

        # Initialisation du cache pour cette √©tape
        if self.use_cache and stage <= 3:  # Cache seulement pour √©tapes 1-3
            self.sequence_cache.initialize_stage_cache(stage)

        # M√©triques de l'√©tape
        stage_losses = []
        epoch_in_stage = 0
        early_stop = False

        # Boucle d'entra√Ænement de l'√©tape
        for epoch_in_stage in range(max_epochs):
            epoch_losses = []

            # Ajustement du learning rate si curriculum activ√©
            if self.curriculum:
                self.curriculum.adjust_learning_rate(self.optimizer, stage, epoch_in_stage)

            # M√©lange p√©riodique du cache
            if self.use_cache and epoch_in_stage % 20 == 0:
                self.sequence_cache.shuffle_stage_cache(stage)

            # Entra√Ænement par batch
            for _ in range(cfg.BATCH_SIZE):
                if self.use_cache:
                    batch_sequences = self.sequence_cache.get_stage_batch(stage, 1)
                    seq_data = batch_sequences[0]
                    target_seq = seq_data['target_seq']
                    source_mask = seq_data['source_mask']
                    obstacle_mask = seq_data['obstacle_mask']
                else:
                    target_seq, source_mask, obstacle_mask, _ = simulator.generate_stage_sequence(
                        stage=stage, n_steps=cfg.NCA_STEPS, size=cfg.GRID_SIZE
                    )

                loss = self.train_step(target_seq, source_mask, obstacle_mask, stage)
                epoch_losses.append(loss)

            # Statistiques de l'√©poque
            avg_epoch_loss = np.mean(epoch_losses)
            stage_losses.append(avg_epoch_loss)
            current_lr = self.optimizer.param_groups[0]['lr']

            # Historiques
            self.stage_histories[stage]['losses'].append(avg_epoch_loss)
            self.stage_histories[stage]['epochs'].append(epoch_in_stage)
            self.stage_histories[stage]['lr'].append(current_lr)

            self.global_history['losses'].append(avg_epoch_loss)
            self.global_history['stages'].append(stage)
            self.global_history['epochs'].append(self.total_epochs_trained)

            self.total_epochs_trained += 1

            # Affichage p√©riodique
            if epoch_in_stage % 10 == 0 or epoch_in_stage == max_epochs - 1:
                print(f"  √âpoque {epoch_in_stage:3d}/{max_epochs-1} | "
                      f"Loss: {avg_epoch_loss:.6f} | "
                      f"LR: {current_lr:.2e}")

            # V√©rification de l'avancement automatique (curriculum)
            if (self.curriculum and cfg.ADAPTIVE_THRESHOLDS and
                epoch_in_stage >= 10):  # Minimum 10 √©poques par √©tape
                if self.curriculum.should_advance_stage(stage, stage_losses):
                    print(f"üéØ Convergence atteinte √† l'√©poque {epoch_in_stage}")
                    print(f"   Seuil: {cfg.CONVERGENCE_THRESHOLDS[stage]:.3f}, "
                          f"Loss: {avg_epoch_loss:.6f}")
                    early_stop = True
                    break

        # R√©sum√© de l'√©tape
        final_loss = stage_losses[-1] if stage_losses else float('inf')
        convergence_met = final_loss < cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05)

        stage_metrics = {
            'stage': stage,
            'epochs_trained': epoch_in_stage + 1,
            'final_loss': final_loss,
            'convergence_met': convergence_met,
            'early_stopped': early_stop,
            'loss_history': stage_losses
        }

        print(f"‚úÖ === √âTAPE {stage} - TERMIN√âE ===")
        print(f"üìä √âpoques entra√Æn√©es: {epoch_in_stage + 1}/{max_epochs}")
        print(f"üìâ Perte finale: {final_loss:.6f}")
        print(f"üéØ Convergence: {'‚úÖ OUI' if convergence_met else '‚ùå NON'}")
        print(f"‚ö° Arr√™t pr√©coce: {'‚úÖ OUI' if early_stop else '‚ùå NON'}")

        # Sauvegarde du checkpoint d'√©tape
        if cfg.SAVE_STAGE_CHECKPOINTS:
            self.save_stage_checkpoint(stage, stage_metrics)

        # Lib√©ration du cache de l'√©tape pr√©c√©dente pour √©conomiser la m√©moire
        if self.use_cache and stage > 1:
            self.sequence_cache.clear_stage_cache(stage - 1)

        return stage_metrics

    def train_full_curriculum(self) -> Dict[str, Any]:
        """
        Entra√Ænement complet du curriculum en 4 √©tapes.

        Returns:
            M√©triques compl√®tes de l'entra√Ænement modulaire
        """
        print(f"\nüöÄ === D√âBUT ENTRA√éNEMENT MODULAIRE ===")
        print(f"üéØ Seed: {cfg.SEED}")
        print(f"üìä √âpoques totales pr√©vues: {cfg.TOTAL_EPOCHS}")
        print(f"üîÑ √âtapes: {cfg.STAGE_1_EPOCHS} + {cfg.STAGE_2_EPOCHS} + {cfg.STAGE_3_EPOCHS} + {cfg.STAGE_4_EPOCHS}")
        print(f"üß† Curriculum: {'‚úÖ Activ√©' if cfg.ENABLE_CURRICULUM else '‚ùå D√©sactiv√©'}")

        start_time = time.time()
        self.model.train()

        # Entra√Ænement s√©quentiel des 4 √©tapes
        all_stage_metrics = {}

        # √âTAPE 1: Sans obstacles
        stage_1_metrics = self.train_stage(1, cfg.STAGE_1_EPOCHS)
        all_stage_metrics[1] = stage_1_metrics

        # √âTAPE 2: Un obstacle
        stage_2_metrics = self.train_stage(2, cfg.STAGE_2_EPOCHS)
        all_stage_metrics[2] = stage_2_metrics

        # √âTAPE 3: Obstacles multiples
        stage_3_metrics = self.train_stage(3, cfg.STAGE_3_EPOCHS)
        all_stage_metrics[3] = stage_3_metrics

        # √âTAPE 4: Intensit√©s variables
        stage_4_metrics = self.train_stage(4, cfg.STAGE_4_EPOCHS)
        all_stage_metrics[4] = stage_4_metrics

        # M√©triques globales
        total_time = time.time() - start_time
        total_epochs_actual = sum(metrics['epochs_trained'] for metrics in all_stage_metrics.values())

        global_metrics = {
            'total_epochs_planned': cfg.TOTAL_EPOCHS,
            'total_epochs_actual': total_epochs_actual,
            'total_time_seconds': total_time,
            'total_time_formatted': f"{total_time/60:.1f} min",
            'stage_metrics': all_stage_metrics,
            'final_loss': stage_4_metrics['final_loss'],
            'all_stages_converged': all(m['convergence_met'] for m in all_stage_metrics.values()),
            'global_history': self.global_history,
            'stage_histories': self.stage_histories,
            'stage_start_epochs': self.stage_start_epochs  # AJOUT de la cl√© manquante
        }

        print(f"\nüéâ === ENTRA√éNEMENT MODULAIRE TERMIN√â ===")
        print(f"‚è±Ô∏è  Temps total: {total_time/60:.1f} minutes")
        print(f"üìä √âpoques totales: {total_epochs_actual}/{cfg.TOTAL_EPOCHS}")
        print(f"üéØ Toutes √©tapes converg√©es: {'‚úÖ OUI' if global_metrics['all_stages_converged'] else '‚ùå NON'}")
        print(f"üìâ Perte finale: {global_metrics['final_loss']:.6f}")

        # Sauvegarde du mod√®le final et des m√©triques
        self.save_final_model(global_metrics)

        return global_metrics

    def save_stage_checkpoint(self, stage: int, metrics: Dict[str, Any]):
        """Sauvegarde le checkpoint d'une √©tape."""
        stage_dir = Path(cfg.OUTPUT_DIR) / f"stage_{stage}"
        stage_dir.mkdir(parents=True, exist_ok=True)

        # Sauvegarde du mod√®le
        model_path = stage_dir / "model_checkpoint.pth"
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'stage': stage,
            'metrics': metrics,
            'config': cfg.__dict__
        }, model_path)

        # Sauvegarde des m√©triques en JSON
        metrics_path = stage_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2, default=str)

        print(f"üíæ Checkpoint √©tape {stage} sauvegard√©: {stage_dir}")

    def save_final_model(self, global_metrics: Dict[str, Any]):
        """Sauvegarde le mod√®le final et toutes les m√©triques."""
        # Mod√®le final
        final_model_path = Path(cfg.OUTPUT_DIR) / "final_model.pth"
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'global_metrics': global_metrics,
            'config': cfg.__dict__
        }, final_model_path)

        # M√©triques compl√®tes
        full_metrics_path = Path(cfg.OUTPUT_DIR) / "complete_metrics.json"
        with open(full_metrics_path, 'w') as f:
            json.dump(global_metrics, f, indent=2, default=str)

        print(f"üíæ Mod√®le final et m√©triques sauvegard√©s: {cfg.OUTPUT_DIR}")

# =============================================================================
# Syst√®me de visualisation progressive (NOUVEAU)
# =============================================================================

class ProgressiveVisualizer:
    """
    Syst√®me de visualisation avanc√© pour l'apprentissage modulaire.
    G√©n√®re des animations et graphiques comparatifs par √©tape.
    """

    def __init__(self, interactive: bool = interactive_mode):
        self.interactive = interactive
        self.frame_data = {}  # Donn√©es par √©tape

    def visualize_stage_results(self, model: ImprovedNCA, stage: int,
                              vis_seed: int = args.vis_seed) -> Dict[str, Any]:
        """
        Visualise les r√©sultats d'une √©tape sp√©cifique.

        Args:
            model: Mod√®le NCA entra√Æn√©
            stage: Num√©ro d'√©tape √† visualiser
            vis_seed: Graine pour les visualisations

        Returns:
            Dictionnaire avec les donn√©es de visualisation
        """
        print(f"\nüé® G√©n√©ration des visualisations pour l'√©tape {stage}...")

        # G√©n√©ration de la s√©quence de test avec seed fixe
        torch.manual_seed(vis_seed)
        np.random.seed(vis_seed)

        # Pour l'√©tape 4, √©chantillonner une intensit√© variable pour la visualisation
        source_intensity = None
        if stage == 4:
            # Utilise une intensit√© moyenne pour la visualisation (progression √† 0.5)
            intensity_manager = SimulationIntensityManager(cfg.DEVICE)
            source_intensity = intensity_manager.sample_simulation_intensity(0.5)  # Milieu de l'√©tape 4
            source_intensity = 0.2

        # G√©rer le tuple de retour avec 4 √©l√©ments et r√©cup√©rer l'intensit√© utilis√©e
        target_seq, source_mask, obstacle_mask, used_intensity = simulator.generate_stage_sequence(
            stage=stage,
            n_steps=cfg.POSTVIS_STEPS,
            size=cfg.GRID_SIZE,
            seed=vis_seed,
            source_intensity=source_intensity
        )

        # Pr√©diction du mod√®le
        model.eval()
        updater = OptimizedNCAUpdater(model, cfg.DEVICE) if cfg.USE_OPTIMIZATIONS else NCAUpdater(model, cfg.DEVICE)

        # Simulation NCA avec torch.no_grad() pour √©viter le gradient
        nca_sequence = []
        grid_pred = torch.zeros_like(target_seq[0])
        grid_pred[source_mask] = used_intensity  # Utilise l'intensit√© r√©elle
        nca_sequence.append(grid_pred.clone())

        with torch.no_grad():  # D√©sactive le calcul de gradient pour les visualisations
            for _ in range(cfg.POSTVIS_STEPS):
                # Utilise toujours l'intensit√© appropri√©e pour chaque √©tape
                grid_pred = updater.step(grid_pred, source_mask, obstacle_mask, used_intensity if stage == 4 else None)
                nca_sequence.append(grid_pred.clone())

        # Cr√©ation des visualisations avec .detach() pour s√©curit√©
        vis_data = {
            'stage': stage,
            'target_sequence': [t.detach().cpu().numpy() for t in target_seq],
            'nca_sequence': [t.detach().cpu().numpy() for t in nca_sequence],
            'source_mask': source_mask.detach().cpu().numpy(),
            'obstacle_mask': obstacle_mask.detach().cpu().numpy(),
            'vis_seed': vis_seed,
            'source_intensity': used_intensity  # Intensit√© r√©elle utilis√©e pour toutes les √©tapes
        }

        # Sauvegarde des animations
        self._create_stage_animations(vis_data)
        self._create_stage_convergence_plot(vis_data)

        model.train()
        return vis_data

    def _create_stage_animations(self, vis_data: Dict[str, Any]):
        """Cr√©e les animations GIF pour une √©tape."""
        stage = vis_data['stage']
        stage_dir = Path(cfg.OUTPUT_DIR) / f"stage_{stage}"
        stage_dir.mkdir(parents=True, exist_ok=True)

        # Animation comparative
        self._save_comparison_gif(
            vis_data['target_sequence'],
            vis_data['nca_sequence'],
            vis_data['obstacle_mask'],
            stage_dir / f"animation_comparaison_√©tape_{stage}.gif",
            f"√âtape {stage} - Comparaison Cible vs NCA",
            vis_data['source_intensity']  # AJOUT : intensit√© pour le titre
        )

        # Animation NCA seule
        self._save_single_gif(
            vis_data['nca_sequence'],
            vis_data['obstacle_mask'],
            stage_dir / f"animation_nca_√©tape_{stage}.gif",
            f"√âtape {stage} - Pr√©diction NCA",
            vis_data['source_intensity']  # AJOUT : intensit√© pour le titre
        )

        print(f"‚úÖ Animations √©tape {stage} sauvegard√©es dans {stage_dir}")

    def _create_stage_convergence_plot(self, vis_data: Dict[str, Any]):
        """Cr√©e le graphique de convergence pour une √©tape."""
        stage = vis_data['stage']
        stage_dir = Path(cfg.OUTPUT_DIR) / f"stage_{stage}"

        target_seq = vis_data['target_sequence']
        nca_seq = vis_data['nca_sequence']

        # Calcul de l'erreur temporelle
        errors = []
        for t in range(min(len(target_seq), len(nca_seq))):
            error = np.mean((target_seq[t] - nca_seq[t]) ** 2)
            errors.append(error)

        # Graphique
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(errors, 'b-', linewidth=2, label='Erreur MSE')
        ax.axhline(y=cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05),
                  color='r', linestyle='--', label=f'Seuil convergence √©tape {stage}')

        ax.set_xlabel('Pas de temps')
        ax.set_ylabel('Erreur MSE')
        ax.set_title(f'Convergence √âtape {stage} - Seed {vis_data["vis_seed"]}')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        convergence_path = stage_dir / f"convergence_√©tape_{stage}.png"
        plt.savefig(convergence_path, dpi=150, bbox_inches='tight')
        plt.close()

        print(f"‚úÖ Graphique de convergence √©tape {stage} sauvegard√©: {convergence_path}")

    def _save_comparison_gif(self, target_seq: List[np.ndarray], nca_seq: List[np.ndarray],
                            obstacle_mask: np.ndarray, filepath: Path, title: str, source_intensity: float = 1.0):
        """Sauvegarde un GIF de comparaison c√¥te √† c√¥te."""
        import matplotlib.animation as animation

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        def animate(frame):
            ax1.clear()
            ax2.clear()

            # Cible
            im1 = ax1.imshow(target_seq[frame], cmap='hot', vmin=0, vmax=1)
            ax1.contour(obstacle_mask, levels=[0.5], colors='cyan', linewidths=2)
            ax1.set_title(f'Cible - t={frame} (I={source_intensity:.3f})')
            ax1.set_xticks([])
            ax1.set_yticks([])

            # NCA
            im2 = ax2.imshow(nca_seq[frame], cmap='hot', vmin=0, vmax=1)
            ax2.contour(obstacle_mask, levels=[0.5], colors='cyan', linewidths=2)
            ax2.set_title(f'NCA - t={frame} (I={source_intensity:.3f})')
            ax2.set_xticks([])
            ax2.set_yticks([])

            return [im1, im2]

        n_frames = min(len(target_seq), len(nca_seq))
        ani = animation.FuncAnimation(fig, animate, frames=n_frames, interval=200, blit=False)
        ani.save(filepath, writer='pillow', fps=5)
        plt.close()

    def _save_single_gif(self, sequence: List[np.ndarray], obstacle_mask: np.ndarray,
                        filepath: Path, title: str, source_intensity: float = 1.0):
        """Sauvegarde un GIF d'une s√©quence unique."""
        import matplotlib.animation as animation

        fig, ax = plt.subplots(figsize=(8, 8))

        def animate(frame):
            ax.clear()
            im = ax.imshow(sequence[frame], cmap='hot', vmin=0, vmax=1)
            ax.contour(obstacle_mask, levels=[0.5], colors='cyan', linewidths=2)
            ax.set_title(f'{title} - t={frame} (I={source_intensity:.3f})')
            ax.set_xticks([])
            ax.set_yticks([])
            return [im]

        ani = animation.FuncAnimation(fig, animate, frames=len(sequence), interval=200, blit=False)
        ani.save(filepath, writer='pillow', fps=5)
        plt.close()

    def create_curriculum_summary(self, global_metrics: Dict[str, Any]):
        """Cr√©e un r√©sum√© visuel complet du curriculum d'apprentissage."""
        print("\nüé® G√©n√©ration du r√©sum√© visuel du curriculum...")

        # Graphique de progression globale
        self._plot_curriculum_progression(global_metrics)

        # Comparaison inter-√©tapes
        self._plot_stage_comparison(global_metrics)

        # M√©triques de performance
        self._plot_performance_metrics(global_metrics)

        print("‚úÖ R√©sum√© visuel complet g√©n√©r√©")

    def _plot_curriculum_progression(self, metrics: Dict[str, Any]):
        """Graphique de la progression globale du curriculum."""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))

        # Historique des pertes avec codes couleur par √©tape
        losses = metrics['global_history']['losses']
        stages = metrics['global_history']['stages']
        epochs = metrics['global_history']['epochs']

        stage_colors = {1: 'green', 2: 'orange', 3: 'red', 4: 'purple'}

        for stage in [1, 2, 3, 4]:
            stage_indices = [i for i, s in enumerate(stages) if s == stage]
            stage_losses = [losses[i] for i in stage_indices]
            stage_epochs = [epochs[i] for i in stage_indices]

            if stage_losses:
                ax1.plot(stage_epochs, stage_losses,
                        color=stage_colors[stage],
                        label=f'√âtape {stage}',
                        linewidth=2)

        # Seuils de convergence
        for stage in [1, 2, 3, 4]:
            threshold = cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05)
            ax1.axhline(y=threshold, color=stage_colors[stage],
                       linestyle='--', alpha=0.7,
                       label=f'Seuil √©tape {stage}')

        ax1.set_xlabel('√âpoque')
        ax1.set_ylabel('Perte MSE')
        ax1.set_title('Progression du Curriculum d\'Apprentissage')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')

        # Learning rate par √©tape
        for stage in [1, 2, 3, 4]:
            stage_history = metrics['stage_histories'][stage]
            if stage_history['lr']:
                stage_epochs_local = [metrics['stage_start_epochs'].get(stage, 0) + e
                                    for e in stage_history['epochs']]
                ax2.plot(stage_epochs_local, stage_history['lr'],
                        color=stage_colors[stage],
                        label=f'LR √âtape {stage}',
                        linewidth=2)

        ax2.set_xlabel('√âpoque')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('√âvolution du Learning Rate par √âtape')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')

        plt.tight_layout()
        plt.savefig(Path(cfg.OUTPUT_DIR) / "curriculum_progression.png",
                   dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_stage_comparison(self, metrics: Dict[str, Any]):
        """Graphique de comparaison entre √©tapes."""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        stages = [1, 2, 3, 4]
        stage_names = ["Sans obstacles", "Un obstacle", "Obstacles multiples", "Intensit√©s variables"]
        stage_colors = ['green', 'orange', 'red', 'purple']

        # Pertes finales par √©tape
        final_losses = [metrics['stage_metrics'][s]['final_loss'] for s in stages]
        convergence_status = [metrics['stage_metrics'][s]['convergence_met'] for s in stages]

        bars = ax1.bar(stage_names, final_losses, color=stage_colors, alpha=0.7)
        for i, (bar, converged) in enumerate(zip(bars, convergence_status)):
            if converged:
                bar.set_edgecolor('darkgreen')
                bar.set_linewidth(3)

        ax1.set_ylabel('Perte finale')
        ax1.set_title('Perte Finale par √âtape')
        ax1.set_yscale('log')

        # √âpoques utilis√©es par √©tape
        epochs_used = [metrics['stage_metrics'][s]['epochs_trained'] for s in stages]
        epochs_planned = [cfg.STAGE_1_EPOCHS, cfg.STAGE_2_EPOCHS, cfg.STAGE_3_EPOCHS, cfg.STAGE_4_EPOCHS]

        x = np.arange(len(stages))
        width = 0.35

        ax2.bar(x - width/2, epochs_planned, width, label='Pr√©vues', alpha=0.7, color='lightblue')
        ax2.bar(x + width/2, epochs_used, width, label='Utilis√©es', alpha=0.7, color='darkblue')

        ax2.set_xlabel('√âtape')
        ax2.set_ylabel('Nombre d\'√©poques')
        ax2.set_title('√âpoques Pr√©vues vs Utilis√©es')
        ax2.set_xticks(x)
        ax2.set_xticklabels(stage_names, rotation=15)
        ax2.legend()

        # Temps de convergence
        convergence_times = []
        for stage in stages:
            stage_losses = metrics['stage_metrics'][stage]['loss_history']
            threshold = cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05)

            convergence_epoch = None
            for i, loss in enumerate(stage_losses):
                if loss < threshold:
                    convergence_epoch = i
                    break

            convergence_times.append(convergence_epoch if convergence_epoch else len(stage_losses))

        ax3.plot(stages, convergence_times, 'o-', linewidth=2, markersize=8, color='purple')
        ax3.set_xlabel('√âtape')
        ax3.set_ylabel('√âpoque de convergence')
        ax3.set_title('Vitesse de Convergence par √âtape')
        ax3.grid(True, alpha=0.3)

        # Efficacit√© (convergence / √©poques utilis√©es)
        efficiency = [(1.0 if convergence_status[i] else 0.5) / max(epochs_used[i], 1)
                     for i in range(len(stages))]

        ax4.bar(stage_names, efficiency, color=stage_colors, alpha=0.7)
        ax4.set_ylabel('Efficacit√© (convergence/√©poque)')
        ax4.set_title('Efficacit√© d\'Apprentissage par √âtape')

        plt.tight_layout()
        plt.savefig(Path(cfg.OUTPUT_DIR) / "stage_comparison.png",
                   dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_performance_metrics(self, metrics: Dict[str, Any]):
        """Graphique des m√©triques de performance globales."""
        fig, ax = plt.subplots(figsize=(12,  8))

        # R√©sum√© textuel des performances
        total_time = metrics['total_time_seconds']
        total_epochs = metrics['total_epochs_actual']
        all_converged = metrics['all_stages_converged']
        final_loss = metrics['final_loss']

        summary_text = f"""
üéØ R√âSUM√â ENTRA√éNEMENT MODULAIRE NCA v7__

üìä STATISTIQUES GLOBALES:
   ‚Ä¢ Seed: {cfg.SEED}
   ‚Ä¢ Temps total: {total_time/60:.1f} minutes ({total_time:.1f}s)
   ‚Ä¢ √âpoques totales: {total_epochs}/{cfg.TOTAL_EPOCHS}
   ‚Ä¢ Toutes √©tapes converg√©es: {'‚úÖ OUI' if all_converged else '‚ùå NON'}
   ‚Ä¢ Perte finale: {final_loss:.6f}

üèÜ PERFORMANCE PAR √âTAPE:"""

        for stage in [1, 2, 3, 4]:
            stage_data = metrics['stage_metrics'][stage]
            stage_name = {1: "Sans obstacles", 2: "Un obstacle", 3: "Obstacles multiples", 4: "Intensit√©s variables"}[stage]

            summary_text += f"""
   ‚Ä¢ √âtape {stage} ({stage_name}):
     - √âpoques: {stage_data['epochs_trained']} (converg√©e: {'‚úÖ' if stage_data['convergence_met'] else '‚ùå'})
     - Perte finale: {stage_data['final_loss']:.6f}
     - Arr√™t pr√©coce: {'‚úÖ' if stage_data['early_stopped'] else '‚ùå'}"""

        summary_text += f"""

‚öôÔ∏è  CONFIGURATION:
   ‚Ä¢ Curriculum learning: {'‚úÖ' if cfg.ENABLE_CURRICULUM else '‚ùå'}
   ‚Ä¢ Seuils adaptatifs: {'‚úÖ' if cfg.ADAPTIVE_THRESHOLDS else '‚ùå'}
   ‚Ä¢ Cache optimis√©: {'‚úÖ' if cfg.USE_SEQUENCE_CACHE else '‚ùå'}
   ‚Ä¢ Updater vectoris√©: {'‚úÖ' if cfg.USE_VECTORIZED_PATCHES else '‚ùå'}

üìà ARCHITECTURE:
   ‚Ä¢ Taille grille: {cfg.GRID_SIZE}x{cfg.GRID_SIZE}
   ‚Ä¢ Couches cach√©es: {cfg.HIDDEN_SIZE} neurones, {cfg.N_LAYERS} couches
   ‚Ä¢ Pas temporels NCA: {cfg.NCA_STEPS}
   ‚Ä¢ Taille batch: {cfg.BATCH_SIZE}
        """

        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,
               fontsize=10, verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))

        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('R√©sum√© Performance Entra√Ænement Modulaire NCA v7__',
                    fontsize=14, fontweight='bold')

        plt.tight_layout()
        plt.savefig(Path(cfg.OUTPUT_DIR) / "performance_summary.png",
                   dpi=150, bbox_inches='tight')
        plt.close()

# =============================================================================
# Gestionnaire d'intensit√©s variables (NOUVEAU pour Version 8__)
# =============================================================================

class SimulationIntensityManager:
    """
    Gestionnaire des intensit√©s variables pour l'√©tape 4.
    √âchantillonne et g√®re les intensit√©s selon un curriculum progressif.
    """

    def __init__(self, device: str = cfg.DEVICE):
        self.device = device
        self.intensity_history = []

    def sample_simulation_intensity(self, epoch_progress: float) -> float:
        """
        √âchantillonne une intensit√© selon l'avancement de l'entra√Ænement.

        Args:
            epoch_progress: Progression dans l'√©tape 4 (0.0 √† 1.0)

        Returns:
            Intensit√© √©chantillonn√©e pour cette simulation
        """
        intensity_range = self.get_progressive_range(epoch_progress)

        # √âchantillonnage uniforme dans la plage progressive
        min_intensity, max_intensity = intensity_range
        intensity = min_intensity + (max_intensity - min_intensity) * torch.rand(1, device=self.device).item()

        # Validation et ajustement si n√©cessaire
        intensity = self.validate_intensity(intensity)

        # Historique pour statistics et debugging
        self.intensity_history.append(intensity)

        return intensity

    def get_progressive_range(self, epoch_progress: float) -> Tuple[float, float]:
        """
        Calcule la plage d'intensit√© progressive selon l'avancement.

        Args:
            epoch_progress: Progression (0.0 = d√©but √©tape 4, 1.0 = fin √©tape 4)

        Returns:
            (min_intensity, max_intensity) pour cette progression
        """
        initial_range = cfg.STAGE_4_SOURCE_CONFIG['initial_range']  # [0.5, 1.0]
        final_range = cfg.STAGE_4_SOURCE_CONFIG['final_range']      # [0.0, 1.0]

        # Interpolation lin√©aire entre plages initiale et finale
        min_intensity = initial_range[0] + epoch_progress * (final_range[0] - initial_range[0])
        max_intensity = initial_range[1] + epoch_progress * (final_range[1] - initial_range[1])

        return (min_intensity, max_intensity)

    def validate_intensity(self, intensity: float) -> float:
        """
        Valide et ajuste une intensit√© si n√©cessaire.

        Args:
            intensity: Intensit√© √† valider

        Returns:
            Intensit√© valid√©e et ajust√©e
        """
        # Assure que l'intensit√© est dans [0.0, 1.0]
        intensity = max(0.0, min(1.0, intensity))

        # √âvite les intensit√©s quasi-nulles probl√©matiques sauf si exactement 0.0
        if 0.0 < intensity < 0.001:
            intensity = 0.001

        return intensity

    def get_intensity_statistics(self) -> Dict[str, float]:
        """
        Retourne les statistiques des intensit√©s utilis√©es.

        Returns:
            Dictionnaire avec les statistiques
        """
        if not self.intensity_history:
            return {'count': 0, 'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}

        intensities = np.array(self.intensity_history)

        return {
            'count': len(intensities),
            'mean': float(np.mean(intensities)),
            'std': float(np.std(intensities)),
            'min': float(np.min(intensities)),
            'max': float(np.max(intensities)),
            'median': float(np.median(intensities))
        }

    def clear_history(self):
        """Efface l'historique pour √©conomiser la m√©moire."""
        # Garde seulement les 1000 derni√®res intensit√©s
        if len(self.intensity_history) > 1000:
            self.intensity_history = self.intensity_history[-1000:]

# =============================================================================
# Fonction principale d'ex√©cution (NOUVEAU)
# =============================================================================

def main():
    """
    Fonction principale pour l'entra√Ænement modulaire progressif.
    Orchestre tout le processus d'apprentissage par curriculum.
    """
    print(f"\n" + "="*80)
    print(f"üöÄ NEURAL CELLULAR AUTOMATON - APPRENTISSAGE MODULAIRE v7__")
    print(f"="*80)

    try:
        # Initialisation du mod√®le
        print("\nüîß Initialisation du mod√®le...")
        model = ImprovedNCA(
            input_size=11,  # 9 (patch 3x3) + 1 (source) + 1 (obstacle)
            hidden_size=cfg.HIDDEN_SIZE,
            n_layers=cfg.N_LAYERS
        ).to(cfg.DEVICE)

        print(f"üìä Param√®tres du mod√®le: {sum(p.numel() for p in model.parameters()):,}")

        # Initialisation de l'entra√Æneur modulaire
        print("üéØ Initialisation de l'entra√Æneur modulaire...")
        trainer = ModularTrainer(model, cfg.DEVICE)

        # Lancement de l'entra√Ænement complet
        print("üöÄ Lancement de l'entra√Ænement modulaire...")
        global_metrics = trainer.train_full_curriculum()

        # G√©n√©ration des visualisations progressives
        print("\nüé® G√©n√©ration des visualisations...")
        visualizer = ProgressiveVisualizer(interactive_mode)

        # Visualisation par √©tape avec le mod√®le final
        for stage in [1, 2, 3, 4]:
            stage_vis = visualizer.visualize_stage_results(model, stage, args.vis_seed)

        # R√©sum√© visuel complet du curriculum
        visualizer.create_curriculum_summary(global_metrics)

        # Rapport final
        print(f"\n" + "="*80)
        print(f"üéâ ENTRA√éNEMENT MODULAIRE TERMIN√â AVEC SUCC√àS!")
        print(f"="*80)
        print(f"üìÅ R√©sultats sauvegard√©s dans: {cfg.OUTPUT_DIR}")
        print(f"‚è±Ô∏è  Temps total: {global_metrics['total_time_formatted']}")
        print(f"üìä √âpoques: {global_metrics['total_epochs_actual']}/{global_metrics['total_epochs_planned']}")
        print(f"üéØ Convergence: {'‚úÖ TOUTES' if global_metrics['all_stages_converged'] else '‚ùå PARTIELLE'}")
        print(f"üìâ Perte finale: {global_metrics['final_loss']:.6f}")

        # D√©tail par √©tape
        print(f"\nüìã D√âTAIL PAR √âTAPE:")
        for stage in [1, 2, 3, 4]:
            stage_data = global_metrics['stage_metrics'][stage]
            stage_name = {1: "Sans obstacles", 2: "Un obstacle", 3: "Obstacles multiples", 4: "Intensit√©s variables"}[stage]
            status = "‚úÖ CONVERG√âE" if stage_data['convergence_met'] else "‚ùå NON CONVERG√âE"
            print(f"   √âtape {stage} ({stage_name}): {status} - {stage_data['final_loss']:.6f}")

        print(f"\nüé® Fichiers de visualisation g√©n√©r√©s:")
        print(f"   ‚Ä¢ Animations par √©tape: stage_X/")
        print(f"   ‚Ä¢ Progression curriculum: curriculum_progression.png")
        print(f"   ‚Ä¢ Comparaison √©tapes: stage_comparison.png")
        print(f"   ‚Ä¢ R√©sum√© performance: performance_summary.png")
        print(f"   ‚Ä¢ M√©triques compl√®tes: complete_metrics.json")

        # VERSION 8__ : G√©n√©ration de la suite compl√®te de visualisations √©tendues
        print(f"\nüé® G√©n√©ration de la suite compl√®te de visualisations v8__...")

        # Pr√©paration des m√©triques d'intensit√© pour l'√©tape 4
        if 'stage_metrics' in global_metrics and 4 in global_metrics['stage_metrics']:
            stage_4_data = global_metrics['stage_metrics'][4]

            # Extraction des m√©triques d'intensit√© si disponibles
            intensity_metrics = {}
            if 'intensity_stats_history' in stage_4_data:
                intensity_metrics['stage_4_metrics'] = {
                    'intensity_history': trainer.intensity_manager.intensity_history,
                    'performance_by_intensity': {
                        'intensities': trainer.intensity_manager.intensity_history,
                        'losses': stage_4_data.get('loss_history', [])
                    }
                }

            # Ajout des m√©triques d'intensit√© aux m√©triques globales
            global_metrics['intensity_metrics'] = intensity_metrics

        # Appel de la suite de visualisation compl√®te
        create_complete_visualization_suite(model, global_metrics, simulator, cfg)

        return global_metrics

    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur")
        return None
    except Exception as e:
        print(f"\n‚ùå ERREUR lors de l'entra√Ænement:")
        print(f"   {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    # Ex√©cution du programme principal
    results = main()

    if results is not None:
        print(f"\nüéØ Programme termin√© avec succ√®s!")
        print(f"üìä R√©sultats disponibles dans la variable 'results'")
    else:
        print(f"\n‚ùå Programme termin√© avec erreurs")
        exit(1)
