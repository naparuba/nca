import time
from pathlib import Path
from typing import TYPE_CHECKING, List

import numpy as np
import torch
from torch import optim as optim, nn as nn

from config import CONFIG, DEVICE
from nca_model import NCA
from sequences import OptimizedSequenceCache
from stage_manager import STAGE_MANAGER

if TYPE_CHECKING:
    from stages.base_stage import BaseStage
    from sequence import SimulationSequence


class Trainer:
    """
    SystÃ¨me d'entraÃ®nement modulaire progressif.
    GÃ¨re l'apprentissage par Ã©tapes avec transitions automatiques.
    """
    
    
    def __init__(self, model: NCA):
        self._model = model
        
        # Choix de l'updater optimisÃ©
        print("ğŸš€ Utilisation de l'updater optimisÃ© vectorisÃ©")
        
        # Optimiseur et planificateur
        self._optimizer = optim.AdamW(model.parameters(), lr=CONFIG.LEARNING_RATE, weight_decay=1e-4)
        self._loss_fn = nn.MSELoss()
        
        # Cache optimisÃ© par Ã©tape
        self._sequence_cache = OptimizedSequenceCache()
    
    
    def _train_step(self, sequence):
        # type: (SimulationSequence) -> float
        
        """
        Un pas d'entraÃ®nement adaptÃ© Ã  l'Ã©tape courante.

        Args:
            sequence: Sequence d'entraÃ®nement
        Returns:
            Perte pour ce pas
        """
        
        self._optimizer.zero_grad()
        
        target_sequence = sequence.get_target_sequence()
        source_mask = sequence.get_source_mask()
        obstacle_mask = sequence.get_obstacle_mask()
        
        # Initialisation
        grid_pred = torch.zeros_like(target_sequence[0])
        grid_pred[source_mask] = CONFIG.SOURCE_INTENSITY
        
        total_loss = torch.tensor(0.0, device=DEVICE)
        
        # DÃ©roulement temporel
        for t_step in range(CONFIG.NCA_STEPS):
            target = target_sequence[t_step + 1]
            grid_pred = self._model.step(grid_pred, source_mask, obstacle_mask)
            
            # Perte pondÃ©rÃ©e selon l'Ã©tape
            step_loss = self._loss_fn(grid_pred, target)
            
            total_loss = total_loss + step_loss
        
        avg_loss = total_loss / CONFIG.NCA_STEPS
        
        # Backpropagation avec clipping
        avg_loss.backward()
        torch.nn.utils.clip_grad_norm_(self._model.parameters(), max_norm=1.0)
        
        self._optimizer.step()
        return avg_loss.item()
    
    
    def _adjust_learning_rate(self, stage_nb: int, epoch_in_stage: int):
        """Ajuste le learning rate selon l'Ã©tape et la progression."""
        
        base_lr = CONFIG.LEARNING_RATE
        
        # RÃ©duction progressive par Ã©tape, from 1.0 -> 0.6
        stage_lr = base_lr * (1.0 - ((stage_nb - 1) / (len(STAGE_MANAGER.get_stages()) - 1)) * 0.4)
        
        # DÃ©croissance cosine au sein de l'Ã©tape
        cos_factor = 0.5 * (1 + np.cos(np.pi * epoch_in_stage / CONFIG.NB_EPOCHS_BY_STAGE))
        final_lr = stage_lr * (0.1 + 0.9 * cos_factor)  # Ne descends pas sous 10% du LR de base
        
        for param_group in self._optimizer.param_groups:
            param_group['lr'] = final_lr
    
    
    def _train_stage(self, stage, max_epochs):
        # type: (BaseStage, int) -> None
        """
        EntraÃ®nement complet d'une Ã©tape spÃ©cifique.

        Args:
            stage: Serieux?
            max_epochs: Nombre maximum d'Ã©poques pour cette Ã©tape

        Returns:
            Dictionnaire avec les mÃ©triques de l'Ã©tape
        """
        stage_nb = stage.get_stage_nb()
        print(f"\nğŸ¯ === Ã‰TAPE {stage_nb} - DÃ‰BUT ===")
        print(f"ğŸ“‹ {stage.get_display_name()}")
        print(f"â±ï¸  Maximum {max_epochs} Ã©poques")
        
        
        # Initialisation du cache pour cette Ã©tape
        self._sequence_cache.initialize_stage_cache(stage)
        
        # MÃ©triques de l'Ã©tape
        stage_losses = []
        stage_lrs = []
        epoch_in_stage = 0
        
        # Boucle d'entraÃ®nement de l'Ã©tape
        for epoch_in_stage in range(max_epochs):
            epoch_losses = []  # type: List[float]
            
            # Ajustement du learning rate si curriculum activÃ©
            self._adjust_learning_rate(stage_nb, epoch_in_stage)
            
            # MÃ©lange pÃ©riodique du cache
            # if epoch_in_stage % 20 == 0:
            #    self._sequence_cache.shuffle_stage_cache(stage_nb)
            
            # EntraÃ®nement par batch
            for _ in range(CONFIG.BATCH_SIZE):
                sequence = self._sequence_cache.get_stage_sample(stage)
                loss = self._train_step(sequence)  # type: float
                epoch_losses.append(loss)
            
            # Statistiques de l'Ã©poque
            avg_epoch_loss = np.mean(epoch_losses)
            stage_losses.append(avg_epoch_loss)
            current_lr = self._optimizer.param_groups[0]['lr']
            stage_lrs.append(current_lr)
            
            # Affichage pÃ©riodique
            if epoch_in_stage % 10 == 0 or epoch_in_stage == max_epochs - 1:
                print(f"  Ã‰poque {epoch_in_stage:3d}/{max_epochs - 1} | "
                      f"Loss: {avg_epoch_loss:.6f} | "
                      f"LR: {current_lr:.2e}")
        
        stage.set_metrics(epoch_in_stage + 1, stage_losses, stage_lrs)
        
        print(f"âœ… === Ã‰TAPE {stage_nb} - TERMINÃ‰E ===")
        print(f"ğŸ“Š Ã‰poques entraÃ®nÃ©es: {epoch_in_stage + 1}/{max_epochs}")
        
        # Sauvegarde du checkpoint d'Ã©tape
        stage = STAGE_MANAGER.get_stage(stage_nb)
        stage.save_stage_checkpoint(self._model.state_dict(), self._optimizer.state_dict())
        
        # LibÃ©ration du cache de l'Ã©tape prÃ©cÃ©dente pour Ã©conomiser la mÃ©moire
        if stage_nb > 1:
            self._sequence_cache.clear_stage_cache(stage_nb - 1)
    
    
    def train_full_curriculum(self) -> None:
        """
        EntraÃ®nement complet du curriculum en 3 Ã©tapes.

        Returns:
            MÃ©triques complÃ¨tes de l'entraÃ®nement modulaire
        """
        print(f"\nğŸš€ === DÃ‰BUT ENTRAÃNEMENT MODULAIRE ===")
        print(f"ğŸ¯ Seed: {CONFIG.SEED}")
        print(f"ğŸ“Š Ã‰poques totales prÃ©vues: {CONFIG.TOTAL_EPOCHS}")
        print(f"ğŸ”„ Ã‰poques par Ã©tapes: {CONFIG.NB_EPOCHS_BY_STAGE}")
        
        start_time = time.time()
        self._model.train()
        
        # EntraÃ®nement sÃ©quentiel
        for stage in STAGE_MANAGER.get_stages():
            self._train_stage(stage, CONFIG.NB_EPOCHS_BY_STAGE)
        
        # MÃ©triques globales
        total_time = time.time() - start_time
        
        print(f"\nğŸ‰ === ENTRAÃNEMENT MODULAIRE TERMINÃ‰ ===")
        print(f"â±ï¸  Temps total: {total_time / 60:.1f} minutes")
        print(f"ğŸ“Š Ã‰poques totales: {CONFIG.TOTAL_EPOCHS}")
        
        # Sauvegarde du modÃ¨le final et des mÃ©triques
        self._save_final_model()
        
        return
    
    
    def _save_final_model(self):
        """Sauvegarde le modÃ¨le final et toutes les mÃ©triques."""
        # ModÃ¨le final
        final_model_path = Path(CONFIG.OUTPUT_DIR) / "final_model.pth"
        torch.save({
            'model_state_dict':     self._model.state_dict(),
            'optimizer_state_dict': self._optimizer.state_dict(),
            # 'global_metrics':       global_metrics_,
            'config':               CONFIG.__dict__
        }, final_model_path)
        
        print(f"ğŸ’¾ ModÃ¨le final et mÃ©triques sauvegardÃ©s: {CONFIG.OUTPUT_DIR}")
