# SpÃ©cification: NCA Modulaire v11 - Architecture DÃ©couplÃ©e

## Vue d'ensemble

**Version 11** reprÃ©sente une refactorisation majeure vers une architecture dÃ©couplÃ©e et modulaire pour l'apprentissage progressif des Neural Cellular Automata (NCA). Le systÃ¨me implÃ©mente un curriculum d'apprentissage en 3 Ã©tapes avec une sÃ©paration claire des responsabilitÃ©s.

## Architecture Modulaire - Ã‰tat Actuel

### Philosophie de Conception

L'architecture v11 suit les principes suivants:
- **SÃ©paration des responsabilitÃ©s**: Chaque module a une responsabilitÃ© unique et bien dÃ©finie
- **DÃ©couplage**: Les modules communiquent via des interfaces claires
- **ExtensibilitÃ©**: Ajout facile de nouvelles Ã©tapes ou fonctionnalitÃ©s
- **Singleton pattern**: Utilisation de singletons pour les gestionnaires globaux (STAGE_MANAGER, simulator)

### Structure Actuelle des Modules

```
v11/
â”œâ”€â”€ config.py           â†’ ModularConfig (paramÃ¨tres centralisÃ©s)
â”œâ”€â”€ torching.py         â†’ DEVICE (dÃ©tection CUDA/CPU)
â”œâ”€â”€ main.py             â†’ Point d'entrÃ©e principal
â”œâ”€â”€ nca_model.py        â†’ ImprovedNCA (rÃ©seau neuronal)
â”œâ”€â”€ updater.py          â†’ OptimizedNCAUpdater (application du modÃ¨le)
â”œâ”€â”€ simulator.py        â†’ HeatDiffusionSimulator (simulation physique)
â”œâ”€â”€ sequences.py        â†’ OptimizedSequenceCache (cache par Ã©tape)
â”œâ”€â”€ stage_manager.py    â†’ StageManager (orchestration Ã©tapes)
â”œâ”€â”€ trainer.py          â†’ ModularTrainer (logique d'entraÃ®nement)
â”œâ”€â”€ visualizer.py       â†’ ProgressiveVisualizer (gÃ©nÃ©ration graphiques)
â””â”€â”€ stages/
    â”œâ”€â”€ base_stage.py           â†’ BaseStage (classe abstraite)
    â”œâ”€â”€ stage_1_no_obstacle.py  â†’ Stage1NoObstacle
    â”œâ”€â”€ stage_2_one_obstacle.py â†’ Stage2OneObstacle
    â””â”€â”€ stage_3_few_obstacles.pyâ†’ Stage3FewObstacles
```

## Graphe d'Appels - Flux Principal

### 1. Initialisation (main.py)
```
main() 
  â”œâ”€â”€ torch.manual_seed(CONFIG.SEED)
  â”œâ”€â”€ ImprovedNCA() â†’ nca_model.py
  â”œâ”€â”€ ModularTrainer(model) â†’ trainer.py
  â”œâ”€â”€ trainer.train_full_curriculum()
  â”œâ”€â”€ ProgressiveVisualizer() â†’ visualizer.py
  â””â”€â”€ visualizer.visualize_stage_results()
```

### 2. EntraÃ®nement Modulaire (trainer.py)
```
ModularTrainer.train_full_curriculum()
  â””â”€â”€ for stage in STAGE_MANAGER.get_stages():
      â””â”€â”€ _train_stage(stage, CONFIG.NB_EPOCHS_BY_STAGE)
          â”œâ”€â”€ sequence_cache.initialize_stage_cache(stage)
          â”‚   â””â”€â”€ simulator.generate_stage_sequence(stage, ...)
          â”‚       â””â”€â”€ stage.generate_environment(size, source_pos)
          â”œâ”€â”€ for epoch in range(max_epochs):
          â”‚   â”œâ”€â”€ _adjust_learning_rate(stage_nb, epoch_in_stage)
          â”‚   â”œâ”€â”€ sequence_cache.get_stage_batch(stage, 1)
          â”‚   â””â”€â”€ _train_step(target_seq, source_mask, obstacle_mask)
          â”‚       â””â”€â”€ updater.step(grid_pred, source_mask, obstacle_mask)
          â”‚           â””â”€â”€ model(valid_patches)
          â””â”€â”€ stage.save_stage_checkpoint(model_state, optimizer_state)
```

### 3. GÃ©nÃ©ration des Stages (stages/)
```
STAGE_MANAGER.get_stages()
  â”œâ”€â”€ Stage1NoObstacle.generate_environment() â†’ torch.zeros()
  â”œâ”€â”€ Stage2OneObstacle.generate_environment() â†’ Un obstacle
  â””â”€â”€ Stage3FewObstacles.generate_environment() â†’ Obstacles multiples
      â””â”€â”€ _validate_connectivity() â†’ Flood-fill algorithm
```

### 4. Simulation Physique (simulator.py)
```
HeatDiffusionSimulator.generate_stage_sequence()
  â”œâ”€â”€ stage.generate_environment(size, source_pos)
  â”œâ”€â”€ Initialisation: grid[i0, j0] = SOURCE_INTENSITY
  â””â”€â”€ for _ in range(n_steps):
      â””â”€â”€ step(grid, source_mask, obstacle_mask)
          â””â”€â”€ F.conv2d(x, kernel, padding=1) # Diffusion 3x3
```

### 5. Cache et Optimisations (sequences.py)
```
OptimizedSequenceCache
  â”œâ”€â”€ initialize_stage_cache(stage)
  â”‚   â””â”€â”€ for i in range(STAGE_CACHE_SIZE):
  â”‚       â””â”€â”€ simulator.generate_stage_sequence()
  â”œâ”€â”€ get_stage_batch(stage, batch_size)
  â””â”€â”€ clear_stage_cache(stage_nb) # LibÃ©ration mÃ©moire
```

### 6. Visualisation (visualizer.py)
```
ProgressiveVisualizer
  â”œâ”€â”€ visualize_stage_results(model, stage)
  â”‚   â”œâ”€â”€ simulator.generate_stage_sequence()
  â”‚   â”œâ”€â”€ OptimizedNCAUpdater.step() # PrÃ©diction NCA
  â”‚   â”œâ”€â”€ _create_stage_animations()
  â”‚   â””â”€â”€ _create_stage_convergence_plot()
  â””â”€â”€ create_curriculum_summary()
      â”œâ”€â”€ _plot_curriculum_progression()
      â””â”€â”€ _plot_stage_comparison()
```

## DÃ©tail des Modules

### 1. Configuration (config.py)
**ResponsabilitÃ©**: ParamÃ¨tres centralisÃ©s du systÃ¨me
```python
class ModularConfig:
    SEED = 3333
    NB_EPOCHS_BY_STAGE = 30
    TOTAL_EPOCHS = 90  # 3 * 30
    GRID_SIZE = 16
    LEARNING_RATE = 1e-3
    BATCH_SIZE = 4
    STAGE_CACHE_SIZE = 250
```

### 2. ModÃ¨le NCA (nca_model.py)
**ResponsabilitÃ©**: Architecture du rÃ©seau neuronal
```python
class ImprovedNCA(nn.Module):
    - input_size: 11 (patch 3x3 + source + obstacle)
    - Architecture: Linear + BatchNorm + ReLU + Dropout
    - Sortie: delta * 0.1 (scaling pour stabilitÃ©)
```

### 3. Updater (updater.py)
**ResponsabilitÃ©**: Application optimisÃ©e du modÃ¨le NCA
```python
class OptimizedNCAUpdater:
    - Extraction vectorisÃ©e des patches 3x3
    - Application seulement sur positions valides
    - Contraintes: obstacles = 0, sources = constantes
```

### 4. Simulateur (simulator.py)
**ResponsabilitÃ©**: Simulation physique de rÃ©fÃ©rence
```python
class HeatDiffusionSimulator:
    - Noyau de convolution 3x3 pour diffusion
    - GÃ©nÃ©ration de sÃ©quences par stage
    - Gestion automatique des contraintes
```

### 5. Gestionnaire de Stages (stage_manager.py)
**ResponsabilitÃ©**: Orchestration des Ã©tapes d'apprentissage
```python
class StageManager:
    - Liste des stages: [Stage1, Stage2, Stage3]
    - Attribution automatique des numÃ©ros de stage
    - Interface unifiÃ©e: get_stages(), get_stage(nb)
```

### 6. Stages Individuels (stages/)
**ResponsabilitÃ©**: DÃ©finition des environnements par Ã©tape

#### BaseStage (base_stage.py)
```python
class BaseStage(ABC):
    - MÃ©triques: epochs_trained, loss_history, stage_lrs
    - Sauvegarde: checkpoints + mÃ©triques JSON
    - Interface: generate_environment() [abstraite]
```

#### Stage1NoObstacle
```python
- Environnement: torch.zeros() (pas d'obstacles)
- Objectif: Apprentissage de la diffusion de base
```

#### Stage2OneObstacle
```python
- Environnement: Un obstacle de taille alÃ©atoire
- Contraintes: Ã‰vitement de la source, placement valide
```

#### Stage3FewObstacles
```python
- Environnement: 2-4 obstacles multiples
- Validation: Algorithme de connectivitÃ© (flood-fill)
- Contrainte: 50% minimum de connectivitÃ©
```

### 7. Cache de SÃ©quences (sequences.py)
**ResponsabilitÃ©**: Optimisation mÃ©moire et performance
```python
class OptimizedSequenceCache:
    - Cache sÃ©parÃ© par stage (250 sÃ©quences/stage)
    - LibÃ©ration automatique des stages prÃ©cÃ©dents
    - MÃ©lange pÃ©riodique pour diversitÃ©
```

### 8. EntraÃ®neur (trainer.py)
**ResponsabilitÃ©**: Logique d'entraÃ®nement modulaire
```python
class ModularTrainer:
    - Learning rate adaptatif: 1.0 â†’ 0.6 linÃ©aire par stage
    - DÃ©croissance cosine intra-stage
    - Gradient clipping (max_norm=1.0)
    - Sauvegarde automatique des checkpoints
```

### 9. Visualiseur (visualizer.py)
**ResponsabilitÃ©**: GÃ©nÃ©ration des graphiques et animations
```python
class ProgressiveVisualizer:
    - Animations GIF de comparaison par stage
    - Graphiques de convergence
    - RÃ©sumÃ© global du curriculum
    - MÃ©triques de performance
```

## Flux de DonnÃ©es

### 1. SÃ©quence d'EntraÃ®nement
```
Stage â†’ Environment â†’ Simulator â†’ Target Sequence
  â†“
Cache â†’ Batch â†’ Trainer â†’ Model â†’ Prediction
  â†“
Loss â†’ Optimizer â†’ Model Update
```

### 2. Gestion MÃ©moire
```
Stage N: Initialize Cache (250 sequences)
  â†“
Stage N: Training Loop
  â†“
Stage N+1: Clear Cache N, Initialize Cache N+1
```

### 3. Learning Rate Dynamique
```
base_lr = 1e-3
stage_multiplier = 1.0 - ((stage_nb-1) / (n_stages-1)) * 0.4
cosine_factor = 0.5 * (1 + cos(Ï€ * epoch / epochs_per_stage))
final_lr = base_lr * stage_multiplier * (0.1 + 0.9 * cosine_factor)
```

## Points d'Extension

### 1. Ajout de Nouveaux Stages
```python
# 1. CrÃ©er stage_4_complex_obstacles.py
class Stage4ComplexObstacles(BaseStage):
    def generate_environment(self, size, source_pos):
        # ImplÃ©mentation spÃ©cifique
        
# 2. Ajouter dans stage_manager.py
self._stages = [Stage1(), Stage2(), Stage3(), Stage4()]
```

### 2. Nouveaux Types de Visualisation
```python
# Dans visualizer.py
def create_3d_visualization(self, vis_data):
    # Visualisation 3D des gradients
    
def create_flow_field_analysis(self, vis_data):
    # Analyse des champs de flux
```

### 3. Optimisations SupplÃ©mentaires
```python
# Cache intelligent avec LRU
# ParallÃ©lisation des stages
# Mixed precision training
# Distributed training
```

## MÃ©triques et Monitoring

### 1. MÃ©triques par Stage
- **Loss history**: Ã‰volution de la perte MSE
- **Learning rates**: Historique des LR adaptatifs
- **Epochs trained**: Nombre d'Ã©poques utilisÃ©es
- **Convergence**: Temps de convergence par stage

### 2. MÃ©triques Globales
- **Temps total**: DurÃ©e d'entraÃ®nement complÃ¨te
- **MÃ©moire**: Utilisation GPU/CPU par stage
- **StabilitÃ©**: Variance des performances

### 3. Visualisations GÃ©nÃ©rÃ©es
- **Animations**: GIFs de comparaison cible/prÃ©diction
- **Convergence**: Graphiques d'erreur temporelle
- **Curriculum**: Progression globale multi-stage
- **Performance**: Comparaisons inter-stages

## Ã‰tat Actuel vs Prochaines Refactorisations

### âœ… RÃ©alisÃ© en v11
- DÃ©couplage complet des modules
- Architecture orientÃ©e objet claire
- SystÃ¨me de stages extensible
- Cache optimisÃ© par Ã©tape
- Visualisations avancÃ©es
- Learning rate adaptatif dynamique

### ðŸŽ¯ Prochaines AmÃ©liorations Possibles
- **Scheduler avancÃ©**: Transitions adaptatives entre stages
- **MÃ©triques temps rÃ©el**: Dashboard de monitoring
- **ParallÃ©lisation**: EntraÃ®nement multi-GPU
- **Validation**: Tests unitaires pour chaque module
- **Documentation**: API docs complÃ¨te
- **Configuration**: YAML/JSON externe pour les paramÃ¨tres

## Contraintes Techniques

### 1. MÃ©moire
- Cache limitÃ© Ã  250 sÃ©quences/stage
- LibÃ©ration automatique des stages prÃ©cÃ©dents
- Utilisation de `.detach().cpu()` pour les visualisations

### 2. Performance
- Extraction vectorisÃ©e des patches (unfold)
- Application seulement sur positions valides
- Gradient clipping pour stabilitÃ© numÃ©rique

### 3. ReproductibilitÃ©
- Seeds fixes: CONFIG.SEED (entraÃ®nement), VISUALIZATION_SEED (visualisation)
- Deterministic operations sur GPU si disponible

Cette architecture v11 reprÃ©sente un systÃ¨me robuste et extensible pour l'apprentissage modulaire progressif des NCA, avec une sÃ©paration claire des responsabilitÃ©s et des interfaces bien dÃ©finies pour les futures extensions.
