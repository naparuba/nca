import argparse
import json
import os
import time
from pathlib import Path
from typing import Tuple, List, Dict, Any

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# =============================================================================
# Configuration et initialisation modulaire
# =============================================================================

SEED = 3333

VISUALIZATION_SEED=3333

NB_EPOCHS_BY_STAGE = 200

TOTAL_EPOCHS = NB_EPOCHS_BY_STAGE * 3

STAGNATION_THRESHOLD = 0.000001  # seuil de stagnation pour avancer d'√©tape
STAGNATION_PATIENCE = NB_EPOCHS_BY_STAGE // 4  # if we flat for 1/4 of an epochs, we consider it stagnated

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


class ModularConfig:
    """
    Configuration √©tendue pour l'apprentissage modulaire progressif.
    H√©rite des param√®tres de base et ajoute la gestion des √©tapes.
    """
    
    
    def __init__(self):
        # Param√®tres mat√©riels de base
        
        # Param√®tres de grille
        self.GRID_SIZE = 16
        self.SOURCE_INTENSITY = 1.0
        
        # Param√®tres d'entra√Ænement modulaire
        self.TOTAL_EPOCHS = TOTAL_EPOCHS  # Augment√© pour l'apprentissage modulaire
        self.NCA_STEPS = 20
        self.LEARNING_RATE = 1e-3
        self.BATCH_SIZE = 4
        
        # Calcul automatique des √©poques par √©tape
        self.STAGE_1_EPOCHS = NB_EPOCHS_BY_STAGE
        self.STAGE_2_EPOCHS = NB_EPOCHS_BY_STAGE
        self.STAGE_3_EPOCHS = NB_EPOCHS_BY_STAGE
        
        # Seuils de convergence adaptatifs par √©tape
        self.CONVERGENCE_THRESHOLDS = {
            1: 0.01,  # √âtape 1: convergence stricte
            2: 0.02,  # √âtape 2: tol√©rance accrue
            3: 0.05  # √âtape 3: tol√©rance maximale
        }
        
        # Param√®tres de visualisation
        self.PREVIS_STEPS = 30
        self.POSTVIS_STEPS = 50
        self.OUTPUT_DIR = "outputs"
        
        # Param√®tres du mod√®le
        self.HIDDEN_SIZE = 128
        self.N_LAYERS = 3
        
        # Param√®tres d'obstacles par √©tape
        self.STAGE_OBSTACLE_CONFIG = {
            1: {'min_obstacles': 0, 'max_obstacles': 0},  # Pas d'obstacles
            2: {'min_obstacles': 1, 'max_obstacles': 1},  # Un seul obstacle
            3: {'min_obstacles': 2, 'max_obstacles': 4}  # 2-4 obstacles
        }
        
        self.MIN_OBSTACLE_SIZE = 2
        self.MAX_OBSTACLE_SIZE = 4
        
        # Optimisations
        self.CACHE_SIZE = 200
        self.USE_MIXED_PRECISION = False
        
        # Nouveaux param√®tres modulaires
        self.MAX_STAGE_RETRIES = 3  # Tentatives max par √©tape
        self.EARLY_STOPPING_PATIENCE = 20  # Patience pour arr√™t pr√©coce
        self.STAGE_TRANSITION_SMOOTHING = True  # Lissage des transitions


def parse_modular_arguments():
    """
    Parse les arguments √©tendus pour l'apprentissage modulaire.
    """
    parser = argparse.ArgumentParser(
            description='Neural Cellular Automaton - Apprentissage Modulaire Progressif',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Arguments de base
    parser.add_argument('--seed', type=int, default=123,
                        help='Graine al√©atoire pour la reproductibilit√©')
    parser.add_argument('--vis-seed', type=int, default=3333,
                        help='Graine pour les visualisations')
    parser.add_argument('--total-epochs', type=int, default=200,
                        help='Nombre total d\'√©poques d\'entra√Ænement')
    parser.add_argument('--grid-size', type=int, default=16,
                        help='Taille de la grille')
    parser.add_argument('--batch-size', type=int, default=4,
                        help='Taille des batches')
    
    # Arguments modulaires (NOUVEAUX)
    parser.add_argument('--stage1-ratio', type=float, default=0.5,
                        help='Ratio d\'√©poques pour l\'√©tape 1 (sans obstacles)')
    parser.add_argument('--stage2-ratio', type=float, default=0.3,
                        help='Ratio d\'√©poques pour l\'√©tape 2 (un obstacle)')
    parser.add_argument('--stage3-ratio', type=float, default=0.2,
                        help='Ratio d\'√©poques pour l\'√©tape 3 (obstacles multiples)')
    
    parser.add_argument('--adaptive-thresholds', action='store_true', default=True,
                        help='Utiliser des seuils adaptatifs pour l\'avancement')
    parser.add_argument('--max-obstacles', type=int, default=4,
                        help='Nombre maximum d\'obstacles en √©tape 3')
    parser.add_argument('--save-stage-checkpoints', action='store_true', default=True,
                        help='Sauvegarder les mod√®les √† chaque √©tape')
    
    return parser.parse_args()


# Parse des arguments et configuration
args = parse_modular_arguments()
cfg = ModularConfig()

# Mise √† jour depuis les arguments
cfg.TOTAL_EPOCHS = args.total_epochs
cfg.GRID_SIZE = args.grid_size
cfg.BATCH_SIZE = args.batch_size

# Configuration max obstacles √©tape 3
cfg.STAGE_OBSTACLE_CONFIG[3]['max_obstacles'] = args.max_obstacles


# Gestion matplotlib (h√©rit√©e de v6)
def setup_matplotlib():
    """Configure matplotlib pour l'affichage interactif ou la sauvegarde."""
    try:
        matplotlib.use('Qt5Agg')
        plt.ion()
        fig, ax = plt.subplots()
        plt.close(fig)
        print("‚úÖ Mode interactif activ√©")
        return True
    except:
        try:
            matplotlib.use('TkAgg')
            plt.ion()
            fig, ax = plt.subplots()
            plt.close(fig)
            print("‚úÖ Mode interactif activ√© (TkAgg)")
            return True
        except:
            print("‚ö†Ô∏è  Mode non-interactif d√©tect√© - les animations seront sauvegard√©es")
            matplotlib.use('Agg')
            return False


# Initialisation
interactive_mode = setup_matplotlib()
if os.name == 'nt':
    interactive_mode = False

torch.manual_seed(SEED)
np.random.seed(SEED)

# Cr√©ation du r√©pertoire de sortie avec seed
cfg.OUTPUT_DIR = f"outputs"
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

print(f"üéØ Configuration Modulaire NCA v11")
print(f"Device: {DEVICE}")
print(f"Seed: {SEED}")
print(f"Mode interactif: {interactive_mode}")
print(f"R√©pertoire de sortie: {cfg.OUTPUT_DIR}")
print(f"√âtapes: {cfg.STAGE_1_EPOCHS} + {cfg.STAGE_2_EPOCHS} + {cfg.STAGE_3_EPOCHS} = {cfg.TOTAL_EPOCHS} √©poques")


# =============================================================================
# Gestionnaire d'obstacles progressifs
# =============================================================================

class ProgressiveObstacleManager:
    """
    Gestionnaire intelligent des obstacles selon l'√©tape d'apprentissage.
    G√©n√®re des environnements appropri√©s pour chaque phase du curriculum.
    """
    
    
    def __init__(self):
        self.stage_configs = cfg.STAGE_OBSTACLE_CONFIG
    
    
    def generate_stage_environment(self, stage: int, size: int, source_pos: Tuple[int, int]) -> torch.Tensor:
        """
        G√©n√®re un environnement d'obstacles adapt√© √† l'√©tape courante.
        
        Args:
            stage: Num√©ro d'√©tape (1, 2, ou 3)
            size: Taille de la grille
            source_pos: Position de la source (i, j)
            
        Returns:
            Masque des obstacles [H, W]
        """
        if stage not in self.stage_configs:
            raise ValueError(f"√âtape {stage} non support√©e. √âtapes valides: {list(self.stage_configs.keys())}")
        
        config = self.stage_configs[stage]
        
        if stage == 1:
            return self._generate_stage_1_environment(size)
        elif stage == 2:
            return self._generate_stage_2_environment(size, source_pos)
        elif stage == 3:
            return self._generate_stage_3_environment(size, source_pos)
    
    
    def _generate_stage_1_environment(self, size: int) -> torch.Tensor:
        """√âtape 1: Aucun obstacle - grille vide pour apprentissage de base."""
        return torch.zeros((size, size), dtype=torch.bool, device=DEVICE)
    
    
    def _generate_stage_2_environment(self, size: int, source_pos: Tuple[int, int]) -> torch.Tensor:
        """√âtape 2: Un seul obstacle pour apprentissage du contournement."""
        obstacle_mask = torch.zeros((size, size), dtype=torch.bool, device=DEVICE)
        
        g = torch.Generator(device=DEVICE)
        g.manual_seed(SEED)
        
        # Un seul obstacle de taille al√©atoire
        obstacle_size = torch.randint(cfg.MIN_OBSTACLE_SIZE, cfg.MAX_OBSTACLE_SIZE + 1,
                                      (1,), generator=g, device=DEVICE).item()
        
        # Placement en √©vitant la source et les bords
        max_pos = size - obstacle_size
        if max_pos <= 1:
            return obstacle_mask  # Grille trop petite
        
        source_i, source_j = source_pos
        
        for attempt in range(100):  # Plus de tentatives pour √©tape 2
            i = torch.randint(1, max_pos, (1,), generator=g, device=DEVICE).item()
            j = torch.randint(1, max_pos, (1,), generator=g, device=DEVICE).item()
            
            # V√©rifier non-chevauchement avec source
            if not (i <= source_i < i + obstacle_size and j <= source_j < j + obstacle_size):
                obstacle_mask[i:i + obstacle_size, j:j + obstacle_size] = True
                break
        
        return obstacle_mask
    
    
    def _generate_stage_3_environment(self, size: int, source_pos: Tuple[int, int]) -> torch.Tensor:
        """√âtape 3: Obstacles multiples pour gestion de la complexit√©."""
        obstacle_mask = torch.zeros((size, size), dtype=torch.bool, device=DEVICE)
        
        g = torch.Generator(device=DEVICE)
        g.manual_seed(SEED)
        
        config = self.stage_configs[3]
        n_obstacles = torch.randint(config['min_obstacles'], config['max_obstacles'] + 1,
                                    (1,), generator=g, device=DEVICE).item()
        
        source_i, source_j = source_pos
        placed_obstacles = []
        
        for obstacle_idx in range(n_obstacles):
            obstacle_size = torch.randint(cfg.MIN_OBSTACLE_SIZE, cfg.MAX_OBSTACLE_SIZE + 1,
                                          (1,), generator=g, device=DEVICE).item()
            
            max_pos = size - obstacle_size
            if max_pos <= 1:
                continue
            
            for attempt in range(50):
                i = torch.randint(1, max_pos, (1,), generator=g, device=DEVICE).item()
                j = torch.randint(1, max_pos, (1,), generator=g, device=DEVICE).item()
                
                # V√©rifications multiples pour √©tape 3
                valid_position = True
                
                # 1. Pas de chevauchement avec source
                if i <= source_i < i + obstacle_size and j <= source_j < j + obstacle_size:
                    valid_position = False
                
                # 2. Pas de chevauchement avec obstacles existants
                for obs_i, obs_j, obs_size in placed_obstacles:
                    if (i < obs_i + obs_size and i + obstacle_size > obs_i and
                            j < obs_j + obs_size and j + obstacle_size > obs_j):
                        valid_position = False
                        break
                
                if valid_position:
                    obstacle_mask[i:i + obstacle_size, j:j + obstacle_size] = True
                    placed_obstacles.append((i, j, obstacle_size))
                    break
        
        # Validation finale de connectivit√©
        if not self._validate_connectivity(obstacle_mask, source_pos):
            print("‚ö†Ô∏è  Connectivit√© non garantie - g√©n√©ration d'un environnement plus simple")
            return self._generate_stage_2_environment(size, source_pos)
        
        return obstacle_mask
    
    
    def _validate_connectivity(self, obstacle_mask: torch.Tensor, source_pos: Tuple[int, int]) -> bool:
        """
        Valide qu'un chemin de diffusion reste possible avec les obstacles.
        Utilise un algorithme de flood-fill simplifi√©.
        """
        H, W = obstacle_mask.shape
        source_i, source_j = source_pos
        
        # Matrice de visite
        visited = torch.zeros_like(obstacle_mask, dtype=torch.bool)
        visited[obstacle_mask] = True  # Les obstacles sont "d√©j√† visit√©s"
        
        # Flood-fill depuis la source
        stack = [(source_i, source_j)]
        visited[source_i, source_j] = True
        accessible_cells = 1
        
        while stack:
            i, j = stack.pop()
            
            # Parcours des 4 voisins
            for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                ni, nj = i + di, j + dj
                
                if (0 <= ni < H and 0 <= nj < W and
                        not visited[ni, nj] and not obstacle_mask[ni, nj]):
                    visited[ni, nj] = True
                    stack.append((ni, nj))
                    accessible_cells += 1
        
        # Au moins 50% de la grille doit √™tre accessible pour une bonne diffusion
        total_free_cells = (H * W) - obstacle_mask.sum().item()
        connectivity_ratio = accessible_cells / max(total_free_cells, 1)
        
        return connectivity_ratio >= 0.5
    
    
    def get_difficulty_metrics(self, stage: int, obstacle_mask: torch.Tensor) -> Dict[str, float]:
        """
        Calcule des m√©triques de difficult√© pour l'environnement g√©n√©r√©.
        
        Returns:
            Dictionnaire avec les m√©triques de complexit√©
        """
        H, W = obstacle_mask.shape
        total_cells = H * W
        obstacle_cells = obstacle_mask.sum().item()
        
        metrics = {
            'stage':            stage,
            'obstacle_ratio':   obstacle_cells / total_cells,
            'free_cells':       total_cells - obstacle_cells,
            'complexity_score': stage * (obstacle_cells / total_cells)
        }
        
        return metrics


# =============================================================================
# Simulateur de diffusion
# =============================================================================

class DiffusionSimulator:
    """
    Simulateur de diffusion de chaleur adapt√© pour l'apprentissage modulaire.
    Utilise le gestionnaire d'obstacles progressifs.
    """
    
    
    def __init__(self):
        self.kernel = torch.ones((1, 1, 3, 3), device=DEVICE) / 9.0
        self.obstacle_manager = ProgressiveObstacleManager()
    
    
    def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor) -> torch.Tensor:
        """Un pas de diffusion de chaleur avec obstacles."""
        x = grid.unsqueeze(0).unsqueeze(0)
        new_grid = F.conv2d(x, self.kernel, padding=1).squeeze(0).squeeze(0)
        
        # Contraintes
        new_grid[obstacle_mask] = 0.0
        new_grid[source_mask] = grid[source_mask]
        
        return new_grid
    
    
    def generate_stage_sequence(self, stage: int, n_steps: int, size: int) -> Tuple[List[torch.Tensor], torch.Tensor, torch.Tensor]:
        """
        G√©n√®re une s√©quence adapt√©e √† l'√©tape d'apprentissage courante.
        
        Args:
            stage: √âtape d'apprentissage (1, 2, ou 3)
            n_steps: Nombre d'√©tapes de simulation
            size: Taille de la grille
            
        Returns:
            (s√©quence, masque_source, masque_obstacles)
        """
        # Position al√©atoire de la source
        g = torch.Generator(device=DEVICE)
        g.manual_seed(SEED)
        i0 = torch.randint(2, size - 2, (1,), generator=g, device=DEVICE).item()
        j0 = torch.randint(2, size - 2, (1,), generator=g, device=DEVICE).item()
        
        # G√©n√©ration d'obstacles selon l'√©tape
        obstacle_mask = self.obstacle_manager.generate_stage_environment(stage, size, (i0, j0))
        
        # Initialisation
        grid = torch.zeros((size, size), device=DEVICE)
        grid[i0, j0] = cfg.SOURCE_INTENSITY
        
        source_mask = torch.zeros_like(grid, dtype=torch.bool)
        source_mask[i0, j0] = True
        
        # S'assurer que la source n'est pas dans un obstacle
        if obstacle_mask[i0, j0]:
            obstacle_mask[i0, j0] = False
        
        # Simulation temporelle
        sequence = [grid.clone()]
        for _ in range(n_steps):
            grid = self.step(grid, source_mask, obstacle_mask)
            sequence.append(grid.clone())
        
        return sequence, source_mask, obstacle_mask


# Instance globale du simulateur modulaire
simulator = DiffusionSimulator()


# =============================================================================
# Mod√®le NCA
# =============================================================================

class ImprovedNCA(nn.Module):
    """
    Neural Cellular Automaton optimis√© pour l'apprentissage modulaire.
    Architecture identique √† v6 mais avec support √©tendu pour le curriculum.
    """
    
    
    def __init__(self, input_size: int = 11, hidden_size: int = cfg.HIDDEN_SIZE,
                 n_layers: int = cfg.N_LAYERS):
        super().__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        
        # Architecture profonde avec normalisation
        layers = []
        current_size = input_size
        
        for i in range(n_layers):
            layers.append(nn.Linear(current_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
            current_size = hidden_size
        
        # Couche de sortie stabilis√©e
        layers.append(nn.Linear(hidden_size, 1))
        layers.append(nn.Tanh())
        
        self.network = nn.Sequential(*layers)
        self.delta_scale = 0.1
    
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass avec scaling des deltas."""
        delta = self.network(x)
        return delta * self.delta_scale


# =============================================================================
# Updaters NCA
# =============================================================================

class OptimizedNCAUpdater:
    """
    Updater optimis√© avec extraction vectoris√©e des patches.
    """
    
    
    def __init__(self, model: ImprovedNCA):
        self.model = model
    
    
    def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor) -> torch.Tensor:
        """Application optimis√©e du NCA."""
        H, W = grid.shape
        
        # Extraction vectoris√©e des patches 3x3
        grid_padded = F.pad(grid.unsqueeze(0).unsqueeze(0), (1, 1, 1, 1), mode='replicate')
        patches = F.unfold(grid_padded, kernel_size=3, stride=1)
        patches = patches.squeeze(0).transpose(0, 1)  # [H*W, 9]
        
        # Features additionnelles
        source_flat = source_mask.flatten().float().unsqueeze(1)  # [H*W, 1]
        obstacle_flat = obstacle_mask.flatten().float().unsqueeze(1)  # [H*W, 1]
        full_patches = torch.cat([patches, source_flat, obstacle_flat], dim=1)  # [H*W, 11]
        
        # Application seulement sur positions valides
        valid_mask = ~obstacle_mask.flatten()
        
        if valid_mask.any():
            valid_patches = full_patches[valid_mask]
            deltas = self.model(valid_patches)
            
            new_grid = grid.clone().flatten()
            new_grid[valid_mask] += deltas.squeeze()
            new_grid = torch.clamp(new_grid, 0.0, 1.0).reshape(H, W)
        else:
            new_grid = grid.clone()
        
        # Contraintes finales
        new_grid[obstacle_mask] = 0.0
        new_grid[source_mask] = grid[source_mask]
        
        return new_grid


# class NCAUpdater:
#     """
#     Updater standard avec boucles Python.
#     """
#
#     def __init__(self, model: ImprovedNCA, device: str = cfg.DEVICE):
#         self.model = model
#         self.device = device
#
#     def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor) -> torch.Tensor:
#         """Application standard du NCA avec boucles."""
#         H, W = grid.shape
#         new_grid = grid.clone()
#
#         patches = []
#         positions = []
#
#         for i in range(1, H-1):
#             for j in range(1, W-1):
#                 if obstacle_mask[i, j]:
#                     continue
#
#                 patch = grid[i-1:i+2, j-1:j+2].reshape(-1)  # 9 √©l√©ments
#                 is_source = source_mask[i, j].float()
#                 is_obstacle = obstacle_mask[i, j].float()
#                 full_patch = torch.cat([patch, is_source.unsqueeze(0), is_obstacle.unsqueeze(0)])
#
#                 patches.append(full_patch)
#                 positions.append((i, j))
#
#         if patches:
#             patches_tensor = torch.stack(patches)
#             deltas = self.model(patches_tensor)
#
#             for idx, (i, j) in enumerate(positions):
#                 new_value = grid[i, j] + deltas[idx].squeeze()
#                 new_grid[i, j] = torch.clamp(new_value, 0.0, 1.0)
#
#         # Contraintes
#         new_grid[obstacle_mask] = 0.0
#         new_grid[source_mask] = grid[source_mask]
#
#         return new_grid

# =============================================================================
# Planificateur de curriculum (NOUVEAU)
# =============================================================================

class CurriculumScheduler:
    """
    Gestionnaire de la progression automatique entre les √©tapes d'apprentissage.
    D√©cide quand passer √† l'√©tape suivante selon des crit√®res adaptatifs.
    """
    
    
    def __init__(self, convergence_thresholds: Dict[int, float], patience: int = 10):
        self.thresholds = convergence_thresholds
        self.patience = patience
        self.stage_metrics_history = {stage: [] for stage in [1, 2, 3]}
        self.no_improvement_counts = {stage: 0 for stage in [1, 2, 3]}
    
    
    def should_advance_stage(self, current_stage: int, recent_losses: List[float]) -> bool:
        """
        D√©termine s'il faut passer √† l'√©tape suivante.

        Args:
            current_stage: √âtape courante
            recent_losses: Pertes r√©centes pour √©valuation

        Returns:
            True si on doit avancer √† l'√©tape suivante
        """
        if not recent_losses:  # or current_stage >= 3:
            return False
        
        # Crit√®re secondaire: stagnation (pas d'am√©lioration)
        if len(recent_losses) >= 2:
            improvement = recent_losses[-2] - recent_losses[-1]
            if improvement < STAGNATION_THRESHOLD:  # Am√©lioration n√©gligeable
                self.no_improvement_counts[current_stage] += 1
            else:
                self.no_improvement_counts[current_stage] = 0
        
        stagnated = self.no_improvement_counts[current_stage] >= STAGNATION_PATIENCE
        
        return stagnated
    
    
    def adjust_learning_rate(self, optimizer, stage: int, epoch_in_stage: int):
        """Ajuste le learning rate selon l'√©tape et la progression."""
        
        base_lr = cfg.LEARNING_RATE
        
        # R√©duction progressive par √©tape
        stage_multipliers = {1: 1.0, 2: 0.8, 3: 0.6}
        stage_lr = base_lr * stage_multipliers.get(stage, 0.5)
        
        # D√©croissance cosine au sein de l'√©tape
        stage_epochs = {1: cfg.STAGE_1_EPOCHS, 2: cfg.STAGE_2_EPOCHS, 3: cfg.STAGE_3_EPOCHS}
        max_epochs = stage_epochs.get(stage, 50)
        
        cos_factor = 0.5 * (1 + np.cos(np.pi * epoch_in_stage / max_epochs))
        final_lr = stage_lr * (0.1 + 0.9 * cos_factor)  # Ne descend pas sous 10% du LR de base
        
        for param_group in optimizer.param_groups:
            param_group['lr'] = final_lr
    
    
    def get_stage_loss_weights(self, stage: int) -> Dict[str, float]:
        """Retourne les poids de pond√©ration des pertes par √©tape."""
        weights = {
            1: {'mse': 1.0, 'convergence': 2.0, 'stability': 1.0},
            2: {'mse': 1.0, 'convergence': 1.5, 'stability': 1.5, 'adaptation': 1.0},
            3: {'mse': 1.0, 'convergence': 1.0, 'stability': 2.0, 'robustness': 1.5}
        }
        return weights.get(stage, weights[1])


# =============================================================================
# Cache de s√©quences optimis√© par √©tape
# =============================================================================

class OptimizedSequenceCache:
    """
    Cache sp√©cialis√© par √©tape pour l'entra√Ænement modulaire.
    Maintient des caches s√©par√©s pour chaque √©tape d'apprentissage.
    """
    
    
    def __init__(self, simulator: DiffusionSimulator):
        self.simulator = simulator
        self.stage_caches = {}  # Cache par √©tape
        self.cache_sizes = {1: 150, 2: 200, 3: 250}  # Plus de vari√©t√© pour √©tapes complexes
        self.current_indices = {}
    
    
    def initialize_stage_cache(self, stage: int):
        """Initialise le cache pour une √©tape sp√©cifique."""
        if stage in self.stage_caches:
            return  # D√©j√† initialis√©
        
        cache_size = self.cache_sizes.get(stage, 200)
        print(f"üéØ G√©n√©ration de {cache_size} s√©quences pour l'√©tape {stage}...")
        
        sequences = []
        for i in range(cache_size):
            if i % 50 == 0:
                print(f"   √âtape {stage}: {i}/{cache_size}")
            
            target_seq, source_mask, obstacle_mask = self.simulator.generate_stage_sequence(
                    stage=stage,
                    n_steps=cfg.NCA_STEPS,
                    size=cfg.GRID_SIZE
            )
            
            sequences.append({
                'target_seq':    target_seq,
                'source_mask':   source_mask,
                'obstacle_mask': obstacle_mask,
                'stage':         stage
            })
        
        self.stage_caches[stage] = sequences
        self.current_indices[stage] = 0
        print(f"‚úÖ Cache √©tape {stage} cr√©√© ({cache_size} s√©quences)")
    
    
    def get_stage_batch(self, stage: int, batch_size: int):
        """R√©cup√®re un batch pour l'√©tape sp√©cifi√©e."""
        if stage not in self.stage_caches:
            self.initialize_stage_cache(stage)
        
        cache = self.stage_caches[stage]
        batch = []
        
        for _ in range(batch_size):
            batch.append(cache[self.current_indices[stage]])
            self.current_indices[stage] = (self.current_indices[stage] + 1) % len(cache)
        
        return batch
    
    
    def shuffle_stage_cache(self, stage: int):
        """M√©lange le cache d'une √©tape sp√©cifique."""
        if stage in self.stage_caches:
            import random
            random.shuffle(self.stage_caches[stage])
    
    
    def clear_stage_cache(self, stage: int):
        """Lib√®re la m√©moire du cache d'une √©tape."""
        if stage in self.stage_caches:
            del self.stage_caches[stage]
            del self.current_indices[stage]
            print(f"üóëÔ∏è  Cache √©tape {stage} lib√©r√©")


# =============================================================================
# Entra√Æneur modulaire principal
# =============================================================================

class ModularTrainer:
    """
    Syst√®me d'entra√Ænement modulaire progressif.
    G√®re l'apprentissage par √©tapes avec transitions automatiques.
    """
    
    
    def __init__(self, model: ImprovedNCA):
        self.model = model
        
        # Choix de l'updater optimis√©
        print("üöÄ Utilisation de l'updater optimis√© vectoris√©")
        self.updater = OptimizedNCAUpdater(model)
        
        # Optimiseur et planificateur
        self.optimizer = optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=1e-4)
        self.loss_fn = nn.MSELoss()
        
        # Curriculum et m√©triques
        
        self.curriculum = CurriculumScheduler(cfg.CONVERGENCE_THRESHOLDS)
        
        # Cache optimis√© par √©tape
        self.sequence_cache = OptimizedSequenceCache(simulator)
        self.use_cache = True
        
        # √âtat d'entra√Ænement
        self.current_stage = 1
        self.stage_histories = {stage: {'losses': [], 'epochs': [], 'lr': []} for stage in [1, 2, 3]}
        self.global_history = {'losses': [], 'stages': [], 'epochs': []}
        self.stage_start_epochs = {}
        self.total_epochs_trained = 0
    
    
    def train_step(self, target_sequence: List[torch.Tensor], source_mask: torch.Tensor,
                   obstacle_mask: torch.Tensor, stage: int) -> float:
        """
        Un pas d'entra√Ænement adapt√© √† l'√©tape courante.

        Args:
            target_sequence: S√©quence cible
            source_mask: Masque des sources
            obstacle_mask: Masque des obstacles
            stage: √âtape courante d'entra√Ænement

        Returns:
            Perte pour ce pas
        """
        self.optimizer.zero_grad()
        
        # Initialisation
        grid_pred = torch.zeros_like(target_sequence[0])
        grid_pred[source_mask] = cfg.SOURCE_INTENSITY
        
        total_loss = torch.tensor(0.0, device=DEVICE)
        
        # D√©roulement temporel
        for t_step in range(cfg.NCA_STEPS):
            target = target_sequence[t_step + 1]
            grid_pred = self.updater.step(grid_pred, source_mask, obstacle_mask)
            
            # Perte pond√©r√©e selon l'√©tape
            step_loss = self.loss_fn(grid_pred, target)
            if self.curriculum:
                weights = self.curriculum.get_stage_loss_weights(stage)
                step_loss = step_loss * weights.get('mse', 1.0)
            
            total_loss = total_loss + step_loss
        
        avg_loss = total_loss / cfg.NCA_STEPS
        
        # Backpropagation avec clipping
        avg_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        return avg_loss.item()
    
    
    def train_stage(self, stage: int, max_epochs: int) -> Dict[str, Any]:
        """
        Entra√Ænement complet d'une √©tape sp√©cifique.

        Args:
            stage: Num√©ro d'√©tape (1, 2, ou 3)
            max_epochs: Nombre maximum d'√©poques pour cette √©tape

        Returns:
            Dictionnaire avec les m√©triques de l'√©tape
        """
        print(f"\nüéØ === √âTAPE {stage} - D√âBUT ===")
        stage_name = {1: "Sans obstacles", 2: "Un obstacle", 3: "Obstacles multiples"}[stage]
        print(f"üìã {stage_name}")
        print(f"‚è±Ô∏è  Maximum {max_epochs} √©poques")
        
        self.current_stage = stage
        self.stage_start_epochs[stage] = self.total_epochs_trained
        
        # Initialisation du cache pour cette √©tape
        if self.use_cache:
            self.sequence_cache.initialize_stage_cache(stage)
        
        # M√©triques de l'√©tape
        stage_losses = []
        epoch_in_stage = 0
        early_stop = False
        
        # Boucle d'entra√Ænement de l'√©tape
        for epoch_in_stage in range(max_epochs):
            epoch_losses = []
            
            # Ajustement du learning rate si curriculum activ√©
            if self.curriculum:
                self.curriculum.adjust_learning_rate(self.optimizer, stage, epoch_in_stage)
            
            # M√©lange p√©riodique du cache
            if self.use_cache and epoch_in_stage % 20 == 0:
                self.sequence_cache.shuffle_stage_cache(stage)
            
            # Entra√Ænement par batch
            for _ in range(cfg.BATCH_SIZE):
                batch_sequences = self.sequence_cache.get_stage_batch(stage, 1)
                seq_data = batch_sequences[0]
                target_seq = seq_data['target_seq']
                source_mask = seq_data['source_mask']
                obstacle_mask = seq_data['obstacle_mask']
                
                loss = self.train_step(target_seq, source_mask, obstacle_mask, stage)
                epoch_losses.append(loss)
            
            # Statistiques de l'√©poque
            avg_epoch_loss = np.mean(epoch_losses)
            stage_losses.append(avg_epoch_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # Historiques
            self.stage_histories[stage]['losses'].append(avg_epoch_loss)
            self.stage_histories[stage]['epochs'].append(epoch_in_stage)
            self.stage_histories[stage]['lr'].append(current_lr)
            
            self.global_history['losses'].append(avg_epoch_loss)
            self.global_history['stages'].append(stage)
            self.global_history['epochs'].append(self.total_epochs_trained)
            
            self.total_epochs_trained += 1
            
            # Affichage p√©riodique
            if epoch_in_stage % 10 == 0 or epoch_in_stage == max_epochs - 1:
                print(f"  √âpoque {epoch_in_stage:3d}/{max_epochs - 1} | "
                      f"Loss: {avg_epoch_loss:.6f} | "
                      f"LR: {current_lr:.2e}")
            
            # V√©rification de l'avancement automatique (curriculum)
            if self.curriculum.should_advance_stage(stage, stage_losses):
                print(f"üéØ Convergence atteinte √† l'√©poque {epoch_in_stage}")
                print(f"   Seuil: {cfg.CONVERGENCE_THRESHOLDS[stage]:.3f}, "
                      f"Loss: {avg_epoch_loss:.6f}")
                early_stop = True
                break
        
        # R√©sum√© de l'√©tape
        final_loss = stage_losses[-1] if stage_losses else float('inf')
        convergence_met = final_loss < cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05)
        
        stage_metrics = {
            'stage':           stage,
            'epochs_trained':  epoch_in_stage + 1,
            'final_loss':      final_loss,
            'convergence_met': convergence_met,
            'early_stopped':   early_stop,
            'loss_history':    stage_losses
        }
        
        print(f"‚úÖ === √âTAPE {stage} - TERMIN√âE ===")
        print(f"üìä √âpoques entra√Æn√©es: {epoch_in_stage + 1}/{max_epochs}")
        print(f"üìâ Perte finale: {final_loss:.6f}")
        print(f"üéØ Convergence: {'‚úÖ OUI' if convergence_met else '‚ùå NON'}")
        print(f"‚ö° Arr√™t pr√©coce: {'‚úÖ OUI' if early_stop else '‚ùå NON'}")
        
        # Sauvegarde du checkpoint d'√©tape
        self.save_stage_checkpoint(stage, stage_metrics)
        
        # Lib√©ration du cache de l'√©tape pr√©c√©dente pour √©conomiser la m√©moire
        if self.use_cache and stage > 1:
            self.sequence_cache.clear_stage_cache(stage - 1)
        
        return stage_metrics
    
    
    def train_full_curriculum(self) -> Dict[str, Any]:
        """
        Entra√Ænement complet du curriculum en 3 √©tapes.

        Returns:
            M√©triques compl√®tes de l'entra√Ænement modulaire
        """
        print(f"\nüöÄ === D√âBUT ENTRA√éNEMENT MODULAIRE ===")
        print(f"üéØ Seed: {SEED}")
        print(f"üìä √âpoques totales pr√©vues: {cfg.TOTAL_EPOCHS}")
        print(f"üîÑ √âtapes: {cfg.STAGE_1_EPOCHS} + {cfg.STAGE_2_EPOCHS} + {cfg.STAGE_3_EPOCHS}")
        
        start_time = time.time()
        self.model.train()
        
        # Entra√Ænement s√©quentiel des 3 √©tapes
        all_stage_metrics = {}
        
        # √âTAPE 1: Sans obstacles
        stage_1_metrics = self.train_stage(1, cfg.STAGE_1_EPOCHS)
        all_stage_metrics[1] = stage_1_metrics
        
        # √âTAPE 2: Un obstacle
        stage_2_metrics = self.train_stage(2, cfg.STAGE_2_EPOCHS)
        all_stage_metrics[2] = stage_2_metrics
        
        # √âTAPE 3: Obstacles multiples
        stage_3_metrics = self.train_stage(3, cfg.STAGE_3_EPOCHS)
        all_stage_metrics[3] = stage_3_metrics
        
        # M√©triques globales
        total_time = time.time() - start_time
        total_epochs_actual = sum(metrics['epochs_trained'] for metrics in all_stage_metrics.values())
        
        global_metrics = {
            'total_epochs_planned': cfg.TOTAL_EPOCHS,
            'total_epochs_actual':  total_epochs_actual,
            'total_time_seconds':   total_time,
            'total_time_formatted': f"{total_time / 60:.1f} min",
            'stage_metrics':        all_stage_metrics,
            'final_loss':           stage_3_metrics['final_loss'],
            'all_stages_converged': all(m['convergence_met'] for m in all_stage_metrics.values()),
            'global_history':       self.global_history,
            'stage_histories':      self.stage_histories,
            'stage_start_epochs':   self.stage_start_epochs  # AJOUT de la cl√© manquante
        }
        
        print(f"\nüéâ === ENTRA√éNEMENT MODULAIRE TERMIN√â ===")
        print(f"‚è±Ô∏è  Temps total: {total_time / 60:.1f} minutes")
        print(f"üìä √âpoques totales: {total_epochs_actual}/{cfg.TOTAL_EPOCHS}")
        print(f"üéØ Toutes √©tapes converg√©es: {'‚úÖ OUI' if global_metrics['all_stages_converged'] else '‚ùå NON'}")
        print(f"üìâ Perte finale: {global_metrics['final_loss']:.6f}")
        
        # Sauvegarde du mod√®le final et des m√©triques
        self.save_final_model(global_metrics)
        
        return global_metrics
    
    
    def save_stage_checkpoint(self, stage: int, metrics: Dict[str, Any]):
        """Sauvegarde le checkpoint d'une √©tape."""
        stage_dir = Path(cfg.OUTPUT_DIR) / f"stage_{stage}"
        stage_dir.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarde du mod√®le
        model_path = stage_dir / "model_checkpoint.pth"
        torch.save({
            'model_state_dict':     self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'stage':                stage,
            'metrics':              metrics,
            'config':               cfg.__dict__
        }, model_path)
        
        # Sauvegarde des m√©triques en JSON
        metrics_path = stage_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2, default=str)
        
        print(f"üíæ Checkpoint √©tape {stage} sauvegard√©: {stage_dir}")
    
    
    def save_final_model(self, global_metrics: Dict[str, Any]):
        """Sauvegarde le mod√®le final et toutes les m√©triques."""
        # Mod√®le final
        final_model_path = Path(cfg.OUTPUT_DIR) / "final_model.pth"
        torch.save({
            'model_state_dict':     self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'global_metrics':       global_metrics,
            'config':               cfg.__dict__
        }, final_model_path)
        
        # M√©triques compl√®tes
        full_metrics_path = Path(cfg.OUTPUT_DIR) / "complete_metrics.json"
        with open(full_metrics_path, 'w') as f:
            json.dump(global_metrics, f, indent=2, default=str)
        
        print(f"üíæ Mod√®le final et m√©triques sauvegard√©s: {cfg.OUTPUT_DIR}")


# =============================================================================
# Syst√®me de visualisation progressive (NOUVEAU)
# =============================================================================

class ProgressiveVisualizer:
    """
    Syst√®me de visualisation avanc√© pour l'apprentissage modulaire.
    G√©n√®re des animations et graphiques comparatifs par √©tape.
    """
    
    
    def __init__(self, interactive: bool = interactive_mode):
        self.interactive = interactive
        self.frame_data = {}  # Donn√©es par √©tape
    
    
    def visualize_stage_results(self, model: ImprovedNCA, stage: int) -> None:
        """
        Visualise les r√©sultats d'une √©tape sp√©cifique.

        Args:
            model: Mod√®le NCA entra√Æn√©
            stage: Num√©ro d'√©tape √† visualiser
        Returns:
            Dictionnaire avec les donn√©es de visualisation
        """
        print(f"\nüé® G√©n√©ration des visualisations pour l'√©tape {stage}...")
        
        # G√©n√©ration de la s√©quence de test avec seed fixe
        torch.manual_seed(VISUALIZATION_SEED)
        np.random.seed(VISUALIZATION_SEED)
        
        target_seq, source_mask, obstacle_mask = simulator.generate_stage_sequence(
                stage=stage,
                n_steps=cfg.POSTVIS_STEPS,
                size=cfg.GRID_SIZE
        )
        
        # Pr√©diction du mod√®le
        model.eval()
        updater = OptimizedNCAUpdater(model)
        
        # Simulation NCA avec torch.no_grad() pour √©viter le gradient
        nca_sequence = []
        grid_pred = torch.zeros_like(target_seq[0])
        grid_pred[source_mask] = cfg.SOURCE_INTENSITY
        nca_sequence.append(grid_pred.clone())
        
        with torch.no_grad():  # D√©sactive le calcul de gradient pour les visualisations
            for _ in range(cfg.POSTVIS_STEPS):
                grid_pred = updater.step(grid_pred, source_mask, obstacle_mask)
                nca_sequence.append(grid_pred.clone())
        
        # Cr√©ation des visualisations avec .detach() pour s√©curit√©
        vis_data = {
            'stage':           stage,
            'target_sequence': [t.detach().cpu().numpy() for t in target_seq],
            'nca_sequence':    [t.detach().cpu().numpy() for t in nca_sequence],
            'source_mask':     source_mask.detach().cpu().numpy(),
            'obstacle_mask':   obstacle_mask.detach().cpu().numpy(),
            'vis_seed':        VISUALIZATION_SEED,
        }
        
        # Sauvegarde des animations
        self._create_stage_animations(vis_data)
        self._create_stage_convergence_plot(vis_data)
        
        model.train()
        return
    
    
    def _create_stage_animations(self, vis_data: Dict[str, Any]):
        """Cr√©e les animations GIF pour une √©tape."""
        stage = vis_data['stage']
        stage_dir = Path(cfg.OUTPUT_DIR) / f"stage_{stage}"
        stage_dir.mkdir(parents=True, exist_ok=True)
        
        # Animation comparative
        self._save_comparison_gif(
                vis_data['target_sequence'],
                vis_data['nca_sequence'],
                vis_data['obstacle_mask'],
                stage_dir / f"animation_comparaison_√©tape_{stage}.gif",
                f"√âtape {stage} - Comparaison Cible vs NCA"
        )
        
        # Animation NCA seule
        self._save_single_gif(
                vis_data['nca_sequence'],
                vis_data['obstacle_mask'],
                stage_dir / f"animation_nca_√©tape_{stage}.gif",
                f"√âtape {stage} - Pr√©diction NCA"
        )
        
        print(f"‚úÖ Animations √©tape {stage} sauvegard√©es dans {stage_dir}")
    
    
    def _create_stage_convergence_plot(self, vis_data: Dict[str, Any]):
        """Cr√©e le graphique de convergence pour une √©tape."""
        stage = vis_data['stage']
        stage_dir = Path(cfg.OUTPUT_DIR) / f"stage_{stage}"
        
        target_seq = vis_data['target_sequence']
        nca_seq = vis_data['nca_sequence']
        
        # Calcul de l'erreur temporelle
        errors = []
        for t in range(min(len(target_seq), len(nca_seq))):
            error = np.mean((target_seq[t] - nca_seq[t]) ** 2)
            errors.append(error)
        
        # Graphique
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(errors, 'b-', linewidth=2, label='Erreur MSE')
        ax.axhline(y=cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05),
                   color='r', linestyle='--', label=f'Seuil convergence √©tape {stage}')
        
        ax.set_xlabel('Pas de temps')
        ax.set_ylabel('Erreur MSE')
        ax.set_title(f'Convergence √âtape {stage} - Seed {vis_data["vis_seed"]}')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        convergence_path = stage_dir / f"convergence_√©tape_{stage}.png"
        plt.savefig(convergence_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Graphique de convergence √©tape {stage} sauvegard√©: {convergence_path}")
    
    
    def _save_comparison_gif(self, target_seq: List[np.ndarray], nca_seq: List[np.ndarray],
                             obstacle_mask: np.ndarray, filepath: Path, title: str):
        """Sauvegarde un GIF de comparaison c√¥te √† c√¥te."""
        import matplotlib.animation as animation
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        
        def animate(frame):
            ax1.clear()
            ax2.clear()
            
            # Cible
            im1 = ax1.imshow(target_seq[frame], cmap='hot', vmin=0, vmax=1)
            ax1.contour(obstacle_mask, levels=[0.5], colors='cyan', linewidths=2)
            ax1.set_title(f'Cible - t={frame}')
            ax1.set_xticks([])
            ax1.set_yticks([])
            
            # NCA
            im2 = ax2.imshow(nca_seq[frame], cmap='hot', vmin=0, vmax=1)
            ax2.contour(obstacle_mask, levels=[0.5], colors='cyan', linewidths=2)
            ax2.set_title(f'NCA - t={frame}')
            ax2.set_xticks([])
            ax2.set_yticks([])
            
            return [im1, im2]
        
        
        n_frames = min(len(target_seq), len(nca_seq))
        ani = animation.FuncAnimation(fig, animate, frames=n_frames, interval=200, blit=False)
        ani.save(filepath, writer='pillow', fps=5)
        plt.close()
    
    
    def _save_single_gif(self, sequence: List[np.ndarray], obstacle_mask: np.ndarray,
                         filepath: Path, title: str):
        """Sauvegarde un GIF d'une s√©quence unique."""
        import matplotlib.animation as animation
        
        fig, ax = plt.subplots(figsize=(8, 8))
        
        
        def animate(frame):
            ax.clear()
            im = ax.imshow(sequence[frame], cmap='hot', vmin=0, vmax=1)
            ax.contour(obstacle_mask, levels=[0.5], colors='cyan', linewidths=2)
            ax.set_title(f'{title} - t={frame}')
            ax.set_xticks([])
            ax.set_yticks([])
            return [im]
        
        
        ani = animation.FuncAnimation(fig, animate, frames=len(sequence), interval=200, blit=False)
        ani.save(filepath, writer='pillow', fps=5)
        plt.close()
    
    
    def create_curriculum_summary(self, global_metrics: Dict[str, Any]):
        """Cr√©e un r√©sum√© visuel complet du curriculum d'apprentissage."""
        print("\nüé® G√©n√©ration du r√©sum√© visuel du curriculum...")
        
        # Graphique de progression globale
        self._plot_curriculum_progression(global_metrics)
        
        # Comparaison inter-√©tapes
        self._plot_stage_comparison(global_metrics)
        
        # M√©triques de performance
        self._plot_performance_metrics(global_metrics)
        
        print("‚úÖ R√©sum√© visuel complet g√©n√©r√©")
    
    
    def _plot_curriculum_progression(self, metrics: Dict[str, Any]):
        """Graphique de la progression globale du curriculum."""
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 15))
        
        # Historique des pertes avec codes couleur par √©tape
        losses = metrics['global_history']['losses']
        stages = metrics['global_history']['stages']
        epochs = metrics['global_history']['epochs']
        
        stage_colors = {1: 'green', 2: 'orange', 3: 'red'}
        
        for stage in [1, 2, 3]:
            stage_indices = [i for i, s in enumerate(stages) if s == stage]
            stage_losses = [losses[i] for i in stage_indices]
            stage_epochs = [epochs[i] for i in stage_indices]
            
            if stage_losses:
                ax1.plot(stage_epochs, stage_losses,
                         color=stage_colors[stage],
                         label=f'√âtape {stage}',
                         linewidth=2)
        
        # Seuils de convergence
        for stage in [1, 2, 3]:
            threshold = cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05)
            ax1.axhline(y=threshold, color=stage_colors[stage],
                        linestyle='--', alpha=0.7,
                        label=f'Seuil √©tape {stage}')
        
        ax1.set_xlabel('√âpoque')
        ax1.set_ylabel('Perte MSE')
        ax1.set_title('Progression du Curriculum d\'Apprentissage')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')
        
        # Learning rate par √©tape
        for stage in [1, 2, 3]:
            stage_history = metrics['stage_histories'][stage]
            if stage_history['lr']:
                stage_epochs_local = [metrics['stage_start_epochs'].get(stage, 0) + e
                                      for e in stage_history['epochs']]
                ax2.plot(stage_epochs_local, stage_history['lr'],
                         color=stage_colors[stage],
                         label=f'LR √âtape {stage}',
                         linewidth=2)
        
        ax2.set_xlabel('√âpoque')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('√âvolution du Learning Rate par √âtape')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')
        
        # Acc√©l√©ration du Learning Rate (d√©riv√©e seconde) pour d√©tecter les changements d'acc√©l√©ration
        for stage in [1, 2, 3]:
            stage_history = metrics['stage_histories'][stage]
            if stage_history['lr'] and len(stage_history['lr']) > 2:  # Besoin d'au moins 3 points pour d√©riv√©e seconde
                # Calcul de la d√©riv√©e premi√®re (vitesse)
                lr_values = stage_history['lr']
                lr_velocity = []
                
                for i in range(1, len(lr_values)):
                    velocity = lr_values[i] - lr_values[i-1]
                    lr_velocity.append(velocity)
                
                # Calcul de la d√©riv√©e seconde (acc√©l√©ration de l'acc√©l√©ration)
                lr_acceleration = []
                for i in range(1, len(lr_velocity)):
                    acceleration = lr_velocity[i] - lr_velocity[i-1]
                    lr_acceleration.append(acceleration)
                
                # √âpoques correspondantes (on commence √† l'√©poque 2 car on a besoin de 3 points pour la d√©riv√©e seconde)
                stage_epochs_acceleration = [metrics['stage_start_epochs'].get(stage, 0) + e
                                           for e in stage_history['epochs'][2:]]
                
                if lr_acceleration:
                    ax3.plot(stage_epochs_acceleration, lr_acceleration,
                             color=stage_colors[stage],
                             label=f'Acc√©l√©ration LR √âtape {stage}',
                             linewidth=2,
                             marker='o', markersize=3, alpha=0.7)
                    
                    # Ligne de r√©f√©rence √† z√©ro pour identifier les changements d'acc√©l√©ration
                    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=1)
                    
                    # Zone n√©gative (d√©c√©l√©ration) en rouge transparent
                    negative_mask = [a < 0 for a in lr_acceleration]
                    if any(negative_mask):
                        ax3.fill_between(stage_epochs_acceleration, lr_acceleration, 0,
                                       where=negative_mask,
                                       alpha=0.2, color='red',
                                       label='Zone de d√©c√©l√©ration' if stage == 1 else "")
                    
                    # Zone positive (acc√©l√©ration croissante) en vert transparent
                    positive_mask = [a > 0 for a in lr_acceleration]
                    if any(positive_mask):
                        ax3.fill_between(stage_epochs_acceleration, lr_acceleration, 0,
                                       where=positive_mask,
                                       alpha=0.2, color='green',
                                       label='Zone d\'acc√©l√©ration' if stage == 1 else "")
                    
                    # D√©tection et marquage des points d'inflexion
                    inflection_points_epochs = []
                    inflection_points_values = []
                    
                    for i in range(1, len(lr_acceleration)):
                        # Point d'inflexion = changement de signe dans l'acc√©l√©ration
                        prev_accel = lr_acceleration[i-1]
                        curr_accel = lr_acceleration[i]
                        
                        # V√©rifier si on traverse z√©ro (changement de signe)
                        if (prev_accel > 0 and curr_accel < 0) or (prev_accel < 0 and curr_accel > 0):
                            # Filtre tr√®s l√©ger pour √©viter seulement le bruit extr√™me
                            if abs(prev_accel) > 1e-12 or abs(curr_accel) > 1e-12:
                                inflection_epoch = stage_epochs_acceleration[i]
                                inflection_value = curr_accel
                                inflection_points_epochs.append(inflection_epoch)
                                inflection_points_values.append(inflection_value)
                    
                    # Marquer les points d'inflexion sur le graphique
                    if inflection_points_epochs:
                        ax3.scatter(inflection_points_epochs, inflection_points_values,
                                  color=stage_colors[stage],
                                  s=80, marker='X',
                                  edgecolors='black', linewidth=2,
                                  label=f'Points d\'inflexion √âtape {stage}' if stage == 1 else "",
                                  zorder=5, alpha=0.9)
                        
                        # Annotations pour les points d'inflexion les plus significatifs
                        for i, (epoch, value) in enumerate(zip(inflection_points_epochs, inflection_points_values)):
                            if i < 3:  # Limite √† 3 annotations par √©tape pour √©viter l'encombrement
                                ax3.annotate(f'Inflexion\n√â{epoch}',
                                           xy=(epoch, value),
                                           xytext=(10, 20 if value > 0 else -30),
                                           textcoords='offset points',
                                           fontsize=8,
                                           color=stage_colors[stage],
                                           bbox=dict(boxstyle='round,pad=0.3',
                                                   facecolor='white',
                                                   edgecolor=stage_colors[stage],
                                                   alpha=0.8),
                                           arrowprops=dict(arrowstyle='->',
                                                         connectionstyle='arc3,rad=0.2',
                                                         color=stage_colors[stage],
                                                         alpha=0.7))
        
        ax3.set_xlabel('√âpoque')
        ax3.set_ylabel('Acc√©l√©ration LR (Œî¬≤LR par √©poque¬≤)')
        ax3.set_title('Acc√©l√©ration du Learning Rate - Points d\'Inflexion et Changements d\'Acc√©l√©ration')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Annotation explicative pour interpr√©ter le graphique d'acc√©l√©ration avec points d'inflexion
        ax3.text(0.02, 0.98,
                'Valeurs n√©gatives = LR d√©c√©l√®re (ralentissement qui s\'acc√©l√®re)\n'
                'Valeurs positives = LR acc√©l√®re (acc√©l√©ration qui s\'intensifie)\n'
                'Valeurs proches de 0 = Vitesse LR constante (acc√©l√©ration stable)\n'
                'X = Points d\'inflexion (changements de dynamique du LR)\n'
                'Les points d\'inflexion indiquent des changements de politique d\'optimisation',
                transform=ax3.transAxes,
                fontsize=9,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(Path(cfg.OUTPUT_DIR) / "curriculum_progression.png",
                    dpi=150, bbox_inches='tight')
        plt.close()
    
    
    def _plot_stage_comparison(self, metrics: Dict[str, Any]):
        """Graphique de comparaison entre √©tapes."""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        stages = [1, 2, 3]
        stage_names = ["Sans obstacles", "Un obstacle", "Obstacles multiples"]
        stage_colors = ['green', 'orange', 'red']
        
        # Pertes finales par √©tape
        final_losses = [metrics['stage_metrics'][s]['final_loss'] for s in stages]
        convergence_status = [metrics['stage_metrics'][s]['convergence_met'] for s in stages]
        
        bars = ax1.bar(stage_names, final_losses, color=stage_colors, alpha=0.7)
        for i, (bar, converged) in enumerate(zip(bars, convergence_status)):
            if converged:
                bar.set_edgecolor('darkgreen')
                bar.set_linewidth(3)
        
        ax1.set_ylabel('Perte finale')
        ax1.set_title('Perte Finale par √âtape')
        ax1.set_yscale('log')
        
        # √âpoques utilis√©es par √©tape
        epochs_used = [metrics['stage_metrics'][s]['epochs_trained'] for s in stages]
        epochs_planned = [cfg.STAGE_1_EPOCHS, cfg.STAGE_2_EPOCHS, cfg.STAGE_3_EPOCHS]
        
        x = np.arange(len(stages))
        width = 0.35
        
        ax2.bar(x - width / 2, epochs_planned, width, label='Pr√©vues', alpha=0.7, color='lightblue')
        ax2.bar(x + width / 2, epochs_used, width, label='Utilis√©es', alpha=0.7, color='darkblue')
        
        ax2.set_xlabel('√âtape')
        ax2.set_ylabel('Nombre d\'√©poques')
        ax2.set_title('√âpoques Pr√©vues vs Utilis√©es')
        ax2.set_xticks(x)
        ax2.set_xticklabels(stage_names, rotation=15)
        ax2.legend()
        
        # Temps de convergence
        convergence_times = []
        for stage in stages:
            stage_losses = metrics['stage_metrics'][stage]['loss_history']
            threshold = cfg.CONVERGENCE_THRESHOLDS.get(stage, 0.05)
            
            convergence_epoch = None
            for i, loss in enumerate(stage_losses):
                if loss < threshold:
                    convergence_epoch = i
                    break
            
            convergence_times.append(convergence_epoch if convergence_epoch else len(stage_losses))
        
        ax3.plot(stages, convergence_times, 'o-', linewidth=2, markersize=8, color='purple')
        ax3.set_xlabel('√âtape')
        ax3.set_ylabel('√âpoque de convergence')
        ax3.set_title('Vitesse de Convergence par √âtape')
        ax3.grid(True, alpha=0.3)
        
        # Efficacit√© (convergence / √©poques utilis√©es)
        efficiency = [(1.0 if convergence_status[i] else 0.5) / max(epochs_used[i], 1)
                      for i in range(len(stages))]
        
        ax4.bar(stage_names, efficiency, color=stage_colors, alpha=0.7)
        ax4.set_ylabel('Efficacit√© (convergence/√©poque)')
        ax4.set_title('Efficacit√© d\'Apprentissage par √âtape')
        
        plt.tight_layout()
        plt.savefig(Path(cfg.OUTPUT_DIR) / "stage_comparison.png",
                    dpi=150, bbox_inches='tight')
        plt.close()
    
    
    def _plot_performance_metrics(self, metrics: Dict[str, Any]):
        """Graphique des m√©triques de performance globales."""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # R√©sum√© textuel des performances
        total_time = metrics['total_time_seconds']
        total_epochs = metrics['total_epochs_actual']
        all_converged = metrics['all_stages_converged']
        final_loss = metrics['final_loss']
        
        summary_text = f"""
üéØ R√âSUM√â ENTRA√éNEMENT MODULAIRE NCA v7__

üìä STATISTIQUES GLOBALES:
   ‚Ä¢ Seed: {SEED}
   ‚Ä¢ Temps total: {total_time / 60:.1f} minutes ({total_time:.1f}s)
   ‚Ä¢ √âpoques totales: {total_epochs}/{cfg.TOTAL_EPOCHS}
   ‚Ä¢ Toutes √©tapes converg√©es: {'‚úÖ OUI' if all_converged else '‚ùå NON'}
   ‚Ä¢ Perte finale: {final_loss:.6f}

üèÜ PERFORMANCE PAR √âTAPE:"""
        
        for stage in [1, 2, 3]:
            stage_data = metrics['stage_metrics'][stage]
            stage_name = {1: "Sans obstacles", 2: "Un obstacle", 3: "Obstacles multiples"}[stage]
            
            summary_text += f"""
   ‚Ä¢ √âtape {stage} ({stage_name}):
     - √âpoques: {stage_data['epochs_trained']} (converg√©e: {'‚úÖ' if stage_data['convergence_met'] else '‚ùå'})
     - Perte finale: {stage_data['final_loss']:.6f}
     - Arr√™t pr√©coce: {'‚úÖ' if stage_data['early_stopped'] else '‚ùå'}"""
        
        summary_text += f"""

üìà ARCHITECTURE:
   ‚Ä¢ Taille grille: {cfg.GRID_SIZE}x{cfg.GRID_SIZE}
   ‚Ä¢ Couches cach√©es: {cfg.HIDDEN_SIZE} neurones, {cfg.N_LAYERS} couches
   ‚Ä¢ Pas temporels NCA: {cfg.NCA_STEPS}
   ‚Ä¢ Taille batch: {cfg.BATCH_SIZE}
        """
        
        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('R√©sum√© Performance Entra√Ænement Modulaire NCA v7__',
                     fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(Path(cfg.OUTPUT_DIR) / "performance_summary.png",
                    dpi=150, bbox_inches='tight')
        plt.close()


# =============================================================================
# Fonction principale d'ex√©cution (NOUVEAU)
# =============================================================================

def main():
    """
    Fonction principale pour l'entra√Ænement modulaire progressif.
    Orchestre tout le processus d'apprentissage par curriculum.
    """
    print(f"\n" + "=" * 80)
    print(f"üöÄ NEURAL CELLULAR AUTOMATON - APPRENTISSAGE MODULAIRE v7__")
    print(f"=" * 80)
    
    try:
        # Initialisation du mod√®le
        print("\nüîß Initialisation du mod√®le...")
        model = ImprovedNCA(
                input_size=11,  # 9 (patch 3x3) + 1 (source) + 1 (obstacle)
                hidden_size=cfg.HIDDEN_SIZE,
                n_layers=cfg.N_LAYERS
        ).to(DEVICE)
        
        print(f"üìä Param√®tres du mod√®le: {sum(p.numel() for p in model.parameters()):,}")
        
        # Initialisation de l'entra√Æneur modulaire
        print("üéØ Initialisation de l'entra√Æneur modulaire...")
        trainer = ModularTrainer(model)
        
        # Lancement de l'entra√Ænement complet
        print("üöÄ Lancement de l'entra√Ænement modulaire...")
        global_metrics = trainer.train_full_curriculum()
        
        # G√©n√©ration des visualisations progressives
        print("\nüé® G√©n√©ration des visualisations...")
        visualizer = ProgressiveVisualizer(interactive_mode)
        
        # Visualisation par √©tape avec le mod√®le final
        for stage in [1, 2, 3]:
            visualizer.visualize_stage_results(model, stage)
        
        # R√©sum√© visuel complet du curriculum
        visualizer.create_curriculum_summary(global_metrics)
        
        # Rapport final
        print(f"\n" + "=" * 80)
        print(f"üéâ ENTRA√éNEMENT MODULAIRE TERMIN√â AVEC SUCC√àS!")
        print(f"=" * 80)
        print(f"üìÅ R√©sultats sauvegard√©s dans: {cfg.OUTPUT_DIR}")
        print(f"‚è±Ô∏è  Temps total: {global_metrics['total_time_formatted']}")
        print(f"üìä √âpoques: {global_metrics['total_epochs_actual']}/{global_metrics['total_epochs_planned']}")
        print(f"üéØ Convergence: {'‚úÖ TOUTES' if global_metrics['all_stages_converged'] else '‚ùå PARTIELLE'}")
        print(f"üìâ Perte finale: {global_metrics['final_loss']:.6f}")
        
        # D√©tail par √©tape
        print(f"\nüìã D√âTAIL PAR √âTAPE:")
        for stage in [1, 2, 3]:
            stage_data = global_metrics['stage_metrics'][stage]
            stage_name = {1: "Sans obstacles", 2: "Un obstacle", 3: "Obstacles multiples"}[stage]
            status = "‚úÖ CONVERG√âE" if stage_data['convergence_met'] else "‚ùå NON CONVERG√âE"
            print(f"   √âtape {stage} ({stage_name}): {status} - {stage_data['final_loss']:.6f}")
        
        print(f"\nüé® Fichiers de visualisation g√©n√©r√©s:")
        print(f"   ‚Ä¢ Animations par √©tape: stage_X/")
        print(f"   ‚Ä¢ Progression curriculum: curriculum_progression.png")
        print(f"   ‚Ä¢ Comparaison √©tapes: stage_comparison.png")
        print(f"   ‚Ä¢ R√©sum√© performance: performance_summary.png")
        print(f"   ‚Ä¢ M√©triques compl√®tes: complete_metrics.json")
        
        return global_metrics
    
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur")
        return None
    except Exception as e:
        print(f"\n‚ùå ERREUR lors de l'entra√Ænement:")
        print(f"   {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # Ex√©cution du programme principal
    results = main()
    
    if results is not None:
        print(f"\nüéØ Programme termin√© avec succ√®s!")
        print(f"üìä R√©sultats disponibles dans la variable 'results'")
    else:
        print(f"\n‚ùå Programme termin√© avec erreurs")
        exit(1)
