"""
Gestionnaire de stages modulaire pour le NCA.
Orchestre l'ex√©cution des stages de mani√®re d√©coupl√©e et extensible.

REFACTORING v11:
- Utilise StageAutoRegistry pour l'auto-d√©couverte des stages
- Utilise StageSequence pour d√©finir l'ordre d'ex√©cution
- Plus de couplage avec les num√©ros de stages
- Identification par slug uniquement
"""

from typing import Dict, Any, List, Optional
import json
from pathlib import Path

from .base_stage import BaseStage
from .registry import StageAutoRegistry
from .sequence import StageSequence


class ModularStageManager:
    """
    Gestionnaire principal des stages modulaires.
    Utilise maintenant le syst√®me de registry auto-d√©couvert et la s√©quence configurable.
    """
    
    def __init__(self, global_config: Any, device: str = "cpu",
                 custom_sequence: Optional[List[str]] = None):
        """
        Initialise le gestionnaire de stages.
        
        Args:
            global_config: Configuration globale du syst√®me
            device: Device PyTorch ('cpu' ou 'cuda')
            custom_sequence: S√©quence personnalis√©e de slugs (optionnel)
        """
        self.global_config = global_config
        self.device = device
        
        # üéØ Nouveau syst√®me bas√© sur registry et s√©quence
        self.registry = StageAutoRegistry()
        self.sequence = StageSequence(custom_sequence)
        
        # Stages actifs index√©s par SLUG, pas par num√©ro
        self.active_stages: Dict[str, BaseStage] = {}
        self.execution_history: List[Dict[str, Any]] = []
        
        # Configuration d'ex√©cution
        self.total_epochs_planned = global_config.TOTAL_EPOCHS
        self.epochs_per_stage = self._calculate_epochs_per_stage()
    
    def _calculate_epochs_per_stage(self) -> Dict[str, int]:
        """
        Calcule le nombre d'√©poques par stage selon leur configuration.
        Utilise maintenant les SLUGS au lieu des num√©ros.
        """
        epochs_per_stage = {}
        
        print("\n=== Calcul d√©taill√© de la r√©partition des √©poques par stage ===")
        print(f"Nombre total d'√©poques planifi√©es: {self.total_epochs_planned}")
        
        # Premi√®re passe : calcul bas√© sur les ratios
        for slug in self.sequence.get_sequence():
            temp_stage = self.registry.create_stage(slug, self.device)
            ratio = temp_stage.config.epochs_ratio
            
            raw_epochs = self.total_epochs_planned * ratio
            epochs = int(raw_epochs)
            
            epochs_per_stage[slug] = epochs
            
            print(f"  Stage '{slug}': ratio={ratio:.3f}, "
                  f"calcul={self.total_epochs_planned}*{ratio:.3f}={raw_epochs:.2f}, "
                  f"arrondi={epochs}")
        
        # Deuxi√®me passe : ajustement pour atteindre exactement le total pr√©vu
        total_calculated = sum(epochs_per_stage.values())
        print(f"\nSomme initiale des √©poques: {total_calculated}/{self.total_epochs_planned}")
        
        if total_calculated != self.total_epochs_planned:
            last_slug = self.sequence.get_sequence()[-1]
            adjustment = self.total_epochs_planned - total_calculated
            
            print(f"  Ajustement requis: {adjustment} √©poques")
            print(f"  Appliqu√© au dernier stage ('{last_slug}'): "
                  f"{epochs_per_stage[last_slug]} ‚Üí {epochs_per_stage[last_slug] + adjustment}")
            
            epochs_per_stage[last_slug] += adjustment
        else:
            print("  Aucun ajustement n√©cessaire, la somme est exacte")
        
        # V√©rification finale
        print("\n=== R√©partition finale des √©poques ===")
        for slug in self.sequence.get_sequence():
            status = "‚ö†Ô∏è AUCUNE √âPOQUE" if epochs_per_stage[slug] == 0 else "‚úì"
            print(f"  Stage '{slug}': {epochs_per_stage[slug]} √©poques {status}")
        
        print(f"Total final: {sum(epochs_per_stage.values())}/{self.total_epochs_planned}\n")
        
        return epochs_per_stage
    
    def initialize_stage(self, slug: str) -> BaseStage:
        """
        Initialise un stage sp√©cifique par son slug.
        
        Args:
            slug: Identifiant unique du stage
            
        Returns:
            Instance du stage initialis√©
        """
        print(f"\nüéØ Initialisation du Stage '{slug}'...")
        
        # Param√®tres sp√©cialis√©s selon le stage et ses capacit√©s
        stage_class = self.registry.get_stage(slug)
        stage_kwargs = self._get_stage_kwargs(stage_class)
        
        # Cr√©ation du stage
        stage = self.registry.create_stage(slug, self.device, **stage_kwargs)
        
        # Configuration des donn√©es d'entra√Ænement
        training_config = stage.prepare_training_data(self.global_config)
        stage.training_config = training_config
        
        # Stockage dans les stages actifs
        self.active_stages[slug] = stage
        
        print(f"‚úÖ Stage '{slug}' ({stage.config.description}) initialis√©")
        print(f"   ‚è±Ô∏è  √âpoques pr√©vues: {self.epochs_per_stage[slug]}")
        print(f"   üéØ Seuil convergence: {stage.config.convergence_threshold}")
        
        return stage
    
    def _get_stage_kwargs(self, stage_class) -> Dict[str, Any]:
        """
        D√©termine les param√®tres √† passer au constructeur d'un stage.
        
        Args:
            stage_class: Classe du stage
            
        Returns:
            Dictionnaire des param√®tres compatibles
        """
        import inspect
        
        sig = inspect.signature(stage_class.__init__)
        param_names = list(sig.parameters.keys())
        
        stage_kwargs = {}
        
        if 'min_obstacle_size' in param_names and hasattr(self.global_config, 'MIN_OBSTACLE_SIZE'):
            stage_kwargs['min_obstacle_size'] = self.global_config.MIN_OBSTACLE_SIZE
        
        if 'max_obstacle_size' in param_names and hasattr(self.global_config, 'MAX_OBSTACLE_SIZE'):
            stage_kwargs['max_obstacle_size'] = self.global_config.MAX_OBSTACLE_SIZE
        
        return stage_kwargs
    
    def execute_stage(self, slug: str, trainer_callback,
                     early_stopping: bool = True) -> Dict[str, Any]:
        """
        Ex√©cute un stage sp√©cifique par son slug.
        
        Args:
            slug: Identifiant unique du stage
            trainer_callback: Fonction callback pour l'entra√Ænement
            early_stopping: Active l'arr√™t pr√©coce
            
        Returns:
            M√©triques de l'ex√©cution du stage
            
        Raises:
            ValueError: Si le nombre d'√©poques allou√© au stage est z√©ro
        """
        if slug not in self.active_stages:
            stage = self.initialize_stage(slug)
        else:
            stage = self.active_stages[slug]
        
        max_epochs = self.epochs_per_stage[slug]
        
        if max_epochs == 0:
            raise ValueError(f"Le Stage '{slug}' ({stage.config.description}) a 0 √©poque allou√©e. "
                           f"Chaque stage doit avoir au moins une √©poque d'entra√Ænement.")
        
        print(f"\nüöÄ === EX√âCUTION STAGE '{slug}' - {stage.config.description.upper()} ===")
        print(f"üìä Maximum {max_epochs} √©poques")
        
        stage.reset_training_history()
        
        execution_result = trainer_callback(stage, max_epochs, early_stopping)
        
        stage_summary = stage.get_stage_summary()
        stage_summary.update(execution_result)
        
        # Sauvegarde automatique du checkpoint si configur√©
        if hasattr(self.global_config, 'SAVE_STAGE_CHECKPOINTS') and self.global_config.SAVE_STAGE_CHECKPOINTS:
            self.save_stage_checkpoint_with_callback(slug, trainer_callback)
        
        # Enregistrement dans l'historique
        self.execution_history.append({
            'stage_slug': slug,
            'timestamp': self._get_timestamp(),
            'summary': stage_summary
        })
        
        print(f"‚úÖ === STAGE '{slug}' TERMIN√â ===")
        
        return stage_summary
    
    def save_stage_checkpoint_with_callback(self, slug: str, trainer_callback):
        """Sauvegarde le checkpoint d'un stage avec callback vers le trainer."""
        if hasattr(trainer_callback, '__self__'):
            trainer = trainer_callback.__self__
            trainer.save_stage_checkpoint(slug)
    
    def execute_full_curriculum(self, trainer_callback) -> Dict[str, Any]:
        """
        Ex√©cute le curriculum complet de tous les stages.
        
        Args:
            trainer_callback: Fonction callback pour l'entra√Ænement
            
        Returns:
            M√©triques globales de l'ex√©cution
        """
        print(f"\nüöÄ === D√âBUT CURRICULUM MODULAIRE ===")
        print(f"üîÑ S√©quence: {' ‚Üí '.join(self.sequence.get_sequence())}")
        print(f"üìä √âpoques totales: {self.total_epochs_planned}")
        
        import time
        start_time = time.time()
        
        all_stage_results = {}
        total_epochs_actual = 0
        
        # Ex√©cution s√©quentielle des stages
        for slug in self.sequence.get_sequence():
            stage_result = self.execute_stage(slug, trainer_callback)
            all_stage_results[slug] = stage_result
            total_epochs_actual += stage_result.get('epochs_trained', 0)
        
        # M√©triques globales
        total_time = time.time() - start_time
        
        global_results = {
            'curriculum_sequence': self.sequence.get_sequence(),
            'total_epochs_planned': self.total_epochs_planned,
            'total_epochs_actual': total_epochs_actual,
            'total_time_seconds': total_time,
            'total_time_formatted': f"{total_time/60:.1f} min",
            'stage_results': all_stage_results,
            'all_stages_converged': all(
                result.get('converged', False)
                for result in all_stage_results.values()
            ),
            'execution_history': self.execution_history
        }
        
        # R√©sum√© final
        final_slug = self.sequence.get_sequence()[-1]
        final_loss = all_stage_results[final_slug].get('final_loss', float('inf'))
        global_results['final_loss'] = final_loss
        
        print(f"\nüéâ === CURRICULUM MODULAIRE TERMIN√â ===")
        print(f"‚è±Ô∏è  Temps total: {total_time/60:.1f} minutes")
        print(f"üìä √âpoques: {total_epochs_actual}/{self.total_epochs_planned}")
        print(f"üéØ Convergence globale: {'‚úÖ' if global_results['all_stages_converged'] else '‚ùå'}")
        print(f"üìâ Perte finale: {final_loss:.6f}")
        
        return global_results
    
    def add_custom_stage(self, slug: str, stage_class):
        """
        Ajoute un stage personnalis√© au gestionnaire.
        
        Args:
            slug: Identifiant unique pour le nouveau stage
            stage_class: Classe du stage personnalis√©
        """
        # Note: Avec l'auto-discovery, il suffit de cr√©er un r√©pertoire
        # Mais on garde cette m√©thode pour la compatibilit√©
        print(f"‚ö†Ô∏è  add_custom_stage est d√©pr√©ci√©. Cr√©ez simplement un r√©pertoire stages/{slug}/")
    
    def save_stage_checkpoint(self, slug: str, model_state: Dict[str, Any],
                             output_dir: Path):
        """
        Sauvegarde le checkpoint d'un stage.
        
        Args:
            slug: Identifiant du stage
            model_state: √âtat du mod√®le √† sauvegarder
            output_dir: R√©pertoire de sortie
        """
        if slug not in self.active_stages:
            print(f"‚ö†Ô∏è  Stage '{slug}' non actif, checkpoint ignor√©")
            return
        
        stage = self.active_stages[slug]
        stage_dir = output_dir / f"stage_{slug}"
        stage_dir.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarde des donn√©es du stage
        checkpoint_data = {
            'stage_slug': slug,
            'stage_config': stage.config.__dict__,
            'training_history': stage.training_history,
            'model_state': model_state,
            'timestamp': self._get_timestamp()
        }
        
        # Sauvegarde sp√©cialis√©e pour les stages avec statistiques d'intensit√©
        if hasattr(stage, 'get_intensity_statistics'):
            checkpoint_data['intensity_statistics'] = stage.get_intensity_statistics()
        
        if hasattr(stage, 'get_attenuation_statistics'):
            checkpoint_data['attenuation_statistics'] = stage.get_attenuation_statistics()
        
        # Fichiers de sauvegarde
        checkpoint_path = stage_dir / "stage_checkpoint.json"
        with open(checkpoint_path, 'w') as f:
            json.dump(checkpoint_data, f, indent=2, default=str)
        
        print(f"üíæ Checkpoint Stage '{slug}' sauvegard√©: {stage_dir}")
    
    def get_stage(self, slug: str) -> Optional[BaseStage]:
        """R√©cup√®re un stage actif par son slug."""
        return self.active_stages.get(slug)
    
    def _get_timestamp(self) -> str:
        """G√©n√®re un timestamp pour les logs."""
        import datetime
        return datetime.datetime.now().isoformat()
    
    def get_curriculum_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© du curriculum configur√©."""
        return {
            'stage_sequence': self.sequence.get_sequence(),
            'epochs_per_stage': self.epochs_per_stage,
            'total_epochs': self.total_epochs_planned,
            'available_stages': self.registry.list_stages(),
            'active_stages': list(self.active_stages.keys())
        }
