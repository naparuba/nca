# Description du Code - NCA Modulaire avec Intensit√©s Variables (Version 8__)

## Vue d'ensemble du syst√®me

Ce document d√©crit l'architecture et l'API des classes du syst√®me **Neural Cellular Automaton (NCA) Modulaire avec Intensit√©s Variables** (Version 8__). Le syst√®me impl√©mente un apprentissage progressif en 4 √©tapes avec la nouveaut√© majeure de l'**√©tape 4** qui introduit les **intensit√©s de source variables**.

### Innovation principale (Version 8__)
- **√âtape 4 avec intensit√©s variables** : Le syst√®me apprend avec des intensit√©s de source diff√©rentes entre les simulations (mais fixes pendant chaque simulation)
- **Curriculum progressif d'intensit√©** : De [0.5, 1.0] vers [0.0, 1.0] au fil de l'entra√Ænement
- **Support complet des intensit√©s** dans tous les composants (simulateur, updaters, visualisations)

## üìã Classes principales et API

### 1. `ModularConfig` - Configuration √©tendue

**R√¥le** : Configuration centralis√©e pour l'apprentissage modulaire avec intensit√©s variables.

**API publique** :
```python
def __init__(self, seed: int = 123)
```

**Propri√©t√©s principales** :
- `STAGE_1_RATIO = 0.3` : 30% des √©poques pour l'√©tape 1 (sans obstacles)
- `STAGE_2_RATIO = 0.3` : 30% des √©poques pour l'√©tape 2 (un obstacle)  
- `STAGE_3_RATIO = 0.2` : 20% des √©poques pour l'√©tape 3 (obstacles multiples)
- `STAGE_4_RATIO = 0.2` : 20% des √©poques pour l'√©tape 4 (intensit√©s variables) ‚≠ê **NOUVEAU**

**Configuration intensit√©s variables (NOUVEAU)** :
```python
VARIABLE_INTENSITY_TRAINING = True
MIN_SOURCE_INTENSITY = 0.0    # Intensit√© minimale (√©teint)
MAX_SOURCE_INTENSITY = 1.0    # Intensit√© maximale (standard)
STAGE_4_SOURCE_CONFIG = {
    'intensity_distribution': 'uniform',
    'sample_per_simulation': True,
    'fixed_during_simulation': True,
    'intensity_range_expansion': True,
    'initial_range': [0.5, 1.0],  # Plage initiale
    'final_range': [0.0, 1.0]     # Plage finale
}
```

**Seuils de convergence par √©tape** :
```python
CONVERGENCE_THRESHOLDS = {
    1: 0.0002,  # Tr√®s strict pour base solide
    2: 0.0002,  # Tr√®s strict pour adaptation obstacles
    3: 0.0002,  # Strict pour complexit√©
    4: 0.0002   # Strict pour intensit√©s variables ‚≠ê NOUVEAU
}
```

---

### 2. `ProgressiveObstacleManager` - Gestionnaire d'obstacles progressifs

**R√¥le** : G√©n√®re des environnements d'obstacles adapt√©s √† chaque √©tape d'apprentissage.

**API publique** :
```python
def __init__(self, device: str = cfg.DEVICE)

def generate_stage_environment(self, stage: int, size: int, source_pos: Tuple[int, int], 
                              seed: Optional[int] = None) -> torch.Tensor
```

**M√©thodes sp√©cialis√©es par √©tape** :
- `_generate_stage_1_environment()` : Grille vide (apprentissage de base)
- `_generate_stage_2_environment()` : Un seul obstacle (contournement)
- `_generate_stage_3_environment()` : 2-4 obstacles (complexit√©)
- `_generate_stage_4_environment()` : 1-2 obstacles (intensit√©s variables) ‚≠ê **NOUVEAU**

**Validation intelligente** :
```python
def _validate_connectivity(self, obstacle_mask: torch.Tensor, source_pos: Tuple[int, int]) -> bool
def get_difficulty_metrics(self, stage: int, obstacle_mask: torch.Tensor) -> Dict[str, float]
```

**Logique progressive** :
- **√âtape 1** : Aucun obstacle pour apprendre la diffusion pure
- **√âtape 2** : Un obstacle avec validation de non-chevauchement source
- **√âtape 3** : Obstacles multiples avec validation de connectivit√© (flood-fill)
- **√âtape 4** : Configuration simplifi√©e pour focus sur intensit√©s variables

---

### 3. `DiffusionSimulator` - Simulateur adapt√© intensit√©s variables

**R√¥le** : Simulateur de diffusion thermique avec support des intensit√©s variables (Version 8__).

**API publique** :
```python
def __init__(self, device: str = cfg.DEVICE)

def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor,
         source_intensity: Optional[float] = None) -> torch.Tensor

def generate_stage_sequence(self, stage: int, n_steps: int, size: int,
                           seed: Optional[int] = None, 
                           source_intensity: Optional[float] = None) -> Tuple[List[torch.Tensor], torch.Tensor, torch.Tensor, Optional[float]]
```

**Innovation Version 8__** :
- **Param√®tre `source_intensity`** : Support intensit√© variable pour √©tape 4
- **Retour √©tendu** : Tuple avec 4 √©l√©ments incluant l'intensit√© utilis√©e
- **Logique adaptative** :
  - **√âtapes 1-3** : `source_intensity=None` ‚Üí intensit√© standard (1.0)
  - **√âtape 4** : `source_intensity` sp√©cifi√©e ‚Üí intensit√© variable

**Algorithme de diffusion** :
```python
# Convolution 3x3 pour diffusion
new_grid = F.conv2d(grid, kernel_3x3, padding=1)
# Contraintes obstacles
new_grid[obstacle_mask] = 0.0
# ‚≠ê NOUVEAU : Support intensit√© variable
if source_intensity is not None:
    new_grid[source_mask] = source_intensity  # √âtape 4
else:
    new_grid[source_mask] = grid[source_mask]  # √âtapes 1-3
```

---

### 4. `ImprovedNCA` - Mod√®le Neural Cellular Automaton

**R√¥le** : R√©seau de neurones profond pour apprendre les r√®gles de diffusion.

**API publique** :
```python
def __init__(self, input_size: int = 11, hidden_size: int = 128, n_layers: int = 3)
def forward(self, x: torch.Tensor) -> torch.Tensor
```

**Architecture** :
- **Entr√©e** : 11 features (patch 3x3 + source + obstacle)
- **Corps** : 3 couches cach√©es avec BatchNorm + ReLU + Dropout
- **Sortie** : 1 valeur (delta) avec Tanh + scaling (0.1)

**Caract√©ristiques** :
- **Stabilit√©** : BatchNorm et scaling des deltas pour √©viter l'instabilit√©
- **R√©gularisation** : Dropout 0.1 pour √©viter le surapprentissage
- **Capacit√©** : ~128k param√®tres pour apprendre les 4 √©tapes

---

### 5. `OptimizedNCAUpdater` & `NCAUpdater` - Application du mod√®le

**R√¥le** : Appliquent le mod√®le NCA sur une grille avec optimisations.

#### `OptimizedNCAUpdater` (Version optimis√©e)
```python
def __init__(self, model: ImprovedNCA, device: str = cfg.DEVICE)
def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor,
         source_intensity: Optional[float] = None) -> torch.Tensor
```

**Innovation Version 8__** :
- **Param√®tre `source_intensity`** : Support intensit√© variable
- **Extraction vectoris√©e** : `F.unfold()` pour patches 3x3 optimis√©e
- **Application s√©lective** : Seulement sur cellules non-obstacles

#### `NCAUpdater` (Version standard)
```python
def step(self, grid: torch.Tensor, source_mask: torch.Tensor, obstacle_mask: torch.Tensor,
         source_intensity: Optional[float] = None) -> torch.Tensor
```

**Logique commune** :
```python
# ‚≠ê NOUVEAU : Support intensit√© variable dans les deux updaters
if source_intensity is not None:
    new_grid[source_mask] = source_intensity  # √âtape 4
else:
    new_grid[source_mask] = grid[source_mask]  # √âtapes 1-3
```

---

### 6. `CurriculumScheduler` - Planificateur de progression

**R√¥le** : G√®re la progression automatique entre les √©tapes avec logique renforc√©e.

**API publique** :
```python
def __init__(self, convergence_thresholds: Dict[int, float], patience: int = 10)

def should_advance_stage(self, current_stage: int, recent_losses: List[float]) -> bool
def adjust_learning_rate(self, optimizer, stage: int, epoch_in_stage: int)
def get_stage_loss_weights(self, stage: int) -> Dict[str, float]
```

**Logique de convergence renforc√©e** :
- **Crit√®re √©tendu** : 10 √©poques minimum (√©tait 5)
- **Convergence ET stabilit√©** : Moyenne < seuil ET variance < 0.001
- **Patience doubl√©e** : 20 √©poques de stagnation avant avancement forc√©
- **Learning rate adaptatif** : R√©duction progressive par √©tape avec cosine decay

**Poids de perte par √©tape** :
```python
weights = {
    1: {'mse': 1.0, 'convergence': 2.0, 'stability': 1.0},
    2: {'mse': 1.0, 'convergence': 1.5, 'stability': 1.5, 'adaptation': 1.0},
    3: {'mse': 1.0, 'convergence': 1.0, 'stability': 2.0, 'robustness': 1.5},
    4: {'mse': 1.0, 'convergence': 1.2, 'stability': 2.5, 'robustness': 2.0}  # ‚≠ê NOUVEAU
}
```

---

### 7. `OptimizedSequenceCache` - Cache par √©tape

**R√¥le** : Cache sp√©cialis√© par √©tape pour optimiser l'entra√Ænement.

**API publique** :
```python
def __init__(self, simulator: DiffusionSimulator, device: str = cfg.DEVICE)

def initialize_stage_cache(self, stage: int)
def get_stage_batch(self, stage: int, batch_size: int)
def shuffle_stage_cache(self, stage: int)
def clear_stage_cache(self, stage: int)
```

**Strat√©gie de cache** :
- **√âtape 1** : 150 s√©quences (base simple)
- **√âtape 2** : 200 s√©quences (vari√©t√© obstacles)
- **√âtape 3** : 250 s√©quences (complexit√© √©lev√©e)
- **√âtape 4** : **PAS DE CACHE** ‚≠ê (intensit√©s variables incompatibles)

**Gestion m√©moire** :
- **Lib√©ration progressive** : Cache √©tape N-1 lib√©r√© quand √©tape N commence
- **M√©lange p√©riodique** : √âvite la m√©morisation de l'ordre

---

### 8. `SimulationIntensityManager` - Gestionnaire d'intensit√©s ‚≠ê **NOUVEAU**

**R√¥le** : G√®re le curriculum d'intensit√©s progressif pour l'√©tape 4.

**API publique** :
```python
def __init__(self, device: str = cfg.DEVICE)

def sample_simulation_intensity(self, epoch_progress: float) -> float
def get_progressive_range(self, epoch_progress: float) -> Tuple[float, float]
def validate_intensity(self, intensity: float) -> float
def get_intensity_statistics(self) -> Dict[str, float]
def clear_history(self)
```

**Curriculum progressif** :
```python
# Progression : 0.0 = d√©but √©tape 4, 1.0 = fin √©tape 4
epoch_progress = epoch / max_epochs

# Interpolation lin√©aire entre plages
# 0% : [0.5, 1.0] ‚Üí Sources moyennes √† fortes
# 50% : [0.25, 1.0] ‚Üí Ajout sources faibles  
# 100% : [0.0, 1.0] ‚Üí Toute la plage (y compris √©teint)
```

**Validation intelligente** :
- **Clipping** : Assure [0.0, 1.0]
- **√âvite quasi-z√©ro** : Si 0 < intensity < 0.001 ‚Üí intensity = 0.001
- **Statistiques** : Historique pour analyse et debugging

---

### 9. `ModularTrainer` - Entra√Æneur principal

**R√¥le** : Orchestre l'entra√Ænement modulaire progressif en 4 √©tapes.

**API publique** :
```python
def __init__(self, model: ImprovedNCA, device: str = cfg.DEVICE)

def train_step(self, target_sequence: List[torch.Tensor], source_mask: torch.Tensor,
               obstacle_mask: torch.Tensor, stage: int, 
               source_intensity: Optional[float] = None) -> float

def train_stage(self, stage: int, max_epochs: int) -> Dict[str, Any]
def train_stage_4(self, max_epochs: int) -> Dict[str, Any]  # ‚≠ê NOUVEAU
def train_full_curriculum(self) -> Dict[str, Any]

def save_stage_checkpoint(self, stage: int, metrics: Dict[str, Any])
def save_final_model(self, global_metrics: Dict[str, Any])
```

**Innovation Version 8__ - M√©thode sp√©cialis√©e `train_stage_4()`** :
```python
for epoch_in_stage in range(max_epochs):
    epoch_progress = epoch_in_stage / max(max_epochs - 1, 1)
    
    for batch_idx in range(cfg.BATCH_SIZE):
        # ‚≠ê √âchantillonne intensit√© pour cette simulation
        current_intensity = self.intensity_manager.sample_simulation_intensity(epoch_progress)
        
        # G√©n√®re s√©quence avec intensit√© fixe
        target_seq, source_mask, obstacle_mask, used_intensity = simulator.generate_stage_sequence(
            stage=4, source_intensity=current_intensity
        )
        
        # Entra√Æne avec cette intensit√© fixe
        loss = self.train_step(target_seq, source_mask, obstacle_mask, stage, current_intensity)
```

**M√©triques √©tendues pour √©tape 4** :
```python
stage_metrics = {
    'stage': 4,
    'epochs_trained': epoch_in_stage + 1,
    'final_loss': final_loss,
    'convergence_met': convergence_met,
    'early_stopped': early_stop,
    'loss_history': stage_losses,
    'intensity_stats_history': intensity_stats_history,  # ‚≠ê NOUVEAU
    'global_intensity_stats': global_intensity_stats     # ‚≠ê NOUVEAU
}
```

**Gestion des historiques** :
- **Par √©tape** : `stage_histories[1-4]` avec pertes, √©poques, learning rates
- **Global** : `global_history` avec progression continue
- **Checkpoints** : Sauvegarde mod√®le + m√©triques √† chaque √©tape

---

### 10. `ProgressiveVisualizer` - Syst√®me de visualisation

**R√¥le** : G√©n√®re visualisations et animations adapt√©es √† chaque √©tape.

**API publique** :
```python
def __init__(self, interactive: bool = interactive_mode)

def visualize_stage_results(self, model: ImprovedNCA, stage: int, 
                           vis_seed: int = 3333) -> Dict[str, Any]
def create_curriculum_summary(self, global_metrics: Dict[str, Any])

# M√©thodes priv√©es pour g√©n√©ration
def _create_stage_animations(self, vis_data: Dict[str, Any])
def _create_stage_convergence_plot(self, vis_data: Dict[str, Any])
def _save_comparison_gif(self, target_seq, nca_seq, obstacle_mask, filepath, title, source_intensity)
def _save_single_gif(self, sequence, obstacle_mask, filepath, title, source_intensity)
```

**Innovation Version 8__ - Affichage d'intensit√©** :
```python
# ‚≠ê Tous les GIFs affichent l'intensit√© dans le titre
ax.set_title(f'{title} - t={frame} (I={source_intensity:.3f})')

# Pour √©tape 4 : intensit√© variable √©chantillonn√©e
# Pour √©tapes 1-3 : intensit√© standard (I=1.000)
```

**Types de visualisations** :
- **Animations comparatives** : Cible vs NCA c√¥te √† c√¥te
- **Animations NCA** : Pr√©diction seule avec obstacles
- **Graphiques de convergence** : Erreur MSE vs temps avec seuils
- **R√©sum√© curriculum** : Progression globale avec codes couleur par √©tape

---

## üîÑ Flux d'ex√©cution principal

### 1. Initialisation
```python
cfg = ModularConfig(seed=123)
model = ImprovedNCA().to(device)
trainer = ModularTrainer(model, device)
```

### 2. Entra√Ænement modulaire (4 √©tapes)
```python
global_metrics = trainer.train_full_curriculum()
# √âtape 1: Sans obstacles (150 √©poques)
# √âtape 2: Un obstacle (150 √©poques) 
# √âtape 3: Obstacles multiples (100 √©poques)
# √âtape 4: Intensit√©s variables (100 √©poques) ‚≠ê NOUVEAU
```

### 3. Visualisations progressives
```python
visualizer = ProgressiveVisualizer()
for stage in [1, 2, 3, 4]:
    visualizer.visualize_stage_results(model, stage)
visualizer.create_curriculum_summary(global_metrics)
```

## üéØ Points cl√©s pour √©volution future

### Architecture modulaire
- **Classes d√©coupl√©es** : Chaque classe a une responsabilit√© claire
- **Interfaces stables** : APIs compatibles pour extensions futures
- **Configuration centralis√©e** : Facilite l'ajout de nouveaux param√®tres

### Support intensit√©s variables
- **Propagation compl√®te** : Tous les composants supportent `source_intensity`
- **R√©trocompatibilit√©** : `source_intensity=None` ‚Üí comportement v7__
- **Extensibilit√©** : Base pour futures extensions (sources multiples, variations temporelles)

### Optimisations et robustesse
- **Cache intelligent** : Optimise √©tapes 1-3, d√©sactiv√© pour √©tape 4
- **Validation rigoureuse** : Connectivit√©, intensit√©s, convergence
- **Gestion erreurs** : Fallbacks et nettoyage m√©moire

### M√©triques et debugging
- **Historiques d√©taill√©s** : Par √©tape et global
- **Statistiques intensit√©** : Analyse compl√®te du curriculum d'intensit√©
- **Visualisations riches** : GIFs avec intensit√©, graphiques multi-√©tapes

Ce syst√®me constitue une base solide pour l'√©volution vers des NCA plus complexes avec gestion avanc√©e des sources variables.
