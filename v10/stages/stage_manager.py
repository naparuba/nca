"""
Gestionnaire de stages modulaire pour le NCA.
Orchestre l'ex√©cution des stages de mani√®re d√©coupl√©e et extensible.
"""

from typing import Dict, Any, List, Optional, Type
import torch
import json
from pathlib import Path

from .base_stage import BaseStage
from .stage1 import Stage1
from .stage2 import Stage2
from .stage3 import Stage3
from .stage4 import Stage4


class StageRegistry:
    """
    Registre des stages disponibles.
    Facilite l'ajout de nouveaux stages sans modification du code existant.
    """
    
    def __init__(self):
        self._stages: Dict[int, Type[BaseStage]] = {}
        self._register_default_stages()
    
    def _register_default_stages(self):
        """Enregistre les stages par d√©faut."""
        self.register_stage(1, Stage1)
        self.register_stage(2, Stage2)
        self.register_stage(3, Stage3)
        self.register_stage(4, Stage4)
    
    def register_stage(self, stage_id: int, stage_class: Type[BaseStage]):
        """
        Enregistre un nouveau stage.
        
        Args:
            stage_id: Identifiant unique du stage
            stage_class: Classe du stage √† enregistrer
        """
        if not issubclass(stage_class, BaseStage):
            raise ValueError(f"Stage class must inherit from BaseStage")
        
        self._stages[stage_id] = stage_class
        print(f"‚úÖ Stage {stage_id} ({stage_class.__name__}) enregistr√©")
    
    def get_stage_class(self, stage_id: int) -> Type[BaseStage]:
        """R√©cup√®re la classe d'un stage par son ID."""
        if stage_id not in self._stages:
            raise ValueError(f"Stage {stage_id} non trouv√© dans le registre")
        return self._stages[stage_id]
    
    def list_available_stages(self) -> List[int]:
        """Liste les IDs des stages disponibles."""
        return sorted(self._stages.keys())
    
    def create_stage(self, stage_id: int, device: str = "cpu", **kwargs) -> BaseStage:
        """
        Cr√©e une instance d'un stage.
        
        Args:
            stage_id: ID du stage √† cr√©er
            device: Device PyTorch
            **kwargs: Arguments suppl√©mentaires pour le constructeur
            
        Returns:
            Instance du stage
        """
        stage_class = self.get_stage_class(stage_id)
        return stage_class(device=device, **kwargs)


class ModularStageManager:
    """
    Gestionnaire principal des stages modulaires.
    Orchestre l'ex√©cution s√©quentielle des stages de mani√®re d√©coupl√©e.
    """
    
    def __init__(self, global_config: Any, device: str = "cpu"):
        self.global_config = global_config
        self.device = device
        self.registry = StageRegistry()
        self.active_stages: Dict[int, BaseStage] = {}
        self.execution_history: List[Dict[str, Any]] = []
        
        # Configuration d'ex√©cution
        self.stage_sequence = self._determine_stage_sequence()
        self.total_epochs_planned = global_config.TOTAL_EPOCHS
        self.epochs_per_stage = self._calculate_epochs_per_stage()
    
    def _determine_stage_sequence(self) -> List[int]:
        """D√©termine la s√©quence d'ex√©cution des stages."""
        # Par d√©faut : tous les stages disponibles dans l'ordre
        available_stages = self.registry.list_available_stages()
        
        # Possibilit√© de personnaliser via la configuration
        if hasattr(self.global_config, 'CUSTOM_STAGE_SEQUENCE'):
            sequence = self.global_config.CUSTOM_STAGE_SEQUENCE
            # Validation
            for stage_id in sequence:
                if stage_id not in available_stages:
                    raise ValueError(f"Stage {stage_id} non disponible")
            return sequence
        
        return available_stages
    
    def _calculate_epochs_per_stage(self) -> Dict[int, int]:
        """Calcule le nombre d'√©poques par stage selon leur configuration."""
        epochs_per_stage = {}
        
        for stage_id in self.stage_sequence:
            # Cr√©er temporairement le stage pour r√©cup√©rer sa config
            temp_stage = self.registry.create_stage(stage_id, self.device)
            ratio = temp_stage.config.epochs_ratio
            epochs = int(self.total_epochs_planned * ratio)
            epochs_per_stage[stage_id] = epochs
        
        # Ajustement pour s'assurer que la somme = total pr√©vu
        total_calculated = sum(epochs_per_stage.values())
        if total_calculated != self.total_epochs_planned:
            # Ajuste le dernier stage
            last_stage = self.stage_sequence[-1]
            adjustment = self.total_epochs_planned - total_calculated
            epochs_per_stage[last_stage] += adjustment
        
        return epochs_per_stage
    
    def initialize_stage(self, stage_id: int) -> BaseStage:
        """
        Initialise un stage sp√©cifique.
        
        Args:
            stage_id: ID du stage √† initialiser
            
        Returns:
            Instance du stage initialis√©
        """
        print(f"\nüéØ Initialisation du Stage {stage_id}...")
        
        # Param√®tres sp√©cialis√©s selon le stage et ses capacit√©s
        stage_class = self.registry.get_stage_class(stage_id)
        stage_kwargs = self._get_stage_kwargs(stage_class)
        
        # Cr√©ation du stage
        stage = self.registry.create_stage(stage_id, self.device, **stage_kwargs)
        
        # Configuration des donn√©es d'entra√Ænement
        training_config = stage.prepare_training_data(self.global_config)
        stage.training_config = training_config
        
        # Stockage dans les stages actifs
        self.active_stages[stage_id] = stage
        
        print(f"‚úÖ Stage {stage_id} ({stage.config.name}) initialis√©")
        print(f"   üìã {stage.config.description}")
        print(f"   ‚è±Ô∏è  √âpoques pr√©vues: {self.epochs_per_stage[stage_id]}")
        print(f"   üéØ Seuil convergence: {stage.config.convergence_threshold}")
        
        return stage
    
    def _get_stage_kwargs(self, stage_class: Type[BaseStage]) -> Dict[str, Any]:
        """
        D√©termine les param√®tres √† passer au constructeur d'un stage.
        
        Args:
            stage_class: Classe du stage
            
        Returns:
            Dictionnaire des param√®tres compatibles
        """
        import inspect
        
        # R√©cup√©ration de la signature du constructeur
        sig = inspect.signature(stage_class.__init__)
        param_names = list(sig.parameters.keys())
        
        stage_kwargs = {}
        
        # Ajout conditionnel des param√®tres selon ce que le stage accepte
        if 'min_obstacle_size' in param_names and hasattr(self.global_config, 'MIN_OBSTACLE_SIZE'):
            stage_kwargs['min_obstacle_size'] = self.global_config.MIN_OBSTACLE_SIZE
        
        if 'max_obstacle_size' in param_names and hasattr(self.global_config, 'MAX_OBSTACLE_SIZE'):
            stage_kwargs['max_obstacle_size'] = self.global_config.MAX_OBSTACLE_SIZE
        
        return stage_kwargs
    
    def execute_stage(self, stage_id: int, trainer_callback,
                     early_stopping: bool = True) -> Dict[str, Any]:
        """
        Ex√©cute un stage sp√©cifique.
        
        Args:
            stage_id: ID du stage √† ex√©cuter
            trainer_callback: Fonction callback pour l'entra√Ænement
            early_stopping: Active l'arr√™t pr√©coce
            
        Returns:
            M√©triques de l'ex√©cution du stage
        """
        if stage_id not in self.active_stages:
            stage = self.initialize_stage(stage_id)
        else:
            stage = self.active_stages[stage_id]
        
        max_epochs = self.epochs_per_stage[stage_id]
        
        print(f"\nüöÄ === EX√âCUTION STAGE {stage_id} - {stage.config.name.upper()} ===")
        print(f"üìä Maximum {max_epochs} √©poques")
        
        # R√©initialisation de l'historique du stage
        stage.reset_training_history()
        
        # Ex√©cution de l'entra√Ænement via callback
        execution_result = trainer_callback(stage, max_epochs, early_stopping)
        
        # G√©n√©ration du r√©sum√©
        stage_summary = stage.get_stage_summary()
        stage_summary.update(execution_result)
        
        # NOUVEAU : Sauvegarde automatique du checkpoint si configur√©
        if hasattr(self.global_config, 'SAVE_STAGE_CHECKPOINTS') and self.global_config.SAVE_STAGE_CHECKPOINTS:
            self.save_stage_checkpoint_with_callback(stage_id, trainer_callback)
        
        # Enregistrement dans l'historique
        self.execution_history.append({
            'stage_id': stage_id,
            'timestamp': self._get_timestamp(),
            'summary': stage_summary
        })
        
        print(f"‚úÖ === STAGE {stage_id} TERMIN√â ===")
        
        return stage_summary
    
    def save_stage_checkpoint_with_callback(self, stage_id: int, trainer_callback):
        """Sauvegarde le checkpoint d'un stage avec callback vers le trainer."""
        if hasattr(trainer_callback, '__self__'):
            trainer = trainer_callback.__self__
            trainer.save_stage_checkpoint(stage_id)
    
    def execute_full_curriculum(self, trainer_callback) -> Dict[str, Any]:
        """
        Ex√©cute le curriculum complet de tous les stages.
        
        Args:
            trainer_callback: Fonction callback pour l'entra√Ænement
            
        Returns:
            M√©triques globales de l'ex√©cution
        """
        print(f"\nüöÄ === D√âBUT CURRICULUM MODULAIRE ===")
        print(f"üîÑ S√©quence: {self.stage_sequence}")
        print(f"üìä √âpoques totales: {self.total_epochs_planned}")
        
        import time
        start_time = time.time()
        
        all_stage_results = {}
        total_epochs_actual = 0
        
        # Ex√©cution s√©quentielle des stages
        for stage_id in self.stage_sequence:
            stage_result = self.execute_stage(stage_id, trainer_callback)
            all_stage_results[stage_id] = stage_result
            total_epochs_actual += stage_result.get('epochs_trained', 0)
        
        # M√©triques globales
        total_time = time.time() - start_time
        
        global_results = {
            'curriculum_sequence': self.stage_sequence,
            'total_epochs_planned': self.total_epochs_planned,
            'total_epochs_actual': total_epochs_actual,
            'total_time_seconds': total_time,
            'total_time_formatted': f"{total_time/60:.1f} min",
            'stage_results': all_stage_results,
            'all_stages_converged': all(
                result.get('converged', False)
                for result in all_stage_results.values()
            ),
            'execution_history': self.execution_history
        }
        
        # R√©sum√© final
        final_stage_id = self.stage_sequence[-1]
        final_loss = all_stage_results[final_stage_id].get('final_loss', float('inf'))
        global_results['final_loss'] = final_loss
        
        print(f"\nüéâ === CURRICULUM MODULAIRE TERMIN√â ===")
        print(f"‚è±Ô∏è  Temps total: {total_time/60:.1f} minutes")
        print(f"üìä √âpoques: {total_epochs_actual}/{self.total_epochs_planned}")
        print(f"üéØ Convergence globale: {'‚úÖ' if global_results['all_stages_converged'] else '‚ùå'}")
        print(f"üìâ Perte finale: {final_loss:.6f}")
        
        return global_results
    
    def add_custom_stage(self, stage_id: int, stage_class: Type[BaseStage]):
        """
        Ajoute un stage personnalis√© au gestionnaire.
        
        Args:
            stage_id: ID unique pour le nouveau stage
            stage_class: Classe du stage personnalis√©
        """
        self.registry.register_stage(stage_id, stage_class)
        
        # Recalcul de la s√©quence et √©poques si n√©cessaire
        self.stage_sequence = self._determine_stage_sequence()
        self.epochs_per_stage = self._calculate_epochs_per_stage()
    
    def save_stage_checkpoint(self, stage_id: int, model_state: Dict[str, Any],
                             output_dir: Path):
        """
        Sauvegarde le checkpoint d'un stage.
        
        Args:
            stage_id: ID du stage
            model_state: √âtat du mod√®le √† sauvegarder
            output_dir: R√©pertoire de sortie
        """
        if stage_id not in self.active_stages:
            print(f"‚ö†Ô∏è  Stage {stage_id} non actif, checkpoint ignor√©")
            return
        
        stage = self.active_stages[stage_id]
        stage_dir = output_dir / f"stage_{stage_id}"
        stage_dir.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarde des donn√©es du stage
        checkpoint_data = {
            'stage_config': stage.config.__dict__,
            'training_history': stage.training_history,
            'model_state': model_state,
            'timestamp': self._get_timestamp()
        }
        
        # Sauvegarde sp√©cialis√©e pour Stage 4 (intensit√©s)
        if hasattr(stage, 'get_intensity_statistics'):
            checkpoint_data['intensity_statistics'] = stage.get_intensity_statistics()
        
        # Fichiers de sauvegarde
        checkpoint_path = stage_dir / "stage_checkpoint.json"
        with open(checkpoint_path, 'w') as f:
            json.dump(checkpoint_data, f, indent=2, default=str)
        
        print(f"üíæ Checkpoint Stage {stage_id} sauvegard√©: {stage_dir}")
    
    def get_stage(self, stage_id: int) -> Optional[BaseStage]:
        """R√©cup√®re un stage actif par son ID."""
        return self.active_stages.get(stage_id)
    
    def _get_timestamp(self) -> str:
        """G√©n√®re un timestamp pour les logs."""
        import datetime
        return datetime.datetime.now().isoformat()
    
    def get_curriculum_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© du curriculum configur√©."""
        return {
            'stage_sequence': self.stage_sequence,
            'epochs_per_stage': self.epochs_per_stage,
            'total_epochs': self.total_epochs_planned,
            'available_stages': self.registry.list_available_stages(),
            'active_stages': list(self.active_stages.keys())
        }
